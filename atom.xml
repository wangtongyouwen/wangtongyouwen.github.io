<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jyh blog</title>
  
  
  <link href="https://wangtongyouwen.github.io/atom.xml" rel="self"/>
  
  <link href="https://wangtongyouwen.github.io/"/>
  <updated>2023-12-14T14:00:38.847Z</updated>
  <id>https://wangtongyouwen.github.io/</id>
  
  <author>
    <name>jyh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>yolo简介</title>
    <link href="https://wangtongyouwen.github.io/post/6a09e8f8.html"/>
    <id>https://wangtongyouwen.github.io/post/6a09e8f8.html</id>
    <published>2023-12-13T08:51:42.000Z</published>
    <updated>2023-12-14T14:00:38.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-comprehensive-review-of-yolo-from-yolov1-to-yolov8-and-beyond">A COMPREHENSIVE REVIEW OF YOLO: FROM YOLOV1 TO YOLOV8 AND BEYOND</h1><p>https://arxiv.org/pdf/2304.00501v1.pdf</p><h2 id="abstract">abstract</h2><p>YOLO 已成为机器人、无人驾驶汽车和视频监控应用的核心实时目标检测系统。我们对 YOLO 的发展进行了全面的分析，研究了从最初的 YOLO 到 YOLOv8 的每次迭代中的创新和贡献。我们首先描述标准指标和后处理;然后，我们讨论了网络架构的主要变化和每个模型的训练技巧。最后，我们总结了 YOLO 发展的重要经验教训，并对其未来发展进行了展望，强调了增强实时目标检测系统的潜在研究方向。</p><h2 id="introduction">1 Introduction</h2><p>实时目标检测已成为众多应用中的关键组成部分，涵盖自动驾驶汽车、机器人、视频监控和增强现实等各个领域。在各种目标检测算法中，YOLO (You Only Look Once)框架以其出色的速度和精度平衡而脱颖而出，能够快速可靠地识别图像中的目标。自成立以来，YOLO 家族经历了多次迭代，每次迭代都建立在以前的版本之上，以解决局限性并提高性能(见图 1)。本文旨在全面回顾 YOLO 框架的发展，从最初的 YOLOv1 到最新的 YOLOv8，阐明每个版本的关键创新、差异和改进。</p><p>本文首先探讨了原始 YOLO 模型的基本概念和体系结构，为 YOLO 家族的后续发展奠定了基础。接下来，我们将深入研究从 YOLOv2 到 YOLOv8 的每个版本中引入的改进和增强。这些改进包括网络设计、损失函数修改、anchor 修改和输入分辨率缩放等各个方面。通过研究这些发展，我们的目标是全面了解 YOLO 框架的发展及其对目标检测的影响。</p><p>除了讨论每个 YOLO 版本的具体进步之外，本文还强调了在整个框架开发过程中出现的速度和准确性之间的权衡。这强调了在选择最合适的 YOLO 模型时考虑特定应用程序的上下文和需求的重要性。</p><p>最后，我们展望了 YOLO 框架的未来方向，触及了进一步研究和开发的潜在途径，这将影响实时目标检测系统的持续进展。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231213175522050.png" alt="image-20231213175522050" style="zoom:50%;" /></p><h2 id="yolo-在不同领域的应用">2 YOLO 在不同领域的应用</h2><p>YOLO 的实时目标检测能力在自动驾驶车辆系统中非常宝贵，可以快速识别和跟踪各种物体，如车辆、行人<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>、自行车和其他障碍物<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>。</p><p>这些能力已经应用于许多领域，包括用于监控的视频序列中的动作识别<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>、运动分析<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>和人机交互<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>。</p><p>YOLO 模型已在农业中用于农作物<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>、病虫害<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>的检测和分类，协助实现精准农业技术和农业流程自动化。它们也适用于生物识别、安全和面部识别系统中的面部检测任务<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>。</p><p>在医学领域，YOLO 已被用于癌症检测<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>、皮肤分割<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>、药丸识别<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>，提高了诊断的准确性，提高了治疗的效率。在遥感领域，它已被用于卫星和航空图像中的目标检测和分类，有助于土地利用制图、城市规划和环境监<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a><a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a><a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>。</p><p>安防系统集成了 YOLO 模型，用于实时监控和分析视频馈送，从而可以快速发现可疑活动<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>、保持社交距离和检测口罩<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>。这些模型还被应用于表面检测，以检测缺陷和异常，加强制造和生产过程中的质量控制<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a><a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a><a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>。</p><p>在交通应用中，YOLO 模型已被用于车牌检测<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>和交通标志识别<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>等任务，有助于智能交通系统和交通管理解决方案的发展。它们已被用于野生动物检测和监测，以识别濒危物种，用于生物多样性保护和生态系统管理<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>。最后，YOLO 已广泛应用于机器人应用<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a><a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>和无人机目标检测<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a><a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>。</p><h2 id="目标检测衡量指标和非最大抑制nms">3 目标检测衡量指标和非最大抑制(NMS)</h2><h3 id="ap-定义">3.1 AP 定义</h3><p>平均精度(AP)，传统上称为平均平均精度(mAP)，是评估目标检测模型性能的常用度量。它测量所有类别的平均精度，提供一个单一的值来比较不同的模型。COCO 数据集没有区分 AP 和 mAP。在本文的其余部分，我们将把这个度量称为 AP。</p><p>在 YOLOv1 和 YOLOv2 中，用于训练和基准测试的数据集是 PASCAL VOC 2007 和 VOC 2012<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>。然而，从 YOLOv3 开始，使用的数据集是 Microsoft COCO (Common Objects in Context)<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>。对于这些数据集，AP 的计算方法不同。下面几节将讨论 AP 背后的基本原理，并解释如何计算 AP。</p><p><strong>精确度和召回率</strong>:<span class="math inline">\(\text{precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}\)</span>,<span class="math inline">\(\text{recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}\)</span></p><p><span class="math display">\[F = \frac{(\alpha^2+1)P*R}{\alpha^2(P+R)}\]</span></p><p>例如，增加检测到的对象的数量(更高的召回率)可能导致更多的误报(更低的精度)。为了考虑这种权衡，AP 指标结合了精确率-召回率曲线，该曲线绘制了不同置信度阈值下的精确率和召回率。该指标通过考虑准确度-召回曲线下的面积，提供了对精度和召回率的平衡评估</p><p><strong>处理多个目标类别</strong>:目标检测模型必须识别和定位图像中的多个目标类别。AP 度量通过分别计算每个类别的平均精度(AP)，然后在所有类别中取这些 AP 的平均值来解决这个问题(这就是为什么它也称为平均平均精度)。这种方法确保对每个类别单独评估模型的性能，为模型的整体性能提供更全面的评估。</p><p><strong>交并比</strong>：目标检测的目的是通过预测边界框来准确定位图像中的目标。AP 度量结合了(IoU)交叉度量来评估预测边界框的质量。IoU 是预测边界框与地面真值边界框的交集面积与并集面积之比(见图 2)。IoU 衡量的是地面真值边界框与预测边界框的重叠程度。COCO 基准考虑多个 IoU 阈值来评估模型在不同定位精度水平下的性能。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214090616347.png" alt="image-20231214090616347" style="zoom:50%;" /></p><h3 id="计算-ap">3.2 计算 AP</h3><h4 id="voc-dataset">VOC Dataset</h4><p>该数据集包括<strong>20</strong>个对象类别。为了计算 VOC 中的 AP，我们遵循以下步骤:</p><pre><code>1. 对于每个类别，通过改变模型预测的置信阈值来计算精确召回曲线。</code></pre><ol start="2" type="1"><li><p>使用在精度-召回率曲线的 11 点插值抽样计算每个类别的平均精度(AP)。</p></li><li><p>通过取所有 20 个类别中 AP 的平均值来计算最终的平均精度(AP)。</p></li></ol><h4 id="microsoft-coco-dataset">Microsoft COCO Dataset</h4><p>该数据集包括 80 个对象类别，并使用更复杂的方法来计算 AP。它不使用 11 点插值，而是使用 101 点插值，也就是说，它计算 101 个召回阈值的精度，从 0 到 1，增量为 0.01。此外，AP 是通过对多个 IoU 值(而不仅仅是一个)进行平均来获得的，除了一个称为 AP50 的通用 AP 度量，它是单个 IoU 阈值 0.5 的 AP。在 COCO 中计算 AP 的步骤如下:</p><ol type="1"><li><p>对于每个类别，通过改变模型预测的置信阈值来计算精确召回曲线。</p></li><li><p>使用 101 点插值计算每个类别的平均精度(AP)。</p></li><li><p>计算不同交汇交汇(IoU)阈值的 AP，通常从 0.5 到 0.95，步长为 0.05。更高的 IoU 阈值需要更准确的预测才能被认为是真正的阳性。</p></li><li><p>对于每个 IoU 阈值，取所有 80 个类别的 ap 的平均值。</p></li><li><p>最后，通过平均每个 IoU 阈值计算的 AP 值来计算总体 AP。</p></li></ol><h3 id="非极大值抑制nms">3.3 非极大值抑制(NMS)</h3><p>非最大抑制(NMS)是一种用于目标检测算法的后处理技术，目的是减少重叠边界框的数量，提高整体检测质量。目标检测算法通常会在同一目标周围生成多个具有不同置信度分数的边界框。NMS 过滤掉冗余和不相关的边界框，只保留最准确的边界框。算法 1 描述了这个过程。图 3 显示了包含多个重叠边界框的对象检测模型的典型输出和 NMS 后的输出。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095103611.png" alt="image-20231214095103611" style="zoom:50%;" /></p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095200651.png" alt="image-20231214095200651" style="zoom:50%;" /></p><h2 id="yolo-you-only-look-once">4 YOLO: You Only Look Once</h2><p>Joseph Redmon 等人的 YOLO 发表于 CVPR 2016<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>。它首次提出了一种实时的端到端目标检测方法。YOLO 这个名字代表“你只看一次”，这是指它是一个能够通过网络的单次传递完成检测任务，而不是以前的方法，要么使用滑动窗口，然后使用分类器，需要在每张图像上运行数百或数千次，要么使用更高级的方法，将任务分为两步，其中第一步检测具有对象或区域建议的可能区域，第二步在建议上运行分类器。此外，YOLO 使用更直接的基于回归的输出来预测检测输出，而 Fast R-CNN<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>$使用两个单独的输出，概率分类和框坐标回归。</p><h3 id="yolov1-简介">4.1 YOLOv1 简介</h3><p>YOLOv1 统一了目标检测步骤，同时检测所有的边界框。为了实现这一点，YOLO 将输入图像划分为 S × S 网格，并预测同一类的 B 个边界框，以及每个网格元素对 C 个不同类的置信度。每个边界框预测由五个值组成:Pc, bx, by, bh, bw，其中 Pc 是框的置信度得分，反映模型对框中包含对象的置信度以及框的准确性。bx 和 by 坐标是相对于网格单元格的框的中心，bh 和 bw 是相对于完整图像的框的高度和宽度。YOLO 的输出是 S × S × (B × 5 + C)张量，可选地跟随非最大抑制(NMS)来去除重复检测。</p><p>在最初的 YOLO 论文中，作者使用了包含 20 个类(C = 20)的 PASCAL VOC 数据集[36];一个 7 × 7 的网格(S = 7)，每个网格元素最多 2 个类(B = 2)，给出 7 × 7 × 30 的输出预测。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095504303.png" alt="image-20231214095504303" style="zoom:50%;" /></p><p>图 4 显示了一个简化的输出向量，考虑了一个 3 乘 3 的网格、3 个类，每个网格一个类代表 8 个值。在这个简化的情况下，YOLO 的输出将是 3 × 3 × 8。</p><p>YOLOv1 在 PASCAL VOC2007 数据集上的平均精度(AP)为 63.4。</p><h3 id="yolov1-框架">4.2 YOLOv1 框架</h3><p>YOLOv1 架构包括 24 个卷积层，然后是两个完全连接的层，用于预测边界框坐标和概率。除最后一层使用线性激活函数外，所有层都使用漏整流线性单元激活<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>。受 GoogLeNet<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>和 Network in Network<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a>的启发，YOLO 使用 1 × 1 卷积层来减少特征映射的数量，并保持相对较低的参数数量。作为激活层，表 1 描述了 YOLOv1 体系结构。作者还介绍了一个更轻的模型，称为 Fast YOLO，由 9 个卷积层组成。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095738606.png" alt="image-20231214095738606" style="zoom:50%;" /></p><h3 id="yolov1-训练">4.3 YOLOv1 训练</h3><p>作者使用 ImageNet 数据集以 224 × 224 的分辨率预训练了 YOLO 的前 20 层<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a>。然后，他们添加了随机初始化权重的最后四层，并使用 PASCAL VOC 2007 和 VOC 2012 数据集<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a>以 448 × 448 的分辨率对模型进行了精细调整，以增加细节，从而更准确地检测目标。</p><p>对于增强，作者使用最多 20%输入图像大小的随机缩放和平移，以及随机曝光和饱和度，在 HSV 色彩空间中，上限因子为 1.5。</p><p>YOLOv1 使用了一个由多个和平方误差组成的损失函数，如图 5 所示。在损失函数中，<span class="math inline">\(\lambda_{\text{coord}} = 5\)</span>是给予边界框预测更多重要性的比例因子，<span class="math inline">\(λ_{\text{noobj}} = 0.5\)</span>是降低不包含对象的框的重要性的比例因子。</p><p>损失的前两项表示局部化损失;它计算预测的边界框位置<span class="math inline">\((x, y)\)</span>和大小<span class="math inline">\((w, h)\)</span>中的误差。请注意，这些误差仅在包含对象的框中计算(由<span class="math inline">\(\mathbb 1^{\text{obj}}_\text{ij}\)</span>表示)，只有在该网格单元中存在对象时才会受到惩罚。第三和第四个损失项表示信心损失;第三项测量在盒子中检测到对象时的置信误差(<span class="math inline">\(\mathbb 1^{\text{obj}}_\text{ij}\)</span>)，第四项测量在盒子中未检测到对象时的置信误差(<span class="math inline">\(\mathbb 1^{\text{noobj}}_\text{ij}\)</span>)。由于大多数盒子是空的，这个损失被<span class="math inline">\(\lambda_{\text{noobj}}\)</span>项加权。最后一个损失分量是分类损失，它仅在对象出现在单元格(<span class="math inline">\(\mathbb 1^{\text{obj}}_\text{ij}\)</span>)中时测量每个类别的类别条件概率的平方误差。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214100445282.png" alt="image-20231214100445282" style="zoom: 80%;" /></p><h3 id="yolov1-优点和不足">4.4 YOLOv1 优点和不足</h3><p>YOLO 的简单架构，以及其新颖的全图像单次回归，使其比现有的目标检测器更快，从而实现实时性能。</p><p>然而，尽管 YOLO 的执行速度比任何目标检测器都快，但与 Fast R-CNN 等最先进的方法相比，定位误差更大<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>。造成这种限制的主要原因有三个:</p><ol type="1"><li><p>它最多只能检测到网格单元中两个同类的物体，这限制了它预测附近物体的能力。</p></li><li><p>它很难预测训练数据中没有出现的长宽比物体。</p></li><li><p>由于下采样层，它从粗糙的目标特征中学习。</p></li></ol><h2 id="yolov2-better-faster-and-stronger">5 YOLOv2: Better, Faster, and Stronger</h2><p>YOLOv2 由 Joseph Redmon 和 Ali Farhadi 在 CVPR 2017 上发表<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a>。它对原来的 YOLO 进行了一些改进，使其更好，保持相同的速度，也更强大-能够检测 9000 个类别!改进如下:</p><ol type="1"><li><p><strong>batch normalization</strong> 所有卷积层的批处理归一化提高了收敛性，并作为正则化器减少过拟合。</p></li><li><p><strong>高分辨率的分类器</strong>。与 YOLOv1 一样，他们使用 ImageNet 在 224 × 224 的分辨率下对模型进行预训练。然而，这一次，他们在 ImageNet 上 finetuned 了 10 个 epoch 的模型，分辨率为 448 × 448，提高了网络在更高分辨率输入下的性能。</p></li><li><p><strong>完全卷积</strong>。他们去掉了密集的层，使用了一个完全卷积的架构。</p></li><li><p><strong>使用 anchor boxes 来预测边界框</strong>。它们使用一组先验框或锚框，锚框是具有预定义形状的框，用于匹配对象的原型形状，如图 6 所示。为每个网格单元定义多个锚框，系统预测每个锚框的坐标和类。网络输出的大小与每个网格单元的锚盒数量成正比。</p></li></ol><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214104432020.png" alt="image-20231214104432020" style="zoom:50%;" /></p><ol start="5" type="1"><li><p><strong>聚类中心</strong>。选择好的先验框有助于网络学习预测更准确的边界框。作者在训练边界盒上运行 k-means 聚类来找到好的先验。他们选择了五个先前的盒子，在召回率和模型复杂性之间进行了很好的权衡。</p></li><li><p><strong>直接位置预测</strong>。与其他预测偏移量的方法<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a>不同，YOLOv2 遵循相同的原理，预测相对于网格单元的位置坐标。网络为每个单元格预测 5 个边界框，每个边界框有 5 个值 tx、ty、tw、th 和 to，其中 to 相当于 YOLOv1 中的 P c，最终得到边界框坐标，如图 7 所示。</p></li></ol><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214104903836.png" alt="image-20231214104903836" style="zoom:50%;" /></p><ol start="7" type="1"><li><strong>细粒度特性</strong>。与 YOLOv1 相比，YOLOv2 去掉一个池化层，得到 416 × 416 输入图像的 13 × 13 输出特征图或网格。YOLOv2 还使用了一个直通层，该层采用 26 × 26 × 512 的特征映射，并通过将相邻的特征堆叠到不同的通道中来重新组织而不是通过空间子采样丢失它们。这将生成 13 × 13 × 2048 个通道维度的特征图，并与低分辨率的 13 × 13 × 1024 个特征图进行连接，得到 13 × 13 × 3072 个特征图。</li><li><strong>多尺度的训练</strong>。由于 YOLOv2 不使用全连接层，因此输入可以是不同的大小。为了使 YOLOv2 对不同的输入大小具有鲁棒性，作者随机训练模型，每 10 批次改变输入大小——从 320 × 320 到 608 × 608。</li></ol><p>通过所有这些改进，YOLOv2 在 PASCAL VOC2007 数据集上的平均精度(AP)达到 78.6%，而 YOLOv1 的平均精度为 63.4%。</p><h3 id="yolov2-框架">5.1 YOLOv2 框架</h3><p>YOLOv2 使用的骨干架构称为 Darknet-19，包含 19 个卷积层和 5 个 maxpooling 层。与 YOLOv1 的架构类似，它的灵感来自于 Network in Network<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a>，在 3 × 3 之间使用 1 × 1 的卷积来减少参数的数量。此外，如上所述，他们使用批归一化来正则化和帮助收敛。</p><p>表 2 显示了带有目标检测头的整个 Darknet-19 主干。使用 PASCAL VOC 数据集时，YOLOv2 预测 5 个边界框，每个边界框有 5 个值和 20 个类。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214105332256.png" alt="image-20231214105332256" style="zoom:80%;" /></p><p>分类头将最后四个卷积层替换为一个包含 1000 个过滤器的单个卷积层，然后是一个全局平均池化层和一个 Softmax。</p><h3 id="yolo9000-is-a-stronger-yolov2">5.2 YOLO9000 is a stronger YOLOv2</h3><p>作者在同一篇文章中介绍了一种训练联合分类和检测的方法。它使用 COCO<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>中的检测标记数据学习边界框坐标和 ImageNet 中的分类数据，以增加可检测的类别数量。在训练过程中，他们将两个数据集结合起来，这样当使用检测训练图像时，它会反向传播检测网络，当使用分类训练图像时，它会反向传播体系结构的分类部分。结果是一个能够检测超过 9000 个类别的 YOLO 模型，因此名称为 YOLO9000。</p><h2 id="yolov3">6 YOLOv3</h2><p>YOLOv3<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a>由 Joseph Redmon 和 Ali Farhadi 于 2018 年在 ArXiv 上发表。它包括重大变化和更大的架构，以与最先进的技术相媲美，同时保持实时性能。在下文中，我们描述了与 YOLOv2 相关的更改。</p><ol type="1"><li><p><strong>边界框预测</strong>。与 YOLOv2 一样，该网络为每个边界框 tx、ty、tw 和 th 预测四个坐标;然而，这一次，YOLOv3 使用逻辑回归预测每个边界框的对象得分。与 ground truth 重叠度最高的锚框得分为 1，其余锚框得分为 0。与 Faster R-CNN<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a>不同，YOLOv3 只为每个 ground truth 对象分配一个锚框。同样，如果没有给对象分配锚盒，只会造成分类损失，不会造成定位损失和置信度损失。</p></li><li><p><strong>类的预测</strong>。他们没有使用 softmax 进行分类，而是使用二元交叉熵来训练独立的逻辑分类器，并将问题作为多标签分类。这种变化允许为同一个框分配多个标签，这可能发生在一些具有重叠标签的复杂数据集<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>上。例如，同一个对象可以是 Person 和 Man。</p></li><li><p><strong>新的主干</strong>。YOLOv3 具有更大的特征提取器，由 53 个带有残差连接的卷积层组成。第 6.1 节更详细地描述了该体系结构。</p></li><li><p><strong>空间金字塔池化</strong>(SPP)虽然在论文中没有提到，但作者还在主干中添加了一个修改的 SPP 块<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a>，该块连接多个最大池化输出而不进行子采样(stride = 1)，每个块具有不同的内核大小 k × k，其中 k = 1,5,9,13 允许更大的感受野。这个版本被称为 YOLOv3-spp，是性能最好的版本，AP50 提高了 2.7%。</p></li><li><p><strong>多尺度预测</strong>。与特征金字塔网络<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a>类似，YOLOv3 在三个不同的尺度上预测三个盒子。第 6.2 节详细描述了多尺度预测机制。</p></li><li><p><strong>边界框先验</strong>。与 YOLOv2 一样，作者也使用 k-means 来确定锚框的边界框先验。不同之处在于，在 YOLOv2 中，他们为每个单元格总共使用了五个先验框，而在 YOLOv3 中，他们为三个不同的尺度使用了三个先验框。</p></li></ol><h3 id="yolov3-架构">6.1 YOLOv3 架构</h3><p>YOLOv3 中呈现的架构主干称为 Darknet-53。它用跨行卷积替换了所有的最大池化层，并添加了残差连接。总的来说，它包含 53 个卷积层。图 8 显示了体系结构的细节。</p><p>Darknet-53 骨干网获得与 ResNet-152 相当的 Top-1 和 Top-5 精度，但几乎快了 2 倍。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214110812922.png" alt="image-20231214110812922" style="zoom:80%;" /></p><h3 id="yolov3-多尺度预测">6.2 YOLOv3 多尺度预测</h3><p>除了更大的架构外，YOLOv3 的一个基本特征是多尺度预测，即在多个网格大小下进行预测。这有助于获得更精细的细节框，并显着提高了对小物体的预测，这是以前版本的 YOLO 的主要弱点之一。</p><p>图 9 所示的多尺度检测架构的工作原理如下:标记为 y1 的第一个输出相当于 YOLOv2 的输出，其中一个 13 × 13 的网格定义了输出。第二个输出 y2 是将 Darknet-53 的(Res × 4)之后的输出与(Res × 8)之后的输出拼接而成。feature map 的大小不同，分别是 13 × 13 和 26 × 26，所以在拼接之前需要进行上采样操作。最后，使用上采样操作，第三个输出 y3 将 26 × 26 特征映射与 52 × 52 特征映射连接起来。</p><p>对于具有 80 个类别的 COCO 数据集，每个尺度提供一个形状为 N × N ×[3x(4+1+80)]的输出张量，其中 N ×N 为特征图(或网格单元)的大小，3 表示每个单元的框数，4+1 包括四个坐标和对象得分。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214112806628.png" alt="image-20231214112806628" style="zoom:80%;" /></p><h3 id="yolov3-结果">6.3 YOLOv3 结果</h3><p>当 YOLOv3 发布时，目标检测的基准已经从 PASCAL VOC 改为 Microsoft COCO<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a>。因此，从这里开始，所有的 yolo 都在 MS COCO 数据集中进行评估。在 20 FPS 下，YOLOv3-spp 的平均精度 AP 为 36.2%，AP50 为 60.6%，达到了当时最先进的水平，速度提高了 2 倍。</p><h2 id="backbone-neck-and-head">7 Backbone Neck and Head</h2><p>此时，目标探测器的结构开始被描述为三个部分:Backbone、Neck 和 Head。图 10 显示了一个高级的 Backbone、Neck 和 Head。</p><p>主干负责从输入图像中提取有用的特征。它通常是在大规模图像分类任务上训练的卷积神经网络(CNN)，例如 ImageNet。主干捕获不同尺度的分层特征，较低级的特征(如边缘和纹理)在较早的层中提取，较高级的特征(如对象部分和语义信息)在较深的层中提取。</p><p>颈部是连接主干和头部的中间部分。它对主干提取的特征进行聚合和细化，往往侧重于增强不同尺度的空间和语义信息。</p><p>颈部可能包括额外的卷积层、特征金字塔网络(FPN)<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a>或其他机制来改善特征的表示。</p><p>头部是物体检测器的最终组成部分;它负责根据脊柱和颈部提供的特征做出预测。它通常由一个或多个特定于任务的子网组成，这些子网执行分类、定位以及最近的实例分割和姿态估计。头部处理颈部提供的特征，为每个候选对象生成预测。最后，一个后处理步骤，如非最大抑制(NMS)，过滤掉重叠的预测，只保留最可靠的检测。</p><p>在 YOLO 模型的其余部分中，我们将使用 Backbone、Neck 和 Head 来描述架构。</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214113148856.png" alt="image-20231214113148856" style="zoom:50%;" /></p><h2 id="yolov4">8 YOLOv4</h2><p>两年过去了，并没有新的 YOLO 版本。直到 2020 年 4 月，Alexey Bochkovskiy, Chien-Y ao Wang, Hong-Y yuan Mark Liao 在 ArXiv 发表了 YOLOv4 的论文<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a>。起初，不同的作者提出了一个新的“官方”版本的 YOLO，这让人感到奇怪;然而，yolo4 保持了相同的 YOLO 理念——实时、开源、单镜头和 DarkNet 框架——并且改进是如此令人满意，社区迅速接受了这个版本作为官方的 yolo4。</p><p>YOLOv4 试图找到最佳的平衡，尝试了许多被归类为 bag-of-freebies 和 bag-of-specials 的变化。Bag-of-freebies 是指只改变训练策略，增加训练成本，但不增加推理时间的方法，最常见的是数据增强。另一方面，bag-of-specials 是稍微增加推理成本但显著提高准确率的方法。这些方法的例子包括扩大感受野<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a><a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a><a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a>，结合特征<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a><a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a><a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a><a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a>和后处理<a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a><a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a><a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a><a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a>等。</p><p>我们将 YOLOv4 的主要变化总结为以下几点:<a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a></p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214113550393.png" alt="image-20231214113550393" style="zoom:80%;" /></p><ol type="1"><li><p>具有(BoS)集成的增强架构。作者尝试了多种骨干网架构，如 ResNeXt50<a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a>、EfficientNet-B3<a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a>和 Darknet-53。性能最好的架构是对 Darknet-53 的修改，采用跨级部分连接(CSPNet)<span class="math inline">\(^{[61]}\)</span>，以 Mish 激活函数<a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a>作为主干(见图 11)。对于颈部，他们使用了来自 YOLOv3- SPP 的修改版本的空间金字塔池(SPP)<a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>和与 YOLOv3 一样的多尺度预测，但使用了修改版本的路径聚合网络(PANet)<a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a>$而不是 FPN 以及修改的空间注意模块(SAM)<a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a>。最后，对于探测头，他们使用 YOLOv3 中的 anchor。因此，将模型命名为 CSPDarknet53-PANet-SPP。添加到 Darknet-53 中的跨级部分连接(CSP)有助于减少模型的计算量，同时保持相同的精度。在 YOLOv3-spp 中，SPP 块在不影响推理速度的情况下增加了接受野。PANet 的修改版本将这些特性连接起来，而不是像在 PANet 的原始文件中那样添加它们。</p></li><li><p>整合(BoF)的高级培训方法。除了常规的增强，如随机亮度、对比度、缩放、裁剪、翻转和旋转，作者还实现了马赛克增强，将四张图像合并为一张图像，允许检测其通常上下文之外的对象，并减少了批量规范化对大型 mini-batch 大小的需求。对于正则化，他们使用 DropBlock<a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a>作为 Dropout<a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a>的替代品，但用于卷积神经网络以及类标签平滑<a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a><a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a>。对于检测器，他们增加了 CIoU 损耗<a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a>和 Cross mini-bath normalization (CmBN)，以便从整个批次收集统计数据，而不是像常规批次归一化那样从单个小批次收集统计数据<a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a>。</p></li><li><p>自我对抗训练(SAT)。为了使模型对扰动更具鲁棒性，对输入图像执行对抗性攻击，以创建一个欺骗，即 ground truth 对象不在图像中，但保持原始标签以检测正确的对象。</p></li><li><p>遗传算法的超参数优化。为了找到用于训练的最优超参数，他们在前 10%的周期中使用遗传算法，并使用余弦退火调度器<a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a>来改变训练期间的学习率。它开始缓慢地降低学习率，然后在训练过程中快速降低，最后略有降低。</p></li></ol><figure><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214160738192.png" alt="image-20231214160738192" /><figcaption aria-hidden="true">image-20231214160738192</figcaption></figure><p>在 MS COCO 数据集测试开发 2017 上进行评估，YOLOv4 在 NVIDIA V100 上以超过 50 FPS 的速度实现了 43.5%的 AP 和 65.7%的 AP50。</p><h2 id="yolov5">9 YOLOv5</h2><p>YOLOv5<a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a>于 2020 年由 Glenn Jocher 在 YOLOv4 发布几个月后发布。在撰写本文时，还没有关于 YOLOv5 的科学论文，但从代码中，我们知道它使用了 YOLOv4 部分中描述的许多改进，主要区别在于它是在 Pytorch 而不是 Darknet 中开发的。</p><p>YOLOv5 是开源的，由 Ultralytics 积极维护，有 250 多个贡献者，并且经常有新的改进。YOLOv5 易于使用、训练和部署。Ultralytics 提供 iOS 和 Android 的移动版本，以及许多标签、培训和部署的集成。</p><p>YOLOv5 提供 5 个缩放版本:YOLOv5n (nano)、YOLOv5s (small)、YOLOv5m (medium)、YOLOv5l (large)、YOLOv5x (extra large)。</p><p>撰写本文时发布的 YOLOv5 版本是 v7.0，其中包括能够分类和实例分割的 YOLOv5 版本。</p><p>在 MS COCO 数据集 test-dev 2017 上进行评估，YOLOv5x 在图像尺寸为 640 像素时实现了 50.7%的 AP。</p><p>使用 batchsize=32，它可以在 NVIDIA V100 上实现 200 FPS 的速度。使用 1536 像素的更大输入尺寸，YOLOv5 实现了 55.8%的 AP。</p><h2 id="scaled-yolov4">10 Scaled-YOLOv4</h2><p>在 YOLOv4 发布一年后，同一作者在 CVPR 2021 上提出了 Scaled-YOLOv4<a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a>。与 YOLOv4 不同的是，Scaled YOLOv4 是在 Pytorch 而不是 Darknet 中开发的。主要的新颖之处是引入了按比例放大和按比例缩小的技术。扩大规模意味着以较低的速度为代价来提高准确性;另一方面，按比例缩小需要生成一个以牺牲准确性为代价提高速度的模型。此外，按比例缩小的模型需要更少的计算能力，并且可以在嵌入式系统上运行。</p><p>这个按比例缩小的架构被称为 YOLOv4-tiny;它是为低端 gpu 设计的，可以在 Jetson TX2 上运行 46 FPS 或在 RTX2080Ti 上运行 440 FPS，在 MS COCO 上实现 22%的 AP。</p><p>按比例扩大的模型体系结构称为 YOLOv4-large，其中包括三种不同的尺寸 P5、P6 和 P7。该架构是为云 GPU 设计的，并实现了最先进的性能，超过了所有以前的模型<a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a><a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a><a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a>，在 MS COCO 上具有 56%的 AP。</p><h2 id="yolor">11 YOLOR</h2><p>YOLOR<a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a>由 YOLOv4 的同一个研究小组于 2021 年 5 月在 ArXiv 上发表。它代表 You Only Learn One Representation。在本文中，作者采用了一种不同的方法;他们开发了一种多任务学习方法，旨在通过学习一般表示和使用子网络创建任务特定表示来为各种任务(例如，分类，检测，姿态估计)创建单个模型。鉴于传统的联合学习方法通常会导致次优特征生成，YOLOR 旨在通过对神经网络的隐式知识进行编码以应用于多个任务来克服这一问题，类似于人类如何利用过去的经验来解决新问题。结果表明，在神经网络中引入隐式知识对所有任务都有好处。</p><p>在 MS COCO 数据集测试开发 2017 上进行评估，在 NVIDIA V100 上 30 FPS 下，YOLOR 的 AP 为 55.4%，AP50 为 73.3%。</p><h2 id="yolox">12 YOLOX</h2><p>YOLOX<a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a>由旷视科技的研究团队于 2021 年 7 月在 ArXiv 上发表。在 Pytorch 中开发并使用来自 Ultralytics 的 YOLOV3 作为起点，它有五个主要的变化:无锚点架构，多个阳性，解耦头部，高级标签分配和强增强。它在 2021 年时中取得了最先进的成绩，在速度和精度之间取得了最佳平衡，在 Tesla V100 上，AP 为 50.1%，FPS 为 68.9%。下面，我们将介绍 YOLOX 相对于 YOLOv3 的五个主要变化:</p><ol type="1"><li><p>Anchor-free。自 YOLOv2 以来，所有后续的 YOLO 版本都是基于锚点的检测器。YOLOX 受无锚点最先进的目标探测器(如 CornerNet<a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a>、CenterNet<a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a>和 FCOS<a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a>)的启发，回归到无锚点架构，简化了训练和解码过程。与 YOLOv3 基线相比，无锚点使 AP 增加了 0.9 个点。</p></li><li><p>多积极的方面。为了弥补锚点缺失造成的巨大不平衡，作者使用中心抽样<a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a>，他们将中心 3 × 3 的区域指定为正区域。这种方法使 AP 提高了 2.1 分。</p></li><li><p>解耦。文献<a href="#fn94" class="footnote-ref" id="fnref94" role="doc-noteref"><sup>94</sup></a><a href="#fn95" class="footnote-ref" id="fnref95" role="doc-noteref"><sup>95</sup></a>表明，分类置信度与定位精度之间可能存在不一致。因此，YOLOX 将两者分为两个头(如图 12 所示)，一个用于分类任务，另一个用于回归任务，将 AP 提高了 1.1 点，加快了模型收敛速度。</p></li><li><p>高级标签分配。文献<a href="#fn96" class="footnote-ref" id="fnref96" role="doc-noteref"><sup>96</sup></a>表明，当多个物体的盒子重叠时，ground truth 标签分配可能存在歧义，并将分配过程表述为最优传输(Optimal Transport, OT)问题。YOLOX 受到这项工作的启发，提出了一个简化的版本，叫做 simOTA。这个改动使 AP 增加了 2.3 点。</p></li><li><p>强大的扩增。YOLOX 使用 MixUP<a href="#fn97" class="footnote-ref" id="fnref97" role="doc-noteref"><sup>97</sup></a>和 Mosaic 增强。作者发现，使用这些增强后，ImageNet 预训练不再有益。强大的增强使 AP 增加 2.4 点。</p></li></ol><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214205859709.png" alt="image-20231214205859709" style="zoom:80%;" /></p><h2 id="yolov6">13 YOLOv6</h2><p>YOLOv6<a href="#fn98" class="footnote-ref" id="fnref98" role="doc-noteref"><sup>98</sup></a>由美团视觉 AI 部门于 2022 年 9 月在 ArXiv 上发表。与 YOLOv4 和 YOLOv5 类似，它为工业应用提供了不同尺寸的各种型号。遵循 achor-free 的方法<a href="#fn99" class="footnote-ref" id="fnref99" role="doc-noteref"><sup>99</sup></a><a href="#fn100" class="footnote-ref" id="fnref100" role="doc-noteref"><sup>100</sup></a>的趋势，YOLOv6 采用了无锚点检测器。该模型的主要新颖之处总结如下:</p><ol type="1"><li><p>一种基于 RepVGG<a href="#fn101" class="footnote-ref" id="fnref101" role="doc-noteref"><sup>101</sup></a>的新型骨干网，称为 EfficientRep，它比以前的 YOLO 骨干网使用更高的并行性。对于颈部，他们使用了 PAN<a href="#fn102" class="footnote-ref" id="fnref102" role="doc-noteref"><sup>102</sup></a>，对于较大的模型，他们使用了 RepBlocks<a href="#fn103" class="footnote-ref" id="fnref103" role="doc-noteref"><sup>103</sup></a>或 CSPStackRep<a href="#fn104" class="footnote-ref" id="fnref104" role="doc-noteref"><sup>104</sup></a> block。受 YOLOX 的启发，他们开发了一个高效的分离头。</p></li><li><p>使用 TOOD<a href="#fn105" class="footnote-ref" id="fnref105" role="doc-noteref"><sup>105</sup></a>中引入的任务对齐学习方法进行标签分配。</p></li><li><p>新的分类和回归损失。他们使用了变焦损失分类<a href="#fn106" class="footnote-ref" id="fnref106" role="doc-noteref"><sup>106</sup></a>和 SIoU <a href="#fn107" class="footnote-ref" id="fnref107" role="doc-noteref"><sup>107</sup></a>/GIoU<a href="#fn108" class="footnote-ref" id="fnref108" role="doc-noteref"><sup>108</sup></a>回归损失。</p></li><li><p>回归和分类任务的自蒸馏策略。</p></li><li><p>一种使用 RepOptimizer<a href="#fn109" class="footnote-ref" id="fnref109" role="doc-noteref"><sup>109</sup></a>和通道式蒸馏<a href="#fn110" class="footnote-ref" id="fnref110" role="doc-noteref"><sup>110</sup></a>的检测量化方案，有助于实现更快的检测器。</p></li></ol><p>在 MS COCO 数据集测试开发 2017 上进行评估，YOLOv6-L 在 NVIDIA Tesla T4 上以 50 FPS 左右的速度实现了 52.5%的 AP 和 70%的 AP50。</p><h2 id="yolov7">14 YOLOv7</h2><p>YOLOv7<a href="#fn111" class="footnote-ref" id="fnref111" role="doc-noteref"><sup>111</sup></a>于 2022 年 7 月由 YOLOv4 和 YOLOR 的同一作者发表在 ArXiv 上。当时，它在速度和精度上超过了所有已知的物体探测器，在 5 FPS 到 160 FPS 的范围内。与 YOLOv4 一样，它只使用 MS COCO 数据集进行训练，没有预先训练主干。YOLOv7 提出了几个架构变化和一系列 bag-of-freebies，在不影响推理速度的情况下提高了准确性，只影响了训练时间。</p><p>The architecture changes of YOLOv7 are:</p><ol type="1"><li><strong>扩展高效层聚合网络(E-ELAN)</strong>。ELAN<a href="#fn112" class="footnote-ref" id="fnref112" role="doc-noteref"><sup>112</sup></a>是一种策略，通过控制最短最长的梯度路径，允许深度模型更有效地学习和收敛。YOLOv7 提出的 E-ELAN 适用于具有无限堆叠计算块的模型。E-ELAN 通过洗牌和合并基数来结合不同群体的特征，在不破坏原有梯度路径的情况下增强网络的学习能力。</li><li><strong>基于连接的模型的模型缩放</strong>。缩放通过调整模型的某些属性来生成不同大小的模型。YOLOv7 的体系结构是基于串联的体系结构，其中标准缩放技术(如深度缩放)会导致过渡层的输入通道和输出通道之间的比率变化，从而导致模型的硬件使用减少。YOLOv7 提出了一种新的基于串联的模型缩放策略，该策略将块的深度和宽度以相同的因子进行缩放，以保持模型的最优结构。</li></ol><p>The bag-of-freebies used in YOLOv7 include:</p><ol type="1"><li><p><strong>计算后的重新参数化卷积</strong>。与 YOLOv6 一样，YOLOv7 的架构也受到了重参数化卷积(RepConv)的启发<a href="#fn113" class="footnote-ref" id="fnref113" role="doc-noteref"><sup>113</sup></a>。然而，他们发现 RepConv 中的身份连接破坏了 ResNet<a href="#fn114" class="footnote-ref" id="fnref114" role="doc-noteref"><sup>114</sup></a>中的残差和 DenseNet<a href="#fn115" class="footnote-ref" id="fnref115" role="doc-noteref"><sup>115</sup></a>中的连接。出于这个原因，他们删除了身份连接并将其称为 RepConvN。</p></li><li><p><strong>粗标签分配辅助 head 和细标签分配主 head</strong>。lead head 负责最终产出，auxiliary head 协助训练。</p></li><li><p><strong>批量规范化的 conv-bn-activation</strong>。这将批归一化的均值和方差集成到推理阶段卷积层的偏差和权重中。</p></li><li><p>在 YOLOR 中启发的隐性知识<a href="#fn116" class="footnote-ref" id="fnref116" role="doc-noteref"><sup>116</sup></a>。</p></li><li><p>指数移动平均作为最终的推理模型。</p></li></ol><p>YOLOv7 与由同一作者开发的以前的 YOLO 模型相比的增强。</p><p>与 YOLOv4 相比，YOLOv7 的参数减少了 75%，计算量减少了 36%，同时平均精度(AP)提高了 1.5%。</p><p>与 YOLOv4-tiny 相比，YOLOv7-tiny 在保持相同 AP 的情况下，分别减少了 39%和 49%的参数和计算量。</p><p>最后，与 YOLOR 相比，YOLOv7 分别减少了 43%和 15%的参数数量和计算量，同时 AP 略有增加 0.4%</p><p>在 MS COCO 数据集 test-dev 2017 上进行评估，YOLOv7-E6 在 NVIDIA V100 上以 50 FPS 的速度，以 1280 像素的输入大小实现了 55.9%的 AP 和 73.5%的 AP50。</p><h2 id="damo-yolo">15 DAMO-YOLO</h2><p>DAMO-YOLO<a href="#fn117" class="footnote-ref" id="fnref117" role="doc-noteref"><sup>117</sup></a>由阿里巴巴集团于 2022 年 11 月在 ArXiv 发表。DAMO-YOLO 受到当前技术的启发，包括以下内容:</p><ol type="1"><li><p>神经结构搜索(NAS)。他们使用阿里巴巴开发的 MAE-NAS<a href="#fn118" class="footnote-ref" id="fnref118" role="doc-noteref"><sup>118</sup></a>方法自动寻找高效架构。</p></li><li><p>large neck。受 GiraffeDet<a href="#fn119" class="footnote-ref" id="fnref119" role="doc-noteref"><sup>119</sup></a>、CSPNet<a href="#fn120" class="footnote-ref" id="fnref120" role="doc-noteref"><sup>120</sup></a>和 ELAN<a href="#fn121" class="footnote-ref" id="fnref121" role="doc-noteref"><sup>121</sup></a>的启发，作者设计了一种可以实时工作的颈部，称为 Efficient-RepGFPN。</p></li><li><p>samll head。作者发现，大颈和小颈的性能更好，他们只留下一个线性层用于分类，一个用于回归。他们称这种方法为 ZeroHead。</p></li><li><p>AlignedOTA 标签分配。动态标签分配方法，如 OTA<a href="#fn122" class="footnote-ref" id="fnref122" role="doc-noteref"><sup>122</sup></a>和 TOOD<a href="#fn123" class="footnote-ref" id="fnref123" role="doc-noteref"><sup>123</sup></a>，由于其相对于静态方法的显著改进而受到欢迎。然而，分类和回归之间的不一致仍然是一个问题，部分原因是分类和回归损失之间的不平衡。为了解决这一问题，他们的 AlignOTA 方法将焦损失<a href="#fn124" class="footnote-ref" id="fnref124" role="doc-noteref"><sup>124</sup></a>引入到分类成本中，并使用预测和 ground truth box 的 IoU 作为软标签，可以为每个目标选择对齐的样本，从全局角度解决问题。</p></li><li><p>知识蒸馏。他们提出的策略包括两个阶段:第一阶段是教师指导学生，第二阶段是学生自主微调。此外，他们在蒸馏方法中加入了两个增强功能:Align 模块，它使学生的特征与教师的特征具有相同的分辨率，以及 Channel-wise Dynamic Temperature，它使教师和学生的特征标准化，以减少实际值差异的影响。</p></li></ol><p>作者生成了名为 DAMO-YOLO-Tiny/Small/Medium 的缩放模型，最佳模型在 NVIDIA V100 上以 233 FPS 的速度实现了 50.0%的 AP</p><h2 id="yolov8">16 YOLOv8</h2><p>YOLOv8<a href="#fn125" class="footnote-ref" id="fnref125" role="doc-noteref"><sup>125</sup></a>于 2023 年 1 月由开发 YOLOv5 的公司 Ultralytics 发布。因为在撰写本文时，还没有关于 YOLOv8 的论文，所以我们需要深入了解与其他 YOLO 版本相比的架构决策。按照目前的趋势，YOLOv8 是无锚的，减少了框预测的数量，加快了非最大印象(NMS)。此外，YOLOv8 在训练过程中使用马赛克增强;然而，因为已经发现，如果在整个训练过程中使用这种增强可能是有害的，所以在最后十个时代禁用它。</p><p>YOLOv8 可以从命令行界面(CLI)运行，也可以作为 PIP 包安装。此外，它还具有用于标记、培训和部署的多个集成。</p><p>YOLOv8 提供了 5 个缩放版本:YOLOv8n(nano)、YOLOv8s(small)、YOLOv8m(medium)、YOLOv8l(large)和 YOLOv8x(extra large)。</p><p>在 MS COCO 数据集测试开发 2017 上进行评估，YOLOv8x 在图像尺寸为 640 像素时实现了 53.9%的 AP(相比之下，在相同输入尺寸下，YOLOv5 的 AP 为 50.7%)，在 NVIDIA A100 和 TensorRT 上的速度为 280 FPS。</p><h2 id="pp-yolo-pp-yolov2-和-pp-yoloe">17 PP-YOLO, PP-YOLOv2 和 PP-YOLOE</h2><p>PP-YOLO 模型已经与我们描述的 YOLO 模型并行发展。但是，我们决定将它们分组在一个部分中，因为它们从 YOLOv3 开始，并且在以前的 PP-YOLO 版本的基础上逐渐改进。尽管如此，这些模型对 YOLO 的演变产生了影响。PP-YOLO<a href="#fn126" class="footnote-ref" id="fnref126" role="doc-noteref"><sup>126</sup></a>与 YOLOv4 和 YOLOv5 类似，是基于 YOLOv3 的。该研究于 2020 年 7 月由百度公司的研究人员发表在 ArXiv 上。</p><p>作者使用了 PaddlePaddle<a href="#fn127" class="footnote-ref" id="fnref127" role="doc-noteref"><sup>127</sup></a>深度学习平台，因此其名称为 PP。遵循我们从 YOLOv4 开始看到的趋势，PP-YOLO 增加了十个现有的技巧来提高探测器的准确性，保持速度不变。根据作者的说法，本文并不是要介绍一种新的目标检测器，而是要展示如何一步一步地构建一个更好的检测器。PP-YOLO 使用的大多数技巧与 yolo4 中使用的技巧不同，重叠的技巧使用不同的实现。PP-YOLO 对 YOLOv3 的变化如下:</p><ol type="1"><li><p>ResNet50-vd 骨干网络在最后阶段用一个增强了可变形卷积的架构取代 DarkNet-53 骨干网<a href="#fn128" class="footnote-ref" id="fnref128" role="doc-noteref"><sup>128</sup></a>，以及一个蒸馏的预训练模型，该模型在 ImageNet 上具有更高的分类精度。这个架构被称为 ResNet5-vd-dcn。</p></li><li><p>更大的批处理规模以提高训练的稳定性，它们从 64 增加到 192，同时更新了训练计划和学习率。</p></li><li><p>维护训练参数的移动平均线，并使用它们代替最终训练值。</p></li><li><p>DropBlock 只应用于 FPN。</p></li><li><p>在另一个分支中，将 IoU 损失与边界盒回归的 l1 损失一起添加。</p></li><li><p>添加了 IoU 预测分支来测量定位精度以及 IoU 感知损失。在推理过程中，YOLOv3 将分类概率和客观评分相乘计算最终检测结果，PP-YOLO 还将预测 IoU 相乘考虑定位精度。</p></li><li><p>采用类似于 YOLOv4 的网格敏感方法改进网格边界的边界框中心预测。</p></li><li><p>采用矩阵式 NMS<a href="#fn129" class="footnote-ref" id="fnref129" role="doc-noteref"><sup>129</sup></a>，可以并行运行，比传统 NMS 运行速度更快。</p></li><li><p>CoordConv<a href="#fn130" class="footnote-ref" id="fnref130" role="doc-noteref"><sup>130</sup></a>用于 FPN 的 1 × 1 卷积，在检测头的第一个卷积层上。CoordConv 允许网络学习平移不变性，提高检测定位。</p></li><li><p>空间金字塔池化仅在顶部特征图上使用，以增加主干的接受野。</p></li></ol><h3 id="pp-yolo-数据增强和预处理">17.1 PP-YOLO 数据增强和预处理</h3><p>PP-YOLO 采用了以下增强和预处理:</p><ol type="1"><li><p>混合训练<a href="#fn131" class="footnote-ref" id="fnref131" role="doc-noteref"><sup>131</sup></a>，从<span class="math inline">\(Beta(\alpha,\beta)\)</span>分布中抽样权值，其中 α = 1.5， β = 1.5。</p></li><li><p>随机颜色失真。</p></li><li><p>随机的扩大。</p></li><li><p>随机裁剪和随机翻转的概率为 0.5。</p></li><li><p>RGB 通道 z-score 归一化，均值为[0.485,0.456,0.406]，标准差为[0.229,0.224,0.225]。</p></li><li><p>从[320,352,384,416,448,480,512,544,576,608]中均匀绘制多个图像大小。</p></li></ol><h3 id="results">17.2 results</h3><p>在 MS COCO 数据集测试开发 2017 上进行评估，PP-YOLO 在 NVIDIA V100 上以 73 FPS 的速度实现了 45.9%的 AP 和 65.2%的 AP50。</p><h3 id="pp-yolov2">17.3 PP-YOLOv2</h3><p>PP-YOLOv2<a href="#fn132" class="footnote-ref" id="fnref132" role="doc-noteref"><sup>132</sup></a>于 2021 年 4 月在 ArXiv 上发表，并对 PP-YOLO 进行了四项改进，将 NVIDIA V100 上 69 FPS 的 AP 性能从 45.9%提高到 49.5%。PP-YOLOv2 对 PP-YOLO 的修改如下:</p><ol type="1"><li><p>骨干网从 ResNet50 修改为 ResNet101。</p></li><li><p>PAN (Path aggregation network)，替代类似于 YOLOv4 的 FPN。</p></li><li><p>Mish 激活函数。与 YOLOv4 和 YOLOv5 不同的是，它们仅在检测颈部应用了 mish 激活功能，使主干与 ReLU 保持不变。</p></li><li><p>较大的输入尺寸有助于提高小对象的性能。他们将最大的输入大小从 608 扩展到 768，并将每个 GPU 的批处理大小从 24 张减少到 12 张。输入大小从[320,352,384,416,448,480,512,544,576,608,640,672,704,736,768]中均匀抽取。</p></li><li><p>修改后的 IoU 感知分支。他们修改了 IoU 感知损失计算的计算，使用软标签格式代替软权重格式。</p></li></ol><h3 id="pp-yoloe">17.4 PP-YOLOE</h3><p>PP-YOLOE<a href="#fn133" class="footnote-ref" id="fnref133" role="doc-noteref"><sup>133</sup></a>于 2022 年 3 月在 ArXiv 上发表。它对 PP-YOLOv2 进行了改进，在 NVIDIA V100 上以 78.1 FPS 实现了 51.4% AP 的性能。PP-YOLOE 对 PP-YOLOv2 的主要变化有:</p><ol type="1"><li><p>Anchor-free。PP-YOLOE 遵循<a href="#fn134" class="footnote-ref" id="fnref134" role="doc-noteref"><sup>134</sup></a><a href="#fn135" class="footnote-ref" id="fnref135" role="doc-noteref"><sup>135</sup></a><a href="#fn136" class="footnote-ref" id="fnref136" role="doc-noteref"><sup>136</sup></a><a href="#fn137" class="footnote-ref" id="fnref137" role="doc-noteref"><sup>137</sup></a>作品所引领的时代潮流，采用无锚式架构。</p></li><li><p>新的 backbone 和 neck。受 TreeNet<a href="#fn138" class="footnote-ref" id="fnref138" role="doc-noteref"><sup>138</sup></a>的启发，作者使用结合残差和密集连接的 RepResBlocks 修改了脊柱和颈部的结构。</p></li><li><p>任务一致性学习(TAL)。YOLOX 首先提出了任务错位问题，即分类置信度和定位精度在所有情况下都不一致。为了减少这个问题，PP-YOLOE 实现了 TOOD<a href="#fn139" class="footnote-ref" id="fnref139" role="doc-noteref"><sup>139</sup></a>中提出的 TAL，其中包括动态标签分配和任务对齐损失。</p></li><li><p>高效任务对齐头(ET-head)。与 YOLOX 的分类和定位磁头分离不同，PP-YOLOE 使用基于 ood 的单个磁头来提高速度和准确性。</p></li><li><p>变焦(VFL)和分布焦损失(DFL)。VFL<a href="#fn140" class="footnote-ref" id="fnref140" role="doc-noteref"><sup>140</sup></a>使用目标分数对阳性样本进行权重损失，对 IoU 高的样本给予更高的权重。这在训练期间优先考虑高质量的样本。同样，两者都使用 IACS 作为目标，允许对分类和定位质量进行联合学习，从而实现训练和推理的一致性。另一方面，DFL<a href="#fn141" class="footnote-ref" id="fnref141" role="doc-noteref"><sup>141</sup></a>将 Focal Loss 从离散标签扩展到连续标签，从而成功地优化了结合质量估计和类别预测的改进表示。这允许在实际数据中准确地描述灵活的分布，从而消除不一致的风险。</p></li></ol><p>与之前的 YOLO 版本一样，作者通过改变脊柱和颈部的宽度和深度来生成多个缩放模型。这些型号分别为 PP-YOLOE-s(小型)、PP-YOLOE-m(中型)、PP-YOLOE-l(大型)和 PP-YOLOE-x(超大型)。</p><h2 id="discussion">18 discussion</h2><p>本文检查了 15 个 YOLO 版本，从最初的 YOLO 模型到最新的 YOLOv8。表 4 提供了所讨论的 YOLO 版本的概述。从这个表中，我们可以识别出几个关键模式:</p><ul><li><p>锚点:最初的 YOLO 模型相对简单，没有使用锚点，而目前的状态依赖于带锚点的两级探测器。YOLOv2 加入了锚点，从而提高了边界框预测的准确性。这种趋势持续了 5 年，直到 YOLOX 推出了一种无锚的方法，取得了最先进的效果。从那时起，后续的 YOLO 版本已经放弃了锚的使用。</p></li><li><p>框架:最初，YOLO 是使用 Darknet 框架开发的，随后的版本也紧随其后。然而，当 Ultralytics 将 YOLOv3 移植到 PyTorch 时，剩余的 YOLO 版本是使用 PyTorch 开发的，从而导致增强的激增。另一种使用的深度学习语言是 PaddlePaddle，这是一个最初由百度开发的开源框架。</p></li><li><p>骨干:随着时间的推移，YOLO 模型的骨干架构发生了重大变化。从包含简单卷积层和最大池化层的 Darknet 架构开始，后来的模型在 YOLOv4 中加入了跨阶段部分连接(CSP)，在 YOLOv6 和 YOLOv7 中加入了重新参数化，在 DAMO-YOLO 中加入了神经结构搜索。</p></li><li><p>性能:虽然 YOLO 模型的性能随着时间的推移有所提高，但值得注意的是，它们通常优先考虑速度和准确性的平衡，而不是仅仅关注准确性。这种权衡是 YOLO 框架的一个重要方面，它允许跨各种应用程序进行实时对象检测。</p></li></ul><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214214908034.png" alt="image-20231214214908034" style="zoom:80%;" /></p><h3 id="在速度和准确性之间进行权衡">18.1 在速度和准确性之间进行权衡</h3><p>YOLO 系列目标检测模型一直专注于平衡速度和准确性，旨在提供实时性能而不牺牲检测结果的质量。随着 YOLO 框架在各种迭代中不断发展，这种权衡一直是一个反复出现的主题，每个版本都以不同的方式寻求优化这些相互竞争的目标。在最初的 YOLO 模型中，主要关注的是实现高速目标检测。该模型利用单个卷积神经网络(CNN)直接从输入图像中预测物体的位置和类别，从而实现实时处理。然而，这种对速度的强调导致了准确性的妥协，主要是在处理小对象或具有重叠边界框的对象时。</p><p>随后的 YOLO 版本引入了改进和增强，以解决这些限制，同时保持框架的实时功能。例如，YOLOv2 (YOLO9000)引入锚盒和穿透层来改善物体的定位，从而提高精度。此外，YOLOv3 通过采用多尺度特征提取架构增强了模型的性能，从而可以在各种尺度上更好地检测目标。</p><p>随着 YOLO 框架的发展，速度和准确性之间的权衡变得更加微妙。像 YOLOv4 和 YOLOv5 这样的模型引入了创新，比如新的网络主干网、改进的数据增强技术和优化的训练策略。这些发展在不显著影响模型实时性能的情况下显著提高了精度。</p><p>从缩放 yolo4 开始，所有官方 YOLO 模型都对速度和精度之间的权衡进行了微调，提供不同的模型尺度以适应特定的应用和硬件要求。例如，这些版本通常提供针对边缘设备优化的轻量级模型，以准确性换取降低的计算复杂性和更快的处理时间。</p><h2 id="yolo-未来">19 YOLO 未来</h2><p>随着 YOLO 框架的不断发展，我们预计以下趋势和可能性将影响未来的发展:</p><ol type="1"><li><p>研究人员和开发人员将利用深度学习、数据增强和训练技术方面的最新方法，继续完善 YOLO 架构。这个持续的创新过程可能会提高模型的性能、健壮性和效率。</p></li><li><p>基准进化。目前用于评估目标检测模型的基准，COCO 2017，最终可能会被一个更先进、更具挑战性的基准所取代。这反映了前两个 YOLO 版本中使用的 VOC 2007 基准的转变，反映了随着模型变得更加复杂和准确，对更高要求的基准的需求。</p></li><li><p>YOLO 模型及其应用的扩展。随着 YOLO 框架的发展，我们预计每年发布的 YOLO 模型数量将会增加，应用范围也会相应扩大。随着该框架变得更加通用和强大，它可能会被应用于更广泛的领域，从家用电器设备到自动驾驶汽车。</p></li><li><p>扩展到新的领域。YOLO 模型有潜力将其功能扩展到物体检测和分割之外，扩展到视频中的物体跟踪和 3D 关键点估计等领域。随着这些模型的发展，它们可能成为解决更广泛的计算机视觉任务的新解决方案的基础。</p></li><li><p>对各种硬件的适应性。YOLO 模型将进一步跨越硬件平台，从物联网设备到高性能计算集群。这种适应性将支持在各种上下文中部署 YOLO 模型，具体取决于应用程序的需求和约束。此外，通过定制模型以适应不同的硬件规格，YOLO 可以为更多的用户和行业提供访问和有效的服务。</p></li></ol><h2 id="reference">reference</h2><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>W. Lan, J. Dang, Y . Wang, and S. Wang, “Pedestrian detection based on yolo network model,” in 2018 IEEE international conference on mechatronics and automation (ICMA), pp. 1547–1551, IEEE, 2018.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2" role="doc-endnote"><p>W.-Y . Hsu and W.-Y . Lin, “Adaptive fusion of multi-scale yolo for pedestrian detection,” IEEE Access, vol. 9, pp. 110063–110073, 2021.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3" role="doc-endnote"><p>A. Benjumea, I. Teeti, F. Cuzzolin, and A. Bradley, “Y olo-z: Improving small object detection in yolov5 for autonomous vehicles,” arXiv preprint arXiv:2112.11798, 2021.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4" role="doc-endnote"><p>N. M. A. A. Dazlee, S. A. Khalil, S. Abdul-Rahman, and S. Mutalib, “Object detection for autonomous vehicles with sensor-based technology using yolo,” International Journal of Intelligent Systems and Applications in Engineering, vol. 10, no. 1, pp. 129–134, 2022.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn5" role="doc-endnote"><p>S. Liang, H. Wu, L. Zhen, Q. Hua, S. Garg, G. Kaddoum, M. M. Hassan, and K. Y u, “Edge yolo: Real-time intelligent object detection system based on edge-cloud cooperation in autonomous vehicles,” IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 12, pp. 25345–25360, 2022.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn6" role="doc-endnote"><p>Q. Li, X. Ding, X. Wang, L. Chen, J. Son, and J.-Y . Song, “Detection and identification of moving objects at busy traffic road based on yolo v4,” The Journal of the Institute of Internet, Broadcasting and Communication, vol. 21, no. 1, pp. 141–148, 2021.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn7" role="doc-endnote"><p>S. Shinde, A. Kothari, and V . Gupta, “Y olo based human action recognition and localization,” Procedia computer science, vol. 133, pp. 831–838, 2018.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn8" role="doc-endnote"><p>A. H. Ashraf, M. Imran, A. M. Qahtani, A. Alsufyani, O. Almutiry, A. Mahmood, M. Attique, and M. Habib, “Weapons detection for security and video surveillance using cnn and yolo-v5s,” CMC-Comput. Mater . Contin, vol. 70, pp. 2761–2775, 2022.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn9" role="doc-endnote"><p>Y . Zheng and H. Zhang, “Video analysis in sports by lightweight object detection network under the background of sports industry development,” Computational Intelligence and Neuroscience, vol. 2022, 2022.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn10" role="doc-endnote"><p>H. Ma, T. Celik, and H. Li, “Fer-yolo: Detection and classification based on facial expressions,” in Image and Graphics: 11th International Conference, ICIG 2021, Haikou, China, August 6–8, 2021, Proceedings, Part I 11, pp. 28–39, Springer, 2021.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn11" role="doc-endnote"><p>Y . Tian, G. Yang, Z. Wang, H. Wang, E. Li, and Z. Liang, “Apple detection during different growth stages in orchards using the improved yolo-v3 model,” Computers and electronics in agriculture, vol. 157, pp. 417–426, 2019.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn12" role="doc-endnote"><p>D. Wu, S. Lv, M. Jiang, and H. Song, “Using channel pruning-based yolo v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments,” Computers and Electronics in Agriculture, vol. 178, p. 105742, 2020.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn13" role="doc-endnote"><p>M. Lippi, N. Bonucci, R. F. Carpio, M. Contarini, S. Speranza, and A. Gasparri, “A yolo-based pest detection system for precision agriculture,” in 2021 29th Mediterranean Conference on Control and Automation (MED), pp. 342–347, IEEE, 2021.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn14" role="doc-endnote"><p>W. Y ang and Z. Jiachun, “Real-time face detection based on yolo,” in 2018 1st IEEE international conference on knowledge innovation and invention (ICKII), pp. 221–224, IEEE, 2018.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn15" role="doc-endnote"><p>W. Chen, H. Huang, S. Peng, C. Zhou, and C. Zhang, “Y olo-face: a real-time face detector,” The Visual Computer, vol. 37, pp. 805–813, 2021.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn16" role="doc-endnote"><p>M. A. Al-Masni, M. A. Al-Antari, J.-M. Park, G. Gi, T.-Y . Kim, P . Rivera, E. V alarezo, M.-T. Choi, S.-M. Han, and T.-S. Kim, “Simultaneous detection and classification of breast masses in digital mammograms via a deep learning yolo-based cad system,” Computer methods and programs in biomedicine, vol. 157, pp. 85–94, 2018.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn17" role="doc-endnote"><p>Y . Nie, P . Sommella, M. O’Nils, C. Liguori, and J. Lundgren, “Automatic detection of melanoma with yolo deep convolutional neural networks,” in 2019 E-Health and Bioengineering Conference (EHB), pp. 1–4, IEEE, 2019.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn18" role="doc-endnote"><p>H. M. Ünver and E. Ayan, “Skin lesion segmentation in dermoscopic images with combination of yolo and grabcut algorithm,” Diagnostics, vol. 9, no. 3, p. 72, 2019.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn19" role="doc-endnote"><p>L. Tan, T. Huangfu, L. Wu, and W. Chen, “Comparison of retinanet, ssd, and yolo v3 for real-time pill identification,” BMC medical informatics and decision making, vol. 21, pp. 1–11, 2021.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn20" role="doc-endnote"><p>L. Cheng, J. Li, P . Duan, and M. Wang, “A small attentional yolo model for landslide detection from satellite remote sensing images,” Landslides, vol. 18, no. 8, pp. 2751–2765, 2021.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn21" role="doc-endnote"><p>M.-T. Pham, L. Courtrai, C. Friguet, S. Lefèvre, and A. Baussard, “Y olo-fine: One-stage detector of small objects under various backgrounds in remote sensing images,” Remote Sensing, vol. 12, no. 15, p. 2501, 2020.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn22" role="doc-endnote"><p>Y . Qing, W. Liu, L. Feng, and W. Gao, “Improved yolo network for free-angle remote sensing target detection,” Remote Sensing, vol. 13, no. 11, p. 2171, 2021.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn23" role="doc-endnote"><p>Z. Zakria, J. Deng, R. Kumar, M. S. Khokhar, J. Cai, and J. Kumar, “Multiscale and direction target detecting in remote sensing images via modified yolo-v4,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 15, pp. 1039–1048, 2022.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn24" role="doc-endnote"><p>P . Kumar, S. Narasimha Swamy, P . Kumar, G. Purohit, and K. S. Raju, “Real-time, yolo-based intelligent surveillance and monitoring system using jetson tx2,” in Data Analytics and Management: Proceedings of ICDAM, pp. 461–471, Springer, 2021.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn25" role="doc-endnote"><p>K. Bhambani, T. Jain, and K. A. Sultanpure, “Real-time face mask and social distancing violation detection system using yolo,” in 2020 IEEE Bangalore humanitarian technology conference (B-HTC), pp. 1–6, IEEE, 2020.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn26" role="doc-endnote"><p>J. Li, Z. Su, J. Geng, and Y . Yin, “Real-time detection of steel strip surface defects based on improved yolo detection network,” IF AC-PapersOnLine, vol. 51, no. 21, pp. 76–81, 2018.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn27" role="doc-endnote"><p>E. N. Ukhwah, E. M. Y uniarno, and Y . K. Suprapto, “Asphalt pavement pothole detection using deep learning method based on yolo neural network,” in 2019 International Seminar on Intelligent Technology and Its Applications (ISITIA), pp. 35–40, IEEE, 2019.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn28" role="doc-endnote"><p>Y . Du, N. Pan, Z. Xu, F. Deng, Y . Shen, and H. Kang, “Pavement distress detection and classification based on yolo network,” International Journal of Pavement Engineering, vol. 22, no. 13, pp. 1659–1672, 2021.<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn29" role="doc-endnote"><p>R.-C. Chen et al., “Automatic license plate recognition via sliding-window darknet-yolo deep learning,” Image and Vision Computing, vol. 87, pp. 47–56, 2019.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn30" role="doc-endnote"><p>C. Dewi, R.-C. Chen, X. Jiang, and H. Y u, “Deep convolutional neural network for enhancing traffic sign recognition developed on yolo v4,” Multimedia Tools and Applications, vol. 81, no. 26, pp. 37821–37845, 2022.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn31" role="doc-endnote"><p>A. M. Roy, J. Bhaduri, T. Kumar, and K. Raj, “Wildect-yolo: An efficient and robust computer vision-based accurate object localization model for automated endangered wildlife detection,” Ecological Informatics, vol. 75, p. 101919, 2023.<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn32" role="doc-endnote"><p>S. Kulik and A. Shtanko, “Experiments with neural net object detection system yolo on small training datasets for intelligent robotics,” in Advanced Technologies in Robotics and Intelligent Systems: Proceedings of ITR 2019, pp. 157–162, Springer, 2020.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn33" role="doc-endnote"><p>D. H. Dos Reis, D. Welfer, M. A. De Souza Leite Cuadros, and D. F. T. Gamarra, “Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm,” Applied Artificial Intelligence, vol. 33, no. 14, pp. 1290–1305, 2019.<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn34" role="doc-endnote"><p>O. Sahin and S. Ozer, “Y olodrone: Improved yolo architecture for object detection in drone images,” in 2021 44th International Conference on Telecommunications and Signal Processing (TSP), pp. 361–365, IEEE, 2021.<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn35" role="doc-endnote"><p>C. Chen, Z. Zheng, T. Xu, S. Guo, S. Feng, W. Yao, and Y . Lan, “Y olo-based uav technology: A review of the research and its applications,” Drones, vol. 7, no. 3, p. 190, 2023.<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn36" role="doc-endnote"><p>M. Everingham, L. V an Gool, C. K. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc)challenge,” International journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn37" role="doc-endnote"><p>T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in European conference on computer vision, pp. 740–755, Springer, 2014.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn38" role="doc-endnote"><p>J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788, 2016.<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn39" role="doc-endnote"><p>R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, pp. 1440–1448, 2015.<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn40" role="doc-endnote"><p>A. L. Maas, A. Y . Hannun, A. Y . Ng, et al., “Rectifier nonlinearities improve neural network acoustic models,” in Proc. icml, vol. 30, p. 3, Atlanta, Georgia, USA, 2013.<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn41" role="doc-endnote"><p>C. Szegedy, W. Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan, V . V anhoucke, and A. Rabinovich, “Going deeper with convolutions,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn42" role="doc-endnote"><p>M. Lin, Q. Chen, and S. Y an, “Network in network,” arXiv preprint arXiv:1312.4400, 2013.<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn43" role="doc-endnote"><p>O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., “Imagenet large scale visual recognition challenge,” International journal of computer vision, vol. 115, no. 3, pp. 211–252, 2015.<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn44" role="doc-endnote"><p>M. Everingham, L. V an Gool, C. K. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc)challenge,” International journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn45" role="doc-endnote"><p>R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, pp. 1440–1448, 2015.<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn46" role="doc-endnote"><p>J. Redmon and A. Farhadi, “Y olo9000: better, faster, stronger,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263–7271, 2017.<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn47" role="doc-endnote"><p>S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” Advances in neural information processing systems, vol. 28, 2015.<a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn48" role="doc-endnote"><p>M. Lin, Q. Chen, and S. Y an, “Network in network,” arXiv preprint arXiv:1312.4400, 2013.<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn49" role="doc-endnote"><p>T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in European conference on computer vision, pp. 740–755, Springer, 2014.<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn50" role="doc-endnote"><p>J. Redmon and A. Farhadi, “Y olov3: An incremental improvement,” arXiv preprint arXiv:1804.02767, 2018.<a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn51" role="doc-endnote"><p>S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” Advances in neural information processing systems, vol. 28, 2015.<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn52" role="doc-endnote"><p>I. Krasin, T. Duerig, N. Alldrin, V . Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. V eit, et al., “Openimages: A public dataset for large-scale multi-label and multi-class image classification,” Dataset available from https://github. com/openimages, vol. 2, no. 3, p. 18, 2017.<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn53" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.<a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn54" role="doc-endnote"><p>T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.<a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn55" role="doc-endnote"><p>T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in European conference on computer vision, pp. 740–755, Springer, 2014.<a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn56" role="doc-endnote"><p>T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.<a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn57" role="doc-endnote"><p>A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, “Y olov4: Optimal speed and accuracy of object detection,” arXiv preprint arXiv:2004.10934, 2020.<a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn58" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.<a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn59" role="doc-endnote"><p>L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Y uille, “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,” IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834–848, 2017.<a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn60" role="doc-endnote"><p>S. Liu, D. Huang, et al., “Receptive field block net for accurate and fast object detection,” in Proceedings of the European conference on computer vision (ECCV), pp. 385–400, 2018.<a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn61" role="doc-endnote"><p>T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.<a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn62" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.<a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn63" role="doc-endnote"><p>B. Hariharan, P . Arbeláez, R. Girshick, and J. Malik, “Hypercolumns for object segmentation and fine-grained localization,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 447–456, 2015.<a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn64" role="doc-endnote"><p>Q. Zhao, T. Sheng, Y . Wang, Z. Tang, Y . Chen, L. Cai, and H. Ling, “M2det: A single-shot object detector based on multi-level feature pyramid network,” in Proceedings of the AAAI conference on artificial intelligence, vol. 33, pp. 9259–9266, 2019.<a href="#fnref64" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn65" role="doc-endnote"><p>A. L. Maas, A. Y . Hannun, A. Y . Ng, et al., “Rectifier nonlinearities improve neural network acoustic models,” in Proc. icml, vol. 30, p. 3, Atlanta, Georgia, USA, 2013.<a href="#fnref65" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn66" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015.<a href="#fnref66" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn67" role="doc-endnote"><p>D. Misra, “Mish: A self regularized non-monotonic neural activation function,” arXiv preprint arXiv:1908.08681, vol. 4, no. 2, pp. 10–48550, 2019.<a href="#fnref67" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn68" role="doc-endnote"><p>N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, “Soft-nms–improving object detection with one line of code,” in Proceedings of the IEEE international conference on computer vision, pp. 5561–5569, 2017.<a href="#fnref68" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn69" role="doc-endnote"><p>S. Wang, J. Zhao, N. Ta, X. Zhao, M. Xiao, and H. Wei, “A real-time deep learning forest fire monitoring algorithm based on an improved pruned+ kd model,” Journal of Real-Time Image Processing, vol. 18, no. 6, pp. 2319–2329, 2021.<a href="#fnref69" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn70" role="doc-endnote"><p>S. Xie, R. Girshick, P . Dollár, Z. Tu, and K. He, “Aggregated residual transformations for deep neural networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492–1500, 2017.<a href="#fnref70" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn71" role="doc-endnote"><p>M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,” in International conference on machine learning, pp. 6105–6114, PMLR, 2019.<a href="#fnref71" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn72" role="doc-endnote"><p>D. Misra, “Mish: A self regularized non-monotonic neural activation function,” arXiv preprint arXiv:1908.08681, vol. 4, no. 2, pp. 10–48550, 2019.<a href="#fnref72" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn73" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.<a href="#fnref73" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn74" role="doc-endnote"><p>S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8759–8768, 2018.<a href="#fnref74" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn75" role="doc-endnote"><p>S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional block attention module,” in Proceedings of the European conference on computer vision (ECCV), pp. 3–19, 2018.<a href="#fnref75" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn76" role="doc-endnote"><p>G. Ghiasi, T.-Y . Lin, and Q. V . Le, “Dropblock: A regularization method for convolutional networks,” Advances in neural information processing systems, vol. 31, 2018.<a href="#fnref76" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn77" role="doc-endnote"><p>N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple way to prevent neural networks from overfitting,” The journal of machine learning research, vol. 15, no. 1, pp. 1929–1958, 2014.<a href="#fnref77" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn78" role="doc-endnote"><p>C. Szegedy, V . V anhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the inception architecture for computer vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016.<a href="#fnref78" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn79" role="doc-endnote"><p>M. A. Islam, S. Naha, M. Rochan, N. Bruce, and Y . Wang, “Label refinement network for coarse-to-fine semantic segmentation,” arXiv preprint arXiv:1703.00551, 2017.<a href="#fnref79" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn80" role="doc-endnote"><p>Z. Zheng, P . Wang, W. Liu, J. Li, R. Y e, and D. Ren, “Distance-iou loss: Faster and better learning for bounding box regression,” in Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 12993–13000, 2020.<a href="#fnref80" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn81" role="doc-endnote"><p>S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in International conference on machine learning, pp. 448–456, PMLR, 2015.<a href="#fnref81" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn82" role="doc-endnote"><p>I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent with warm restarts,” arXiv preprint arXiv:1608.03983, 2016.<a href="#fnref82" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn83" role="doc-endnote"><p>G. Jocher, “YOLOv5 by Ultralytics.” https://github.com/ultralytics/yolov5, 2020. Accessed: February 30, 2023.<a href="#fnref83" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn84" role="doc-endnote"><p>C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao, “Scaled-yolov4: Scaling cross stage partial network,” in Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 13029–13038, 2021.<a href="#fnref84" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn85" role="doc-endnote"><p>M. Tan, R. Pang, and Q. V . Le, “Efficientdet: Scalable and efficient object detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020.<a href="#fnref85" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn86" role="doc-endnote"><p>T.-Y . Lin, P . Goyal, R. Girshick, K. He, and P . Dollár, “Focal loss for dense object detection,” in Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017.<a href="#fnref86" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn87" role="doc-endnote"><p>X. Long, K. Deng, G. Wang, Y . Zhang, Q. Dang, Y . Gao, H. Shen, J. Ren, S. Han, E. Ding, et al., “Pp-yolo: An effective and efficient implementation of object detector,” arXiv preprint arXiv:2007.12099, 2020.<a href="#fnref87" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn88" role="doc-endnote"><p>C.-Y . Wang, I.-H. Yeh, and H.-Y . M. Liao, “Y ou only learn one representation: Unified network for multiple tasks,” arXiv preprint arXiv:2105.04206, 2021.<a href="#fnref88" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn89" role="doc-endnote"><p>Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Y olox: Exceeding yolo series in 2021,” arXiv preprint arXiv:2107.08430, 2021.<a href="#fnref89" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn90" role="doc-endnote"><p>H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,” in Proceedings of the European conference on computer vision (ECCV), pp. 734–750, 2018.<a href="#fnref90" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn91" role="doc-endnote"><p>K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet: Keypoint triplets for object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6569–6578, 2019.<a href="#fnref91" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn92" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref92" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn93" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref93" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn94" role="doc-endnote"><p>G. Song, Y . Liu, and X. Wang, “Revisiting the sibling head in object detector,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11563–11572, 2020.<a href="#fnref94" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn95" role="doc-endnote"><p>Y . Wu, Y . Chen, L. Y uan, Z. Liu, L. Wang, H. Li, and Y . Fu, “Rethinking classification and localization for object detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10186–10195, 2020.<a href="#fnref95" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn96" role="doc-endnote"><p>Z. Ge, S. Liu, Z. Li, O. Y oshie, and J. Sun, “Ota: Optimal transport assignment for object detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 303–312, 2021.<a href="#fnref96" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn97" role="doc-endnote"><p>H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.<a href="#fnref97" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn98" role="doc-endnote"><p>C. Li, L. Li, H. Jiang, K. Weng, Y . Geng, L. Li, Z. Ke, Q. Li, M. Cheng, W. Nie, et al., “Y olov6: A single-stage object detection framework for industrial applications,” arXiv preprint arXiv:2209.02976, 2022.<a href="#fnref98" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn99" role="doc-endnote"><p>Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Y olox: Exceeding yolo series in 2021,” arXiv preprint arXiv:2107.08430, 2021.<a href="#fnref99" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn100" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref100" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn101" role="doc-endnote"><p>X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making vgg-style convnets great again,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13733–13742, 2021.<a href="#fnref101" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn102" role="doc-endnote"><p>S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8759–8768, 2018.<a href="#fnref102" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn103" role="doc-endnote"><p>X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making vgg-style convnets great again,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13733–13742, 2021.<a href="#fnref103" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn104" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, Y .-H. Wu, P .-Y . Chen, J.-W. Hsieh, and I.-H. Y eh, “Cspnet: A new backbone that can enhance learning capability of cnn,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 390–391, 2020.<a href="#fnref104" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn105" role="doc-endnote"><p>C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object detection,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3490–3499, IEEE Computer Society, 2021.<a href="#fnref105" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn106" role="doc-endnote"><p>H. Zhang, Y . Wang, F. Dayoub, and N. Sunderhauf, “V arifocalnet: An iou-aware dense object detector,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8514–8523, 2021.<a href="#fnref106" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn107" role="doc-endnote"><p>Z. Gevorgyan, “Siou loss: More powerful learning for bounding box regression,” arXiv preprint arXiv:2205.12740, 2022.<a href="#fnref107" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn108" role="doc-endnote"><p>H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, “Generalized intersection over union: A metric and a loss for bounding box regression,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 658–666, 2019.<a href="#fnref108" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn109" role="doc-endnote"><p>X. Ding, H. Chen, X. Zhang, K. Huang, J. Han, and G. Ding, “Re-parameterizing your optimizers rather than architectures,” arXiv preprint arXiv:2205.15242, 2022.<a href="#fnref109" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn110" role="doc-endnote"><p>C. Shu, Y . Liu, J. Gao, Z. Yan, and C. Shen, “Channel-wise knowledge distillation for dense prediction,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5311–5320, 2021.<a href="#fnref110" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn111" role="doc-endnote"><p>C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao, “Y olov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,” arXiv preprint arXiv:2207.02696, 2022.<a href="#fnref111" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn112" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, and I.-H. Y eh, “Designing network design strategies through gradient path analysis,” arXiv preprint arXiv:2211.04800, 2022.<a href="#fnref112" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn113" role="doc-endnote"><p>X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making vgg-style convnets great again,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13733–13742, 2021.<a href="#fnref113" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn114" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.<a href="#fnref114" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn115" role="doc-endnote"><p>G. Huang, Z. Liu, L. V an Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.<a href="#fnref115" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn116" role="doc-endnote"><p>C.-Y . Wang, I.-H. Yeh, and H.-Y . M. Liao, “Y ou only learn one representation: Unified network for multiple tasks,” arXiv preprint arXiv:2105.04206, 2021.<a href="#fnref116" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn117" role="doc-endnote"><p>X. Xu, Y . Jiang, W. Chen, Y . Huang, Y . Zhang, and X. Sun, “Damo-yolo: A report on real-time object detection design,” arXiv preprint arXiv:2211.15444, 2022.<a href="#fnref117" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn118" role="doc-endnote"><p>Alibaba, “TinyNAS.” https://github.com/alibaba/lightweight-neural-architecture-search,2023. Accessed: March 18, 2023.<a href="#fnref118" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn119" role="doc-endnote"><p>Z. Tan, J. Wang, X. Sun, M. Lin, H. Li, et al., “Giraffedet: A heavy-neck paradigm for object detection,” in International Conference on Learning Representations, 2021.<a href="#fnref119" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn120" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, Y .-H. Wu, P .-Y . Chen, J.-W. Hsieh, and I.-H. Y eh, “Cspnet: A new backbone that can enhance learning capability of cnn,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 390–391, 2020.<a href="#fnref120" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn121" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, and I.-H. Y eh, “Designing network design strategies through gradient path analysis,” arXiv preprint arXiv:2211.04800, 2022.<a href="#fnref121" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn122" role="doc-endnote"><p>Z. Ge, S. Liu, Z. Li, O. Y oshie, and J. Sun, “Ota: Optimal transport assignment for object detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 303–312, 2021.<a href="#fnref122" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn123" role="doc-endnote"><p>C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object detection,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3490–3499, IEEE Computer Society, 2021.<a href="#fnref123" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn124" role="doc-endnote"><p>T.-Y . Lin, P . Goyal, R. Girshick, K. He, and P . Dollár, “Focal loss for dense object detection,” in Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017.<a href="#fnref124" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn125" role="doc-endnote"><p>G. Jocher, A. Chaurasia, and J. Qiu, “YOLO by Ultralytics.” https://github.com/ultralytics/ultralytics, 2023. Accessed: February 30, 2023.<a href="#fnref125" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn126" role="doc-endnote"><p>X. Long, K. Deng, G. Wang, Y . Zhang, Q. Dang, Y . Gao, H. Shen, J. Ren, S. Han, E. Ding, et al., “Pp-yolo: An effective and efficient implementation of object detector,” arXiv preprint arXiv:2007.12099, 2020.<a href="#fnref126" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn127" role="doc-endnote"><p>Y . Ma, D. Y u, T. Wu, and H. Wang, “Paddlepaddle: An open-source deep learning platform from industrial practice,” Frontiers of Data and Domputing, vol. 1, no. 1, pp. 105–115, 2019.<a href="#fnref127" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn128" role="doc-endnote"><p>J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei, “Deformable convolutional networks,” in Proceedings of the IEEE international conference on computer vision, pp. 764–773, 2017.<a href="#fnref128" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn129" role="doc-endnote"><p>W. Xinlong, Z. Rufeng, K. Tao, L. Lei, and S. Chunhua, “Solov2: Dynamic, faster and stronger,” in Proc. NIPS, 2020.<a href="#fnref129" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn130" role="doc-endnote"><p>R. Liu, J. Lehman, P . Molino, F. Petroski Such, E. Frank, A. Sergeev, and J. Y osinski, “An intriguing failing of convolutional neural networks and the coordconv solution,” Advances in neural information processing systems, vol. 31, 2018.<a href="#fnref130" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn131" role="doc-endnote"><p>H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.<a href="#fnref131" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn132" role="doc-endnote"><p>X. Huang, X. Wang, W. Lv, X. Bai, X. Long, K. Deng, Q. Dang, S. Han, Q. Liu, X. Hu, et al., “Pp-yolov2: A practical object detector,” arXiv preprint arXiv:2104.10419, 2021.<a href="#fnref132" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn133" role="doc-endnote"><p>S. Xu, X. Wang, W. Lv, Q. Chang, C. Cui, K. Deng, G. Wang, Q. Dang, S. Wei, Y . Du, et al., “Pp-yoloe: An evolved version of yolo,” arXiv preprint arXiv:2203.16250, 2022.<a href="#fnref133" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn134" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref134" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn135" role="doc-endnote"><p>K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet: Keypoint triplets for object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6569–6578, 2019.<a href="#fnref135" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn136" role="doc-endnote"><p>H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,” in Proceedings of the European conference on computer vision (ECCV), pp. 734–750, 2018.<a href="#fnref136" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn137" role="doc-endnote"><p>Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Y olox: Exceeding yolo series in 2021,” arXiv preprint arXiv:2107.08430, 2021.<a href="#fnref137" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn138" role="doc-endnote"><p>L. Rao, “Treenet: A lightweight one-shot aggregation convolutional network,” arXiv preprint arXiv:2109.12342, 2021.<a href="#fnref138" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn139" role="doc-endnote"><p>C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object detection,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3490–3499, IEEE Computer Society, 2021.<a href="#fnref139" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn140" role="doc-endnote"><p>H. Zhang, Y . Wang, F. Dayoub, and N. Sunderhauf, “V arifocalnet: An iou-aware dense object detector,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8514–8523, 2021.<a href="#fnref140" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn141" role="doc-endnote"><p>X. Li, W. Wang, L. Wu, S. Chen, X. Hu, J. Li, J. Tang, and J. Y ang, “Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection,” Advances in Neural Information Processing Systems, vol. 33, pp. 21002–21012, 2020.<a href="#fnref141" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;a-comprehensive-review-of-yolo-from-yolov1-to-yolov8-and-beyond&quot;&gt;A COMPREHENSIVE REVIEW OF YOLO: FROM YOLOV1 TO YOLOV8 AND BEYOND&lt;/h</summary>
      
    
    
    
    <category term="yolo" scheme="https://wangtongyouwen.github.io/categories/yolo/"/>
    
    <category term="summary" scheme="https://wangtongyouwen.github.io/categories/yolo/summary/"/>
    
    
    <category term="yolo" scheme="https://wangtongyouwen.github.io/tags/yolo/"/>
    
    <category term="summary" scheme="https://wangtongyouwen.github.io/tags/summary/"/>
    
  </entry>
  
  <entry>
    <title>强化学习1-无状态问题</title>
    <link href="https://wangtongyouwen.github.io/post/800a73d6.html"/>
    <id>https://wangtongyouwen.github.io/post/800a73d6.html</id>
    <published>2023-06-16T02:48:01.000Z</published>
    <updated>2023-12-14T13:52:42.758Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述">问题描述</h2><p>探索 + 利用</p><ul><li>构建中奖概率，以及中奖的期望</li></ul><figure class="highlight plaintext"><figcaption><span>notebook</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#每个老虎机的中奖概率,0-1之间的均匀分布</span><br><span class="line">probs = np.random.uniform(size=10)</span><br><span class="line"></span><br><span class="line">min_num = np.argmin(probs)</span><br><span class="line">temp = probs[0]</span><br><span class="line">probs[0] = probs[min_num]</span><br><span class="line">probs[min_num]=temp</span><br><span class="line"></span><br><span class="line">#记录每个老虎机的返回值，reward期望</span><br><span class="line">rewards = [[1] for _ in range(10)]</span><br><span class="line"></span><br><span class="line">probs, rewards</span><br></pre></td></tr></table></figure><ul><li>尝试一次</li></ul><figure class="highlight plaintext"><figcaption><span>notebook</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def try_and_play():</span><br><span class="line">    i = choose_one()</span><br><span class="line"></span><br><span class="line">    #玩老虎机,得到结果</span><br><span class="line">    reward = 0</span><br><span class="line">    if random.random() &lt; probs[i]:</span><br><span class="line">        reward = 1</span><br><span class="line"></span><br><span class="line">    #记录玩的结果</span><br><span class="line">    rewards[i].append(reward)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">try_and_play()</span><br><span class="line"></span><br><span class="line">rewards</span><br></pre></td></tr></table></figure><ul><li>尝试5000次，统计结果：</li></ul><figure class="highlight plaintext"><figcaption><span>notebook</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#记录每个老虎机的返回值，reward期望</span><br><span class="line">rewards = [[1] for _ in range(10)]</span><br><span class="line">ef get_result():</span><br><span class="line">    #玩N次</span><br><span class="line">    for _ in range(5000):</span><br><span class="line">        try_and_play()</span><br><span class="line"></span><br><span class="line">    #期望的最好结果</span><br><span class="line">    target = probs.max() * 5000</span><br><span class="line"></span><br><span class="line">    #实际玩出的结果</span><br><span class="line">    result = sum([sum(i) for i in rewards])</span><br><span class="line"></span><br><span class="line">    return target, result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">get_result()</span><br></pre></td></tr></table></figure><h2 id="算法描述">算法描述</h2><h3 id="贪婪算法">1 贪婪算法</h3><ul><li>大概率选择目前中奖率最高的，小概率随机探索</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#贪婪算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#有小概率随机选择一根拉杆</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.01</span>:</span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算每个老虎机的奖励平均</span></span><br><span class="line">    rewards_mean = [np.mean(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#选择期望奖励估值最大的拉杆</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(rewards_mean)</span><br></pre></td></tr></table></figure><h3 id="递减的贪婪算法">2 递减的贪婪算法</h3><ul><li>随着次数的增多，随机选择的概率逐渐下降</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机选择的概率递减的贪婪算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#求出现在已经玩了多少次了</span></span><br><span class="line">    played_count = <span class="built_in">sum</span>([<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#随机选择的概率逐渐下降</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">1</span> / played_count:</span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算每个老虎机的奖励平均</span></span><br><span class="line">    rewards_mean = [np.mean(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#选择期望奖励估值最大的拉杆</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(rewards_mean)</span><br></pre></td></tr></table></figure><h3 id="上界置信函数">3 上界置信函数</h3><p><span class="math display">\[UCB = \frac{\sqrt{\sum\text{played\_count}}}{2*\text{played\_count}}\]</span></p><p>其中<span class="math inline">\(\text{play\_count}\)</span>是一个矩阵，表示每个老虎机玩的次数</p><ul><li>能够衡量每个老虎机玩的次数，玩的次数多会越接近0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机选择的概率递减的贪婪算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#求出每个老虎机各玩了多少次</span></span><br><span class="line">    played_count = [<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line">    played_count = np.array(played_count)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#求出上置信界</span></span><br><span class="line">    <span class="comment">#分子是总共玩了多少次,取根号后让他的增长速度变慢</span></span><br><span class="line">    <span class="comment">#分母是每台老虎机玩的次数,乘以2让他的增长速度变快</span></span><br><span class="line">    <span class="comment">#随着玩的次数增加,分母会很快超过分子的增长速度,导致分数越来越小</span></span><br><span class="line">    <span class="comment">#具体到每一台老虎机,则是玩的次数越多,分数就越小,也就是ucb的加权越小</span></span><br><span class="line">    <span class="comment">#所以ucb衡量了每一台老虎机的不确定性,不确定性越大,探索的价值越大</span></span><br><span class="line">    fenzi = played_count.<span class="built_in">sum</span>()**<span class="number">0.5</span></span><br><span class="line">    fenmu = played_count * <span class="number">2</span></span><br><span class="line">    ucb = fenzi / fenmu</span><br><span class="line"></span><br><span class="line">    <span class="comment">#ucb本身取根号</span></span><br><span class="line">    <span class="comment">#大于1的数会被缩小,小于1的数会被放大,这样保持ucb恒定在一定的数值范围内</span></span><br><span class="line">    ucb = ucb**<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算每个老虎机的奖励平均</span></span><br><span class="line">    rewards_mean = [np.mean(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line">    rewards_mean = np.array(rewards_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#ucb和期望求和</span></span><br><span class="line">    ucb += rewards_mean</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ucb.argmax()</span><br></pre></td></tr></table></figure><h3 id="汤普森采样函数">4 汤普森采样函数</h3><ul><li>介绍一下beta分布：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#beta分布测试</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当数字小的时候,beta分布的概率有很大的随机性&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(np.random.beta(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当数字大时,beta分布逐渐稳定&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(np.random.beta(<span class="number">1e5</span>, <span class="number">1e5</span>))</span><br></pre></td></tr></table></figure><ul><li>当rewards总和小的时候：表示实验的次数少，则分布有很大的随机性，表示大量探索；</li><li>当rewards总和变大的时候，表示实验次数大幅增加，则分布变得稳定，选择概率最大的那一个进行实验</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#求出每个老虎机出1的次数+1</span></span><br><span class="line">    count_1 = [<span class="built_in">sum</span>(i) + <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#求出每个老虎机出0的次数+1</span></span><br><span class="line">    count_0 = [<span class="built_in">sum</span>(<span class="number">1</span> - np.array(i)) + <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#按照beta分布计算奖励分布,这可以认为是每一台老虎机中奖的概率</span></span><br><span class="line">    beta = np.random.beta(count_1, count_0)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> beta.argmax()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt;
&lt;p&gt;探索 + 利用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建中奖概率，以及中奖的期望&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;figcaption&gt;&lt;span&gt;noteboo</summary>
      
    
    
    
    <category term="强化学习" scheme="https://wangtongyouwen.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础" scheme="https://wangtongyouwen.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="强化学习" scheme="https://wangtongyouwen.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础" scheme="https://wangtongyouwen.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>优雅的使用rss打破信息茧房</title>
    <link href="https://wangtongyouwen.github.io/post/3e9dbae2.html"/>
    <id>https://wangtongyouwen.github.io/post/3e9dbae2.html</id>
    <published>2023-06-05T01:54:30.000Z</published>
    <updated>2023-06-05T08:41:52.671Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>当今世界信息错综复杂，获取信息的渠道也很多。我曾是一个忠实的b站用户，但是不得不说现在的b站变了味，似乎可能会出现下一个全新的信息源。打破信息茧房的最好方式，并不是找到一个单独的平台，而是找到那些独立的信息提供方，也就是说，我们需要一个跨平台，无广告，无推荐算法的全新方式获取更有广度，更有深度的信息。</p><p>最近看到了一个老但是又重新鲜活起来的新技术——RSS，详细的技术细节，可以自行百度。重点是这项技术能够实现以下几点：</p><ul><li>多平台信息源：各种信息源(微信公众号,b站,Twitter,blogs)</li><li>跨平台：windows,linux,os,etc</li><li>方便易用可拓展</li><li>能够充分利用碎片化时间</li></ul><p>你可以帮这项技术当做一个记事本，这个记事本能够定时刷新最新资讯：</p><ol type="1"><li>这个资讯不是推荐算法给你的，而是你自己订阅的</li><li>不局限于单个平台</li></ol><figure><img src="https://pic3.zhimg.com/80/v2-caf461e861957f5f2ecf6b654d989752_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="前置准备">前置准备</h1><ol type="1"><li><p>需要亿点点科学上网：知名在线 RSS 服务 Feedly（2018.11）、Inoreader（2020.4） 先后被墙，打着万物皆可RSS大旗的 RssHub 官方域名也被墙了（2020.3）</p></li><li><p>一个github账号(vercel搭建服务)</p></li><li><p>Google play 用于安装移动端软件</p></li><li><p>vercel 用于配置 RssHub 服务(后文详解)</p></li></ol><h1 id="基础玩法">基础玩法</h1><ol type="1"><li>找到RSS源</li></ol><p>https://github.com/weekend-project-space/top-rss-list</p><p>https://github.com/ChanceYu/front-end-rss</p><p>公共的RSS源很多，但是大部分都很老旧，国内的源质量不够，国外的源检索和阅读有一定障碍。</p><ol start="2" type="1"><li>找到一个合适的RSS阅读器</li></ol><p>https://www.inoreader.com/ 推荐</p><p>https://feedly.com/</p><h1 id="进阶玩法">进阶玩法</h1><h2 id="前置准备-1">1 前置准备</h2><p>1 RssHub生成源服务器配置 https://github.com/DIYgod/RSSHub</p><p>2 vercel 账号(github)可以直接登陆</p><p>3 RssHub-Radar 嗅取网页中存在的rss订阅 https://github.com/DIYgod/RSSHub-Radar</p><p>4 RSSAid 安卓端嗅取rss订阅 https://github.com/LeetaoGoooo/RSSAid</p><h2 id="rsshub">2 RssHub</h2><p>RSSHub 是一个开源、简单易用、易于扩展的 RSS 生成器，可以给任何奇奇怪怪的内容生成 RSS 订阅源。RSSHub 借助于开源社区的力量快速发展中，目前已适配数百家网站的上千项内容</p><p>可以配合浏览器扩展 <a href="https://github.com/DIYgod/RSSHub-Radar">RSSHub Radar</a> 和 移动端辅助 App <a href="https://github.com/Cay-Zhang/RSSBud">RSSBud</a> (iOS) 与 <a href="https://github.com/LeetaoGoooo/RSSAid">RSSAid</a> (Android) 食用</p><p>首先介绍一下为什么需要自己部署RssHub服务？</p><p>Rss这种服务打击商业行为，所以许多网页会将其原本的域名直接屏蔽(本质也是一种爬虫)，所以只有使用自己的服务才不会被屏蔽，而且自己搭建的数据更加安全</p><ul><li>如何部署？</li></ul><p>https://docs.rsshub.app/install/</p><ul><li>如何使用vercel部署？(想象成一个云服务器，能够直接部署github上面的任何项目)</li></ul><ol type="1"><li>点击下面链接</li></ol><p>https://vercel.com/import/project?template=https://github.com/DIYgod/RSSHub</p><ol start="2" type="1"><li>完成一系列的注册后，一键部署(fork到自己的github仓库即可)</li><li>想要自动更新？</li></ol><p>https://github.com/apps/pull</p><ul><li>不会自己部署？提供下面的域名</li></ul><p><code>服务器1</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frsshub.rssforever.com%2F">https://rsshub.rssforever.com</a> <code>服务器2</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.qiuyuair.com%2F">https://rss.qiuyuair.com</a> <code>服务器3</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.injahow.cn%2F">https://rss.injahow.cn</a> <code>服务器4</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.feiyuyu.net%2F">https://rss.feiyuyu.net</a> <code>服务器5</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.shab.fun%2F">https://rss.shab.fun</a> <code>服务器6</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.itggg.cn%2F">https://rss.itggg.cn</a> <code>服务器7</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frsshub.uneasy.win%2F">https://rsshub.uneasy.win</a> <code>服务器8</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.injahow.cn%2F">https://rss.injahow.cn</a> <code>服务器9</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frsshub.anyant.xyz%2F">https://rsshub.anyant.xyz</a></p><h2 id="rsshub-radar">3 RssHub-Radar</h2><ul><li>为什么需要构建RssHub服务器？</li></ul><p>配合Radar浏览器插件能够直接嗅探任何存在的RSS订阅(真的应有尽有)：RssHub提供路由，这个域名就由你自己搭建的服务来提供咯！</p><ul><li>如何使用Radar？</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202306051249350.png" alt="image-20230605124920895" /><figcaption aria-hidden="true">image-20230605124920895</figcaption></figure><blockquote><p>注意这里只需要填入域名即可！</p></blockquote><p>以b站为例：</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202306051316754.png" alt="image-20230605125151705" /><figcaption aria-hidden="true">image-20230605125151705</figcaption></figure><p>你可以选择复制这个链接，或者直接subscribe, 注意这个subscribe必须要先登陆(在设置中找到目标网站，登陆后即可)</p><h2 id="feeddd订阅微信公众号">4 Feeddd订阅微信公众号</h2><p>由于微信自建生态，所以其中的信息比较难以获取rss订阅，但是通过<a href="https://hamibot.com/">Hamibot</a>能够动态获取推文(人工采集)</p><p>https://github.com/feeddd/feeds</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202306051316326.png" alt="image-20230605130927842" /><figcaption aria-hidden="true">image-20230605130927842</figcaption></figure><p>其实还有大量的公众号没有动态更新，所以单单微信这个生态而言，使用RSS是不方便的。</p><h1 id="end">end</h1><p>如果觉得RSS方便的话，一定要去star一下以上提到的所有项目，因为这是反商业化的，也是互联网精神的所在。</p><blockquote><p>rss才是真正的互联网精神的产物，正如开源一样，如何破除现有的信息茧房？并不是找到更多获取信息的渠道，而是打通blogger与user之间的通道！</p><p>less is more...</p></blockquote><p>more links:</p><p>https://sspai.com/post/75340</p><p>https://zhuanlan.zhihu.com/p/349349861</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;当今世界信息错综复杂，获取信息的渠道也很多。我曾是一个忠实的b站用户，但是不得不说现在的b站变了味，似乎可能会出现下一个全新的信息源。打破信息茧房的最好方式，并不是找到一个单独的平台，而是找到那些独立的信息提供方，也就是说，我们需要</summary>
      
    
    
    
    <category term="tools" scheme="https://wangtongyouwen.github.io/categories/tools/"/>
    
    
    <category term="tools" scheme="https://wangtongyouwen.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>tips:github自动fork</title>
    <link href="https://wangtongyouwen.github.io/post/3627c7a8.html"/>
    <id>https://wangtongyouwen.github.io/post/3627c7a8.html</id>
    <published>2023-05-21T04:30:16.000Z</published>
    <updated>2023-05-23T15:50:54.453Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li>在fork后的项目中新建：</li></ol><p>.github/workflows/main.yml</p><ol start="2" type="1"><li>在main.yml中输入：</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Upstream</span> <span class="string">Sync</span></span><br><span class="line"></span><br><span class="line"><span class="attr">permissions:</span></span><br><span class="line">  <span class="attr">contents:</span> <span class="string">write</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">schedule:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">cron:</span> <span class="string">&quot;0 */12 * * *&quot;</span> <span class="comment"># every 12 hours</span></span><br><span class="line">  <span class="attr">workflow_dispatch:</span> <span class="comment"># manual sync</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">sync:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">repo</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">ref:</span> <span class="string">master</span> <span class="comment"># [branch name]</span></span><br><span class="line">        <span class="attr">fetch-depth:</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Configure</span> <span class="string">Git</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        git config user.name &quot;GitHub Actions Bot&quot;</span></span><br><span class="line"><span class="string">        git config user.email &quot;email@xxx.com&quot;  # send email to you </span></span><br><span class="line"><span class="string"></span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Sync</span> <span class="string">with</span> <span class="string">upstream</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        git remote add upstream https://github.com/[https locations]</span></span><br><span class="line"><span class="string">        git fetch upstream</span></span><br><span class="line"><span class="string">        git merge upstream/master --no-edit</span></span><br><span class="line"><span class="string">        git push</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;在fork后的项目中新建：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;.github/workflows/main.yml&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;在main.yml中输入：&lt;/li&gt;
&lt;/ol&gt;
&lt;fi</summary>
      
    
    
    
    <category term="tips" scheme="https://wangtongyouwen.github.io/categories/tips/"/>
    
    <category term="github" scheme="https://wangtongyouwen.github.io/categories/tips/github/"/>
    
    
    <category term="tips" scheme="https://wangtongyouwen.github.io/tags/tips/"/>
    
    <category term="github" scheme="https://wangtongyouwen.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目6-语音合成</title>
    <link href="https://wangtongyouwen.github.io/post/2b566acf.html"/>
    <id>https://wangtongyouwen.github.io/post/2b566acf.html</id>
    <published>2023-05-06T11:09:28.000Z</published>
    <updated>2023-05-07T14:45:27.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="conditional-variational-autoencoder-with-adversarial-learning-for-end-to-end-text-to-speech">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</h1><p>http://proceedings.mlr.press/v139/kim21f/kim21f.pdf</p><h2 id="论文概述">1 论文概述</h2><p>在这项工作中，作者提出了一种并行的<strong>端到端文本到语音</strong>（TTS）方法，其生成的音频比当前的两阶段TTS系统更自然。这种方法采用了增强正规化流的变分推断和对抗训练过程，从而提高了生成建模的表达能力。此外，作者还提出了一种随机持续时间预测器，用于从输入文本合成具有多样节奏的语音。</p><p>通过对潜在变量的不确定性建模和随机持续时间预测器，该方法表达了自然的一对多关系，即输入文本可以以不同的音高和节奏以多种方式发音。在LJ语音（单一发音者数据集）上进行的主观人类评估（平均意见分数，MOS）表明，该方法优于目前最好的公开可用TTS系统，并实现了与基准真实数据相当的MOS。</p><p>总之，作者提出了一种端到端的TTS方法，采用<strong>变分推断</strong>、<strong>正规化流</strong>和<strong>对抗训练</strong>过程，生成更自然的音频。通过<strong>随机持续时间预测器</strong>和隐变量的不确定性建模，该方法能够实现文本到语音的<strong>自然一对多关系</strong>。实验结果表明，该方法在性能上优于现有的TTS系统，并在主观评估中取得了令人满意的成绩。</p><ul><li>过去研究：文本-&gt;频谱-&gt;波形</li><li>现在：文本-&gt;波形</li></ul><p>提出的方法主要在前三个小节中描述：条件变分自编码器（VAE）的公式化；从变分推断中得出的对齐估计；用于提高合成质量的对抗性训练。</p><ol type="1"><li><strong>条件变分自编码器</strong>（Conditional VAE）公式化：VITS模型采用了条件变分自编码器作为基本框架，通过引入潜在变量来捕捉输入文本的多样性，并进行端到端训练。</li><li><strong>变分推断中的对齐估计</strong>（Alignment Estimation）：VITS模型利用变分推断方法推导出与语音特征对齐的潜在变量表示，这有助于在生成过程中更好地捕捉文本与语音之间的对应关系。</li><li><strong>改进合成质量的对抗训练</strong>（Adversarial Training）：为了提高合成语音的质量，VITS模型采用了对抗训练方法，在生成过程中对抗性地优化生成器和判别器。</li></ol><p>整体架构包括文本编码器、持续时间预测器、潜在变量推断器、语音解码器和对抗性判别器等组件。通过联合训练这些组件，<strong>VITS</strong>模型可以在端到端的设置下实现高质量的文本到语音合成。</p><h2 id="method">2 method</h2><h3 id="conditional-vae">2.1 conditional VAE</h3><p><span class="math display">\[\log p_\theta(x|c)\ge\mathbb{E}_{q_\phi(z|x)}\Big[\log p_\theta(x|z)-\log\frac{q_\phi(z|x)}{p_\theta(z|c)}\Big]\]</span></p><p>左边：需要最大化的目标似然 x:目标(语音波形)，c:条件(text，说话人的音色)</p><p>右边：elbo(evidence lower bound) 如果是纯flow是能够得到解析式的 【重构器-KL散度:后验分布/先验分布】</p><p>最大化elbo：1. 解码器(重构器)在给定隐变量z的情况下，x对应的最大似然</p><ol start="2" type="1"><li>最小化KL散度：后验与先验之间的距离，<span class="math inline">\(\phi\)</span>模型在给定目标的条件下的分布与<span class="math inline">\(\theta\)</span>模型在给定条件下的分布越接近越好</li></ol><h4 id="一般的vae目标函数推导">一般的VAE目标函数推导</h4><ol type="1"><li><span class="math inline">\(P(X)=\int_{z} P(X|z)\times P(z)\)</span> <span class="math inline">\(X\)</span>为目标函数，<span class="math inline">\(Z\)</span>为隐变量，此处为对联合分布进行积分得到边缘分布</li><li>对于大部分<span class="math inline">\(z\)</span>,我们计算出来的<span class="math inline">\(P(X|z)\)</span>为0，因此需要缩小<span class="math inline">\(z\)</span>的样本空间，此处借用另外一个分布<span class="math inline">\(Q(z|X)\)</span>来缩小<span class="math inline">\(z\)</span>的样本空间(或者说增大<span class="math inline">\(z\)</span>产生<span class="math inline">\(X\)</span>的可能性)，并且当<span class="math inline">\(z\)</span>服从分布<span class="math inline">\(Q(z|X)\)</span>时，再计算<span class="math inline">\(P(X|z)\)</span>的期望值比较简单</li><li>那么，当<span class="math inline">\(z\)</span>服从分布<span class="math inline">\(Q(z|X)\)</span>时，<span class="math inline">\(P(X|z)\)</span>的期望值与目标数据的分布<span class="math inline">\(P(X)\)</span>之间是什么关系呢？可以用一个KL散度公式算起：<span class="math inline">\(D[Q(z|X)||P(z|X)]\)</span></li></ol><p><span class="math display">\[D[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X)-\log P(z|X)]\]</span></p><ol start="4" type="1"><li>为了引出<span class="math inline">\(P(X)\)</span>，可以基于贝叶斯公式来改写<span class="math inline">\(P(z|X)\)</span></li></ol><p><span class="math display">\[p(z|X)=\frac{P(z,X)}{P(X)}=\frac{P(X|z)\times P(z)}{P(X)}\]</span></p><ol start="5" type="1"><li>这时KL散度可以重新写成:</li></ol><p><span class="math display">\[D[Q(z|X)||P(z|X)]=\mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X)-\log P(X|z) -\log P(z)+\log P(X)]\]</span></p><ol start="6" type="1"><li><p>因为其中的<span class="math inline">\(P(X)\)</span>与<span class="math inline">\(z\sim Q(z|X)\)</span>无关，因此可以移到期望公式外面;同时，由于: <span class="math display">\[D[Q(z|X)||P(z)]=\mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X)-\log P(z)]\]</span></p><ol start="7" type="1"><li>KL散度进一步改写成</li></ol></li></ol><p><span class="math display">\[D[Q(z|X)||P(z|X)]=-\mathbb{E}_{z\sim Q(z|X)}[\log P(X|z)] + D[Q(z|X)||P(z)]+\log P(X)\]</span></p><ol start="8" type="1"><li>因此</li></ol><p><span class="math display">\[\log P(X)-\textcolor{red}{D[Q(z|X)||P(z|X)]} =\mathbb{E}_{z\sim Q(z|X)}[\log P(X|z)] - D[Q(z|X)||P(z)]\]</span></p><ol start="9" type="1"><li>红色部分恒大于0，故</li></ol><p><span class="math display">\[\log P(X) \ge \mathbb{E}_{z\sim Q(z|X)}[\log P(X|z)] - D[Q(z|X)||P(z)]\]</span></p><p>​ 当且仅当<span class="math inline">\(Q(z|X)\)</span>可以逼近<span class="math inline">\(P(z|X)\)</span>等式成立</p><ol start="10" type="1"><li><p>目标数据的对数似然公式下界如式(8)所示，并且右边第一项是解码器，第二项是先验分布<span class="math inline">\(P(z)\)</span>与后验分布<span class="math inline">\(Q(z|X)\)</span>之间的距离</p></li><li><p>如果希望<span class="math inline">\(\log P(X)\)</span>越大越好，也就是相当于希望解码器基于<span class="math inline">\(z\)</span>分布预测<span class="math inline">\(X\)</span>的概率越大越好，同时先验分布<span class="math inline">\(P(z)\)</span>与后验分布<span class="math inline">\(Q(z|X)\)</span>之间的距离越小越好</p></li></ol><h4 id="vits中的条件vae目标函数推导">VITS中的条件VAE目标函数推导</h4><ol type="1"><li>在本文中，我们可以从KL散度公式算起</li></ol><p><span class="math display">\[D[Q(z|X)||P(z|X,c)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X) - log P(z|X,c)]\]</span></p><ol start="2" type="1"><li>类似的，我们可以yoga贝叶斯公式来改写$ P(z|X,c)$</li></ol><p><span class="math display">\[P(z|X,c) = \frac{P(z,X|c)}{P(X|c)}=\frac{P(X|z)\times P(z|c)}{P(X|c)}\]</span></p><ol start="3" type="1"><li>于是KL散度可以重新写成:</li></ol><p><span class="math display">\[D[Q(z|X)||P(z|X,c)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X) - log P(X|z)-\log P(z|c)] + \log P(X|c)\]</span></p><p>​ 因为<span class="math inline">\(P(X|c)\)</span>与<span class="math inline">\(z\sim Q(z|X)\)</span>无关，因此可以移到期望公式之外</p><ol start="4" type="1"><li>同时，由于：</li></ol><p><span class="math display">\[D[Q(z|X)||P(z|c)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X) -\log P(z|c)]\]</span></p><p>​ KL散度式(9)可以进一步化解为： <span class="math display">\[D[Q(z|X)||P(z|X,c)] = -\mathbb{E}_{z\sim Q(z|X)}[log P(X|z)] + D[Q(z|X)||P(z|c)] + \log P(X|c)\]</span></p><ol start="5" type="1"><li>因此</li></ol><p><span class="math display">\[\log P(X|c) -\textcolor{red}{D[Q(z|X)||P(z|X,c)]} = \mathbb{E}_{z\sim Q(z|X)}[log P(X|z)] -D[Q(z|X)||P(z|c)]\]</span></p><p>红色部分是恒大于0，所以得到: <span class="math display">\[\log P(X|c)  \ge \mathbb{E}_{z\sim Q(z|X)}[log P(X|z)] -D[Q(z|X)||P(z|c)]\]</span> 当且仅当<span class="math inline">\(Q(z|X)\)</span>可以逼近<span class="math inline">\(P(z|X,c)\)</span>时，等式成立</p><ol start="6" type="1"><li>VITS的目标数据的对数似然公式下界中：</li></ol><ul><li><span class="math inline">\(X\)</span>表示音频的梅尔频谱</li><li><span class="math inline">\(c\)</span>表示<strong>文本信息和对其信息</strong>，<span class="math inline">\(z\)</span>表示隐含变量。其中，对齐信息是一个<span style="color:olive">硬对齐矩阵，形状为[|text|,|z|]</span><span class="math inline">\(\textcolor{olive}{}\)</span></li><li>文中提到，发现在<span class="math inline">\(P(z|c)\)</span>表示成简单的高斯分布之后再加一个flow变换，音质效果更好</li></ul><h4 id="重构loss右边等式第一项">重构loss(右边等式第一项)</h4><p>在重构损失中，作者使用了梅尔频谱图（mel-spectrogram）作为目标数据点，而不是原始波形。梅尔频谱图用<span class="math inline">\(X_{mel}\)</span>表示</p><p>这里的关键点是：</p><ol type="1"><li>使用梅尔频谱图作为目标数据点，而不是原始波形。</li><li>将潜在变量<span class="math inline">\(z\)</span>上采样到波形域<span class="math inline">\(\hat y\)</span>。</li><li>将波形域<span class="math inline">\(\hat y\)</span>转换为梅尔频谱图域<span class="math inline">\(\hat X_{mel}\)</span>。</li><li>使用预测和目标梅尔频谱图之间的<span class="math inline">\(L1\)</span>损失作为重构损失。</li></ol><p><span class="math display">\[L_{\text{recon}} = ||X_{mel}-\hat X_{mel}||\]</span></p><p>这可以看作是在假设数据分布为拉普拉斯分布的情况下进行的<strong>最大似然估计</strong>，同时忽略了常数项。通过使用近似于人类听觉系统响应的<strong>梅尔刻度</strong>，将<strong>重构损失定义在梅尔频谱图域以提高感知质量</strong>。注意，从原<strong>始波形估计梅尔频谱图不需要可训练参数</strong>，因为它只使用STFT和线性投影到梅尔刻度。此外，估计仅在训练期间使用，而不是在推理期间使用。在实践中，我们不会对整个潜在变量<span class="math inline">\(z\)</span>进行上采样，而是将部分序列用作解码器的输入，这是用于高效端到端训练的窗口生成器训练。</p><p>这段话的关键点包括：</p><ol type="1"><li>重构损失可以看作是在假设拉普拉斯分布的情况下进行的最大似然估计。</li><li>在梅尔频谱图域定义重构损失，以提高感知质量。</li><li>从原始波形估计梅尔频谱图不需要可训练参数。</li><li>在实践中，仅将部分潜在变量序列用作解码器的输入，以实现高效的端到端训练。</li></ol><h4 id="kl散度右边等式第二项">KL散度(右边等式第二项)</h4><ol type="1"><li>先验编码器的输入条件<span class="math inline">\(c\)</span>由音素<span class="math inline">\(c_{text}\)</span>和音素与潜在变量之间的对齐<span class="math inline">\(A\)</span>组成。(因为文本和音素之间不可能是一一对应的，单个文本可能存在多个音素)</li><li>对齐是一个硬单调注意力矩阵，表示每个输入音素扩展的长度以便与目标语音进行时间对齐。</li><li>在训练迭代中需要估计对齐。</li><li>为了提供更高分辨率的信息，使用线性刻度频谱图<span class="math inline">\(X_{lin}\)</span>作为输入，而不是<strong>梅尔频谱图</strong>。修改后的输入并不违反变分推断的属性。</li></ol><p><span class="math display">\[L_{kl}=\log q_{\phi} (z|X_{lin}) - \log p_{\theta} (z|c_{text},A) \\z \sim q_{\phi}(z|X_{lin}) = N (z;\mu_{\phi}(X_{lin}),\sigma_{\phi}(X_{lin}))\]</span></p><p><span class="math inline">\(z\)</span>是从后验分布中采样得到的，这是一个高斯分布。</p><p><span class="math inline">\(X\)</span>没有用波形频谱(复杂)，也没有用梅尔谱(相对简单)，而是采用了线性刻度频谱</p><p>我们使用因子化正态分布来参数化先验和后验编码器。我们发现增加先验分布的表达能力对于生成逼真的样本很重要。因此，我们在因子化正态先验分布的基础上应用<strong>归一化流</strong>，它允许将一个简单分布通过可逆变换转换成一个更复杂的分布，遵循变量变换规则： <span class="math display">\[p_\theta(z|c)=N(f_\theta(z);\mu_{\theta}(c))\left|\text{det}\frac{\partial f_\theta(z)}{\partial z}\right| \\c=[c_{text},A]\]</span></p><h3 id="alignment-estimation">2.2 Alignment Estimation</h3><h5 id="mas">MAS</h5><p>为了在输入文本和目标语音之间估计对齐关系A，我们采用了<strong>单调对齐搜索</strong>(Monotonic Alignment Search，MAS)，这是一种通过正则化流参数化的数据似然最大化来搜索对齐关系的方法。 <span class="math display">\[\begin{aligned}A &amp; =\underset{\hat{A}}{\arg \max } \log p\left(X \mid c_{\text {text }}, \hat{A}\right) \\&amp; =\underset{\hat{A}}{\arg \max } \log N\left(f(x) ; \mu\left(c_{\text {text }}, \hat{A}\right), \sigma\left(c_{\text {text }}, \hat{A}\right)\right)\end{aligned}\]</span> MAS利用了单调性假设，即输入文本中的字符和目标语音中的帧之间的对齐关系是单调的。这使得搜索过程更加高效，因为只需要在单调增加的路径上搜索。</p><ul><li>monotonic(单调)</li><li>non-skipping</li></ul><p>在我们的设置中直接应用MAS是困难的，因为我们的目标是ELBO（变分下界），而不是精确的对数似然。因此，我们重新定义MAS，使其寻找最大化ELBO的对齐关系，这可以简化为寻找最大化潜变量对数似然的对齐关系。</p><p>为了实现这一目标，我们可以在训练过程中根据<strong>当前模型参数来迭代更新对齐关系</strong>。在每次迭代过程中，我们通过最大化ELBO来调整对齐关系，并相应地更新模型参数。</p><p>这种方法允许我们在优化变分下界的同时，学习输入文本与目标语音之间的对齐关系。通过这种方式，我们可以在训练过程中逐渐改进模型的性能，并在推理阶段生成更自然的语音样本。</p><p><span class="math display">\[\begin{array}{l}\underset{\hat{A}}{\arg \max } \log p_{\theta}\left(X_{\text {mel }} \mid z\right)-\log \frac{q_{\phi}\left(z \mid x_{\text {lin }}\right)}{p_{\theta}\left(z \mid c_{\text {text }}, \hat{A}\right)} =\underset{\hat{A}}{\arg \max } \log p_{\theta}\left(z \mid c_{\text {text }}, \hat{A}\right)=\log N\left(f_{\theta}(z) ; \mu_{\theta}\left(c_{\text {text }}, \hat{A}\right), \sigma_{\theta}\left(c_{\text {text }}, \hat{A}\right)\right)\end{array}\]</span></p><h5 id="duration-prediction-from-text--时长模型">DURATION PREDICTION FROM TEXT--时长模型</h5><p>我们可以通过对估计对齐矩阵 <span class="math inline">\(A\)</span> 的每一行中的所有列求和来计算每个输入令牌的持续时间 <span class="math inline">\(d_i\)</span>：<span class="math inline">\(\sum_j A_{i,j}\)</span>. 该持续时间可以用来训练一个确定性的持续时间预测器，如之前的工作所提出的（Kim et al., 2020），但它不能表达一个人每次以不同语速发音的方式。为了生成类似于人类的语音节奏，我们设计了一个随机持续时间预测器，使其样本遵循给定条件变分自编码器在对抗学习环境下进行端到端文本转语音的音素持续时间分布。随机持续时间预测器是一个基于流的生成模型，通常通过最大似然估计进行训练。然而，直接应用最大似然估计是困难的，因为每个输入音素的持续时间是1）一个离散的整数，需要进行去量化以使用连续的归一化流，以及2）一个标量，由于可逆性限制了高维变换。我们应用变分去量化（Flow++）和变分数据增强（VFlow）来解决这些问题。</p><p>具体来说，我们引入了两个随机变量 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(\nu\)</span>，它们具有与持续时间序列 d 相同的时间分辨率和维度，用于变分去量化和变分数据增强。我们将 <span class="math inline">\(u\)</span> 的支持限制在 [0, 1) 之间，以使差值 d−u 变成一组正实数，并将 <span class="math inline">\(\nu\)</span> 和 d 逐通道地连接，以形成更高维的潜在表示。我们通过近似后验分布 <span class="math inline">\(q_{\phi}(u,\nu|d,c_{text})\)</span> 对这两个变量进行采样。所得到的目标是音素持续时间对数似然的变分下界： <span class="math display">\[\begin{gathered}\log p_{\theta}(d|c_{t e x t})\ge \mathbb{E}_{q_{\phi}(u,\nu|d,c_{t e x t})}\Big[\operatorname{log}\frac{p_{\theta}(d-u,\nu|c_{t e x t})}{q_{\phi}(u,\nu|d,c_{t e x t})}\Big] \end{gathered}\]</span> 持续时间训练损失 <span class="math inline">\(L_{dur}\)</span> 是负变分下界。我们将停止梯度运算符应用于输入条件，以防止将输入的梯度反向传播，从而使持续时间预测器的训练不影响其他模块的训练。</p><p>采样过程相对简单；通过随机噪声经过随机持续时间预测器的逆变换，对音素持续时间进行采样，然后将其转换为整数。</p><h3 id="adversarial-training">2.3 Adversarial Training</h3><p>为了在我们的学习系统中采用对抗性训练，我们添加了一个鉴别器<span class="math inline">\(D\)</span>，用于区分生成器<span class="math inline">\(G\)</span>生成的输出和真实波形<span class="math inline">\(y\)</span>。在这项工作中，我们使用了两种在语音合成中成功应用的损失类型；用于对抗性训练的最小平方损失函数，以及用于训练生成器的额外特征匹配损失：</p><p><span class="math display">\[\begin{gathered}L_{adv}(D) =\mathbb{E}_{(y,z)}\Big[(D(y)-1)^{2}+(D(G(z)))^{2}\Big],  \\L_{adv}(G) =\mathbb{E}_z\Big[(D(G(z))-1)^2\Big],  \\L_{fm}(G) =\mathbb{E}_{(y,z)}\Big[\sum\limits_{l=1}^T\frac{1}{N_l}\|D^l(y)-D^l(G(z))\|_1\Big]\end{gathered}\]</span></p><h3 id="final-loss">2.4 final loss</h3><p><span class="math display">\[L_{vae} = L_{recon} +L_{kl} +L_{dur} +L_{adv}(G) +L_{fm}(G)\]</span></p><h2 id="architecture">3 architecture</h2><p>整个建议模型的架构包括后验编码器、先验编码器、解码器、鉴别器和随机持续时间预测器。后验编码器和鉴别器仅用于训练，而不是用于推理。</p><h3 id="后验编码器">3.1 后验编码器</h3><p>对于后验编码器，使用 WaveGlow 和 Glow-TTS 中使用的非因果 WaveNet 残差块。</p><p>WaveNet 残差块由带有门控激活单元和跳过连接的膨胀卷积层组成。块上方的线性投影层产生正态后验分布的均值和方差。对于多说话者情况，我们在残差块中使用全局条件化添加说话者嵌入。</p><h3 id="先验编码器">3.2 先验编码器</h3><p>先验编码器由处理输入音素 <span class="math inline">\(c_{text}\)</span> 的文本编码器和改善先验分布灵活性的归一化流 <span class="math inline">\(f_\theta\)</span> 组成。文本编码器是一个变压器编码器，它使用相对位置表示而不是绝对位置编码。我们可以通过文本编码器从 <span class="math inline">\(c_{text}\)</span> 获取隐藏表示 <span class="math inline">\(h_{text}\)</span>，并通过文本编码器上方的线性投影层产生用于构造先验分布的均值和方差。归一化流是由 WaveNet 残差块堆叠组成的仿射耦合层的堆叠)。为简化起见，我们设计归一化流为具有雅可比行列式为一的体积保持变换。对于多说话者设置，我们通过全局调节将说话者嵌入添加到归一化流中的残差块。</p><h3 id="解码器">3.3 解码器</h3><p>解码器本质上是 HiFi-GAN V1 生成器。它由一堆转置卷积组成，每个转置卷积后面都跟着一个多感受野融合模块（MRF）。MRF 的输出是具有不同接收字段大小的残差块输出的和。对于多说话者设置，我们添加一个线性层，将说话者嵌入转换并将其添加到输入潜变量<span class="math inline">\(z\)</span>。</p><h3 id="判别器">3.4 判别器</h3><p>我们遵循 HiFi-GAN 提出的<strong>多周期鉴别器的鉴别器架构</strong>。多周期鉴别器是一种马尔可夫窗口为基础的子鉴别器的混合，每一个子鉴别器都在不同周期模式的输入波形上操作。</p><h3 id="随机时长预测器">3.5 随机时长预测器</h3><p>随机持续时间预测器根据条件输入 <span class="math inline">\(h_{text}\)</span> 估计<strong>音素持续时间</strong>的分布。为了有效地参数化随机持续时间预测器，我们堆叠具有<strong>膨胀和深度可分离卷积层</strong>的残差块。我们还将神经样条流应用于耦合层，这些神经样条流采用单调有理二次样条实现可逆非线性变换。与常用的仿射耦合层相比，神经样条流在参数数量相似的情况下提高了变换的表达能力。对于多说话者设置，我们添加一个线性层，将说话者嵌入转换并将其添加到输入 <span class="math inline">\(h_{text}\)</span> 中。</p><h2 id="code">4 code</h2><p>https://github.com/jaywalnut310/vits</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;conditional-variational-autoencoder-with-adversarial-learning-for-end-to-end-text-to-speech&quot;&gt;Conditional Variational Autoencoder wit</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络13-Glow</title>
    <link href="https://wangtongyouwen.github.io/post/c5690277.html"/>
    <id>https://wangtongyouwen.github.io/post/c5690277.html</id>
    <published>2023-05-06T10:56:32.000Z</published>
    <updated>2023-05-07T14:45:27.545Z</updated>
    
    <content type="html"><![CDATA[<h1 id="glow-generative-flow-with-invertible-11-convolutions">Glow: Generative Flow with Invertible 11 Convolutions</h1><p>https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf</p><p>在这篇论文中，作者提出了一种名为Glow的流式生成模型。流式生成模型（Flow-based generative models）在计算精确对数似然（log-likelihood）、进行精确的潜在变量推断以及并行化训练和合成方面具有概念上的吸引力。Glow模型采用了一种可逆的 1×1 卷积操作，该方法相对简单。</p><p>通过使用Glow模型，作者在标准基准测试中实现了对数似然的显著改进。更令人瞩目的是，作者证明了优化纯对数似然目标的基于流的生成模型能够高效地生成和操作大型图像，且生成的图像具有逼真的外观。这表明，Glow这种类型的流式生成模型在生成逼真图像方面具有很大的潜力。</p><ul><li>显式表示目标分布</li><li>可逆</li></ul><p>数据的分布：得到对应的似然(给定模型下，数据的似然)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;glow-generative-flow-with-invertible-11-convolutions&quot;&gt;Glow: Generative Flow with Invertible 11 Convolutions&lt;/h1&gt;
&lt;p&gt;https://proceed</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目5-CLIP搭建相似图像检索系统</title>
    <link href="https://wangtongyouwen.github.io/post/342f7c36.html"/>
    <id>https://wangtongyouwen.github.io/post/342f7c36.html</id>
    <published>2023-05-05T11:22:46.000Z</published>
    <updated>2023-05-06T07:13:23.481Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备工作">1 准备工作</h2><h3 id="clip模型的调用">1.1 clip模型的调用</h3><p>https://github.com/openai/CLIP</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">conda install --<span class="built_in">yes</span> -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install ftfy regex tqdm</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install git+https://github.com/openai/CLIP.git</span></span><br></pre></td></tr></table></figure><p><code>clip.available_models()</code> Returns the names of the available CLIP models.</p><h3 id="准备数据集">1.2 准备数据集</h3><p>与项目三中使用相同的数据集，这个数据集的划分代码略 <a href="https://wangtongyouwen.github.io/post/4627104a.html">[pytorch项目3-基于ResNet的水果蔬菜分类](https://wangtongyouwen.github.io/post/4627104a.html)</a></p><h2 id="train.py">2 train.py</h2><h3 id="get_args_parser">2.1 get_args_parser()</h3><p>这部分是为了能够接收从命令行中读取的参数，其中最关键的是几个数据的路径：训练样本，测试样本，输出的结果照片以及推断出的模型feature_dict</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_args_parser</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;image search task&#x27;</span>, add_help=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># Model parameters</span></span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--input_size&#x27;</span>, default=<span class="number">128</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;images input size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dateset_dir&#x27;</span>, default=<span class="string">&#x27;./dataset_fruit_veg/train&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to load images &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--test_image_dir&#x27;</span>, default=<span class="string">&#x27;./dataset_fruit_veg/val_images&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;images to test, split by comma &quot;,&quot;&#x27;</span>)  <span class="comment"># split from the test dataset, there is no Crossover</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_dir&#x27;</span>, default=<span class="string">&#x27;./output_di&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to save, empty for no saving&#x27;</span>) <span class="comment"># 相似照片</span></span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--model_name&#x27;</span>,default=<span class="string">&#x27;resnet50&#x27;</span>, <span class="comment"># resnet50,resnet152,clip</span></span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;model name&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--feature_dict_file&#x27;</span>,default=<span class="string">&#x27;corpus_feature_dict,npy&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;filename where to save image representations&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--topk&#x27;</span>,default=<span class="number">7</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;k most similar image representations&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--mode&#x27;</span>,default=<span class="string">&#x27;extract&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;extract or predict, for extracting features or predicting similar images from corpus&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure><h3 id="main">2.2 main</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model_names = timm.list_models(pretrained=<span class="literal">True</span>)</span><br><span class="line">    args = get_args_parser()</span><br><span class="line">    args = args.parse_args()</span><br><span class="line"></span><br><span class="line">    model = <span class="literal">None</span></span><br><span class="line">    preprocess = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.model_name != <span class="string">&quot;clip&quot;</span>:</span><br><span class="line">        model = timm.create_model(args.model_name, pretrained=<span class="literal">True</span>)  <span class="comment"># resnet50 resnet152</span></span><br><span class="line">        n_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;number of trainable params (M): %.2f&quot;</span> % (n_parameters / <span class="number">1.e6</span>))</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model, preprocess = clip.load(<span class="string">&quot;ViT-B/32&quot;</span>, device=device)  <span class="comment"># preprocess就是已经归一化后的结果，无需再进行归一化操作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.mode == <span class="string">&quot;extract&quot;</span>:</span><br><span class="line">        <span class="comment"># 第一阶段：图像表征提取</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;use pretrained model <span class="subst">&#123;args.model_name&#125;</span> to extract features&quot;</span>)</span><br><span class="line">        allVectors = extract_features(args, model, image_path=args.dataset_dir, preprocess=preprocess)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 第二阶段：图像检索</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;use pretrained model <span class="subst">&#123;args.model_name&#125;</span> to search <span class="subst">&#123;args.topk&#125;</span> similar images from corpus&quot;</span>)</span><br><span class="line"></span><br><span class="line">        test_images = glob.glob(os.path.join(args.test_image_dir, <span class="string">&quot;*.png&quot;</span>))</span><br><span class="line">        test_images += glob.glob(os.path.join(args.test_image_dir, <span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">        test_images += glob.glob(os.path.join(args.test_image_dir, <span class="string">&quot;*.jpeg&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loading image representation dictionary</span></span><br><span class="line">        allVectors = np.load(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>/<span class="subst">&#123;args.feature_dict_file&#125;</span>&quot;</span>, allow_pickle=<span class="literal">True</span>)  <span class="comment"># 导入字典</span></span><br><span class="line">        allVectors = allVectors.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reading test images</span></span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> tqdm.tqdm(test_images):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;image_file&#125;</span>...&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> args.model_name == <span class="string">&quot;clip&quot;</span>:</span><br><span class="line">                <span class="comment"># CLIP model</span></span><br><span class="line">                allVectors[image_file] = extract_feature_by_CLIP(model, preprocess, image_file)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># resnet50, resnet152</span></span><br><span class="line">                allVectors[image_file] = extract_feature_single(args, model, image_file)</span><br><span class="line"></span><br><span class="line">        sim, keys = getSimilarityMatrix(allVectors)</span><br><span class="line">        <span class="built_in">print</span>(keys)</span><br><span class="line">        result = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> tqdm.tqdm(test_images):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;sorting most similar images as <span class="subst">&#123;image_file&#125;</span>...&quot;</span>)</span><br><span class="line">            index = keys.index(image_file)</span><br><span class="line">            sim_vec = sim[index]</span><br><span class="line"></span><br><span class="line">            indexs = np.argsort(sim_vec)[::-<span class="number">1</span>][<span class="number">1</span>:args.topk]  <span class="comment"># 排序：从小到大的索引</span></span><br><span class="line">            simImages, simScores = [], []</span><br><span class="line">            <span class="keyword">for</span> ind <span class="keyword">in</span> indexs:</span><br><span class="line">                simImages.append(keys[ind])</span><br><span class="line">                simScores.append(sim_vec[ind])</span><br><span class="line">            result[image_file] = (simImages, simScores)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;starting to show similar images...&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> test_images:</span><br><span class="line">            plotSimilarImages(args, image_file, result[image_file][<span class="number">0</span>], result[image_file][<span class="number">1</span>], numRow=<span class="number">1</span>,</span><br><span class="line">                              numCol=args.topk)</span><br></pre></td></tr></table></figure><ul><li>判断模型是resnet50，resnet152 or clip</li><li>判断模式是否是 extract or predict</li></ul><p>predict: 部分代码的步骤：</p><ol type="1"><li>读取图片，判断模型类型，读取特征字典</li><li>将目标图片与特征图片中的所有值做余弦相似度比较，得到的结果矩阵：allvector</li><li>排序，取出前n个目标</li><li>把图片地址和图片计算得到的score储存，最后进行输出</li></ol><h2 id="extract">3 extract</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_single</span>(<span class="params">args, model, file</span>):</span><br><span class="line">    img_rgb = Image.<span class="built_in">open</span>(file).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">    image = img_rgb.resize((args.input_size, args.input_size), Image.LANCZOS)</span><br><span class="line">    image = torchvision.transforms.ToTensor()(image)</span><br><span class="line"></span><br><span class="line">    trainset_mean = [<span class="number">0.4729932</span>, <span class="number">0.43474569</span>, <span class="number">0.3264319</span>]</span><br><span class="line">    trainset_std = [<span class="number">0.37707761</span>, <span class="number">0.36121109</span>, <span class="number">0.34872371</span>]</span><br><span class="line"></span><br><span class="line">    image = torchvision.transforms.Normalize(mean=trainset_mean, std=trainset_std)(image).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        features = model.forward_features(image)</span><br><span class="line">        vec = model.global_pool(features)</span><br><span class="line">        vec = vec.squeeze().numpy()</span><br><span class="line"></span><br><span class="line">    img_rgb.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vec</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>这部分代码将先对图像进行归一化，然后在模型的forward_features层输出特征，在进入池化层最后得到代表图片的字典信息。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_by_CLIP</span>(<span class="params">model, preprocess, file</span>):</span><br><span class="line">    image = preprocess(Image.<span class="built_in">open</span>(file)).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        vec = model.encode_image(image)</span><br><span class="line">        vec = vec.squeeze().cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vec</span><br></pre></td></tr></table></figure><ul><li>使用CLIP中的encode_image输出特征信息</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_features</span>(<span class="params">args, model, image_path=<span class="string">&quot;&quot;</span>, preprocess=<span class="literal">None</span></span>):</span><br><span class="line">    allVectors = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> image_file <span class="keyword">in</span> tqdm.tqdm(glob.glob(os.path.join(image_path, <span class="string">&quot;*&quot;</span>, <span class="string">&quot;*.jpg&quot;</span>))):  <span class="comment"># image_path:train # tqdm:进度条</span></span><br><span class="line">        <span class="keyword">if</span> args.model_name == <span class="string">&quot;clip&quot;</span>:</span><br><span class="line">            allVectors[image_file] = extract_feature_by_CLIP(model, preprocess, image_file)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            allVectors[image_file] = extract_feature_single(args, model, image_file)</span><br><span class="line"></span><br><span class="line">    os.makedirs(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>/<span class="subst">&#123;args.feature_dict_file&#125;</span>&quot;</span>, allVectors)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> allVectors</span><br></pre></td></tr></table></figure><h2 id="计算余弦相似度">4 计算余弦相似度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算余弦相似度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getSimilarityMatrix</span>(<span class="params">vectors_dict</span>):</span><br><span class="line">    v = np.array(<span class="built_in">list</span>(vectors_dict.values()))  <span class="comment"># [NUM,H]</span></span><br><span class="line">    numerator = np.matmul(v, v.T)  <span class="comment"># [NUM,NUM]</span></span><br><span class="line">    denominator = np.matmul(np.linalg.norm(v, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>),</span><br><span class="line">                            np.linalg.norm(v, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>).T)  <span class="comment"># [NUM,NUM]</span></span><br><span class="line"></span><br><span class="line">    sim = numerator / denominator</span><br><span class="line">    keys = <span class="built_in">list</span>(vectors_dict.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sim, keys</span><br></pre></td></tr></table></figure><h2 id="绘制结果">5 绘制结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setAxes</span>(<span class="params">ax, image, query=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    value = kwargs.get(<span class="string">&quot;value&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> query:</span><br><span class="line">        ax.set_xlabel(<span class="string">&quot;Query Image\n&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(image), fontsize=<span class="number">12</span>)</span><br><span class="line">        ax.xaxis.label.set_color(<span class="string">&quot;red&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.set_xlabel(<span class="string">&quot;score=&#123;1:1.3f&#125;\n&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(image, value), fontsize=<span class="number">12</span>)</span><br><span class="line">        ax.xaxis.label.set_color(<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotSimilarImages</span>(<span class="params">args, image, simImages, simValues, numRow=<span class="number">1</span>, numCol=<span class="number">4</span></span>):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    <span class="comment"># set width and height in inches</span></span><br><span class="line"></span><br><span class="line">    fig.set_size_inches(<span class="number">18.5</span>, <span class="number">10.5</span>)</span><br><span class="line">    fig.suptitle(<span class="string">f&quot;use engine model: <span class="subst">&#123;args.model_name&#125;</span>&quot;</span>, fontsize=<span class="number">35</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, numCol * numRow):</span><br><span class="line">        ax = []</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(image)</span><br><span class="line">            ax = fig.add_subplot(numRow, numCol, <span class="number">1</span>)</span><br><span class="line">            setAxes(ax, image.split(os.sep)[-<span class="number">1</span>], query=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(simImages[j - <span class="number">1</span>])</span><br><span class="line">            ax.append(fig.add_subplot(numRow,numCol,j+<span class="number">1</span>))</span><br><span class="line">            setAxes(ax[-<span class="number">1</span>],<span class="string">&quot;_&quot;</span>.join(simImages[j-<span class="number">1</span>].split(os.sep)[-<span class="number">2</span>:]),value=simValues[j-<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># truncated_filename = &quot;&quot;.join(simImages[j - 1].split(os.sep)[-2:])</span></span><br><span class="line">            <span class="comment"># truncated_filename = truncated_filename[:15]  # 只显示前 15 个字符</span></span><br><span class="line">            <span class="comment"># setAxes(ax[-1], truncated_filename, value=simValues[j - 1])</span></span><br><span class="line">        img = img.convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        plt.imshow(img)</span><br><span class="line">        img.close()</span><br><span class="line">    fig.savefig(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>_search_top_<span class="subst">&#123;args.topk-<span class="number">1</span>&#125;</span>_<span class="subst">&#123;image.split(os.sep)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]&#125;</span>.png&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ul><li>这部分展示代码具有参考性，可以用来动态显示结果。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;准备工作&quot;&gt;1 准备工作&lt;/h2&gt;
&lt;h3 id=&quot;clip模型的调用&quot;&gt;1.1 clip模型的调用&lt;/h3&gt;
&lt;p&gt;https://github.com/openai/CLIP&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;ta</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>pytorch进阶2-autoregressive diffusion model</title>
    <link href="https://wangtongyouwen.github.io/post/954ae01.html"/>
    <id>https://wangtongyouwen.github.io/post/954ae01.html</id>
    <published>2023-05-04T09:59:09.000Z</published>
    <updated>2023-05-05T11:57:57.886Z</updated>
    
    <content type="html"><![CDATA[<p>Improved Denoising Diffusion Probabilistic Models</p><p>http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf</p><p>Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting</p><p>http://proceedings.mlr.press/v139/rasul21a/rasul21a.pdf</p><p>Denoising Diffusion Probabilistic Models</p><p>https://arxiv.org/pdf/2006.11239.pdf</p><h2 id="回顾">回顾</h2><h3 id="如何将扩散模型与自回归模型结合">1 如何将扩散模型与自回归模型结合？</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041925965.png" alt="image-20230504192539224" /><figcaption aria-hidden="true">image-20230504192539224</figcaption></figure><p>目标函数发生了变化： <span class="math display">\[\mathbb{E}_{\mathbf{x}^0_t,\epsilon,n}\left[\|\epsilon-\epsilon_\theta(\sqrt{\bar{\alpha}_n}\mathbf{x}^0_t+\sqrt{1-\bar{\alpha}_n}\epsilon,\mathbf{h}_{t-1},n)\|^2\right],\]</span> <img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041930347.png" alt="image-20230504192959481" /></p><ul><li><p>为什么使用采样？而不是按照顺序</p><p>这是与随机梯度下降算法相同，目的是为了使训练更加鲁棒</p></li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041932387.png" alt="image-20230504193228721" /><figcaption aria-hidden="true">image-20230504193228721</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041943539.png" alt="image-20230504194302000" /><figcaption aria-hidden="true">image-20230504194302000</figcaption></figure><h3 id="improved-denoising-diffusion-probabilistic-models">2 Improved Denoising Diffusion Probabilistic Models</h3><ul><li>可学习的方差<span class="math inline">\(\sum_\theta(x_t,t)\)</span></li><li>噪声方案的改进</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041946113.png" alt="image-20230504194648255" /><figcaption aria-hidden="true">image-20230504194648255</figcaption></figure><ul><li>损失函数的改进</li></ul><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041948618.png" alt="image-20230504194753966" /> <span class="math display">\[L_{hybrid} = L_{simple} + \lambda L_{vlb}\]</span></p><h3 id="思维导图">3 思维导图</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305042024133.png" alt="image-20230504202447257" /><figcaption aria-hidden="true">image-20230504202447257</figcaption></figure><h2 id="代码分析">代码分析</h2><p>https://github.com/openai/improved-diffusion</p><h3 id="image_train.py">1 image_train.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_argparser</span>():</span><br><span class="line">    defaults = <span class="built_in">dict</span>(</span><br><span class="line">        data_dir=<span class="string">&quot;&quot;</span>,</span><br><span class="line">        schedule_sampler=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">        lr=<span class="number">1e-4</span>,</span><br><span class="line">        weight_decay=<span class="number">0.0</span>,</span><br><span class="line">        lr_anneal_steps=<span class="number">0</span>,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        microbatch=-<span class="number">1</span>,  <span class="comment"># -1 disables microbatches</span></span><br><span class="line">        ema_rate=<span class="string">&quot;0.9999&quot;</span>,  <span class="comment"># comma-separated list of EMA values</span></span><br><span class="line">        log_interval=<span class="number">10</span>,</span><br><span class="line">        save_interval=<span class="number">10000</span>,</span><br><span class="line">        resume_checkpoint=<span class="string">&quot;&quot;</span>,</span><br><span class="line">        use_fp16=<span class="literal">False</span>,</span><br><span class="line">        fp16_scale_growth=<span class="number">1e-3</span>,</span><br><span class="line">    )</span><br><span class="line">    defaults.update(model_and_diffusion_defaults())</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    add_dict_to_argparser(parser, defaults)</span><br><span class="line">    <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_dict_to_argparser</span>(<span class="params">parser, default_dict</span>):</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> default_dict.items():</span><br><span class="line">        v_type = <span class="built_in">type</span>(v)</span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            v_type = <span class="built_in">str</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(v, <span class="built_in">bool</span>):</span><br><span class="line">            v_type = str2bool</span><br><span class="line">        parser.add_argument(<span class="string">f&quot;--<span class="subst">&#123;k&#125;</span>&quot;</span>, default=v, <span class="built_in">type</span>=v_type)</span><br></pre></td></tr></table></figure><ul><li>这一部分的代码可以在命令行中传参进行简化：传入一个字典自动解析。从字典中自动生成argument parser</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    args = create_argparser().parse_args()</span><br><span class="line"></span><br><span class="line">    dist_util.setup_dist()</span><br><span class="line">    logger.configure()</span><br><span class="line"></span><br><span class="line">    logger.log(<span class="string">&quot;creating model and diffusion...&quot;</span>)</span><br><span class="line">    model, diffusion = create_model_and_diffusion(</span><br><span class="line">        **args_to_dict(args, model_and_diffusion_defaults().keys())</span><br><span class="line">    )</span><br><span class="line">    model.to(dist_util.dev())</span><br><span class="line">    schedule_sampler = create_named_schedule_sampler(args.schedule_sampler, diffusion)</span><br><span class="line"></span><br><span class="line">    logger.log(<span class="string">&quot;creating data loader...&quot;</span>)</span><br><span class="line">    data = load_data(</span><br><span class="line">        data_dir=args.data_dir,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        image_size=args.image_size,</span><br><span class="line">        class_cond=args.class_cond,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    logger.log(<span class="string">&quot;training...&quot;</span>)</span><br><span class="line">    TrainLoop(</span><br><span class="line">        model=model,</span><br><span class="line">        diffusion=diffusion,</span><br><span class="line">        data=data,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        microbatch=args.microbatch,</span><br><span class="line">        lr=args.lr,</span><br><span class="line">        ema_rate=args.ema_rate,</span><br><span class="line">        log_interval=args.log_interval,</span><br><span class="line">        save_interval=args.save_interval,</span><br><span class="line">        resume_checkpoint=args.resume_checkpoint,</span><br><span class="line">        use_fp16=args.use_fp16,</span><br><span class="line">        fp16_scale_growth=args.fp16_scale_growth,</span><br><span class="line">        schedule_sampler=schedule_sampler,</span><br><span class="line">        weight_decay=args.weight_decay,</span><br><span class="line">        lr_anneal_steps=args.lr_anneal_steps,</span><br><span class="line">    ).run_loop()</span><br></pre></td></tr></table></figure><p>传参-&gt;获得模型-&gt;获得数据-&gt;训练</p><p>...improved-diffusion-main_diffusion_util.py</p><h3 id="create_model_and_diffusion">2 create_model_and_diffusion</h3><h4 id="create_gaussian_diffusion">2.1 create_gaussian_diffusion</h4><ul><li>生成扩散过程的模型框架</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_gaussian_diffusion</span>(<span class="params"></span></span><br><span class="line"><span class="params">    *,</span></span><br><span class="line"><span class="params">    steps=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">    learn_sigma=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    sigma_small=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    noise_schedule=<span class="string">&quot;linear&quot;</span>,</span></span><br><span class="line"><span class="params">    use_kl=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    predict_xstart=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    rescale_timesteps=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    rescale_learned_sigmas=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    timestep_respacing=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    betas = gd.get_named_beta_schedule(noise_schedule, steps)</span><br><span class="line">    <span class="keyword">if</span> use_kl:</span><br><span class="line">        loss_type = gd.LossType.RESCALED_KL</span><br><span class="line">    <span class="keyword">elif</span> rescale_learned_sigmas:</span><br><span class="line">        loss_type = gd.LossType.RESCALED_MSE</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loss_type = gd.LossType.MSE</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> timestep_respacing:</span><br><span class="line">        timestep_respacing = [steps]</span><br><span class="line">    <span class="keyword">return</span> SpacedDiffusion(</span><br><span class="line">        use_timesteps=space_timesteps(steps, timestep_respacing),</span><br><span class="line">        betas=betas,</span><br><span class="line">        model_mean_type=(</span><br><span class="line">            gd.ModelMeanType.EPSILON <span class="keyword">if</span> <span class="keyword">not</span> predict_xstart <span class="keyword">else</span> gd.ModelMeanType.START_X</span><br><span class="line">        ),</span><br><span class="line">        model_var_type=(</span><br><span class="line">            (</span><br><span class="line">                gd.ModelVarType.FIXED_LARGE</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> sigma_small</span><br><span class="line">                <span class="keyword">else</span> gd.ModelVarType.FIXED_SMALL</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> learn_sigma</span><br><span class="line">            <span class="keyword">else</span> gd.ModelVarType.LEARNED_RANGE</span><br><span class="line">        ),</span><br><span class="line">        loss_type=loss_type,</span><br><span class="line">        rescale_timesteps=rescale_timesteps,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><ol type="1"><li>get_named_beta_schedule 生成betas的策略 linear increasing/cosine</li><li>class SpacedDiffusion(GaussianDiffusion)</li><li>GaussianDiffusion</li></ol><p>:param betas: a 1-D numpy array of betas for each diffusion timestep,starting at T and going to 1. :param model_mean_type: a ModelMeanType determining what the model outputs. :param model_var_type: a ModelVarType determining how variance is output. :param loss_type: a LossType determining the loss function to use. :param rescale_timesteps: if True, pass floating point timesteps into the model so that they are always scaled like in the original paper (0 to 1000).</p><ol start="4" type="1"><li>p_mean_variance</li><li>_vb_terms_bpd</li><li>training_losses</li></ol><h4 id="unet">2.2 Unet</h4><p>详见Unet(笔记)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Improved Denoising Diffusion Probabilistic Models&lt;/p&gt;
&lt;p&gt;http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf&lt;/p&gt;
&lt;p&gt;Autoregressiv</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="diffusion model" scheme="https://wangtongyouwen.github.io/categories/pytorch/diffusion-model/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="diffusion model" scheme="https://wangtongyouwen.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>pytorch进阶1-DDPM</title>
    <link href="https://wangtongyouwen.github.io/post/8c9cf490.html"/>
    <id>https://wangtongyouwen.github.io/post/8c9cf490.html</id>
    <published>2023-05-03T11:07:37.000Z</published>
    <updated>2023-05-05T11:57:57.889Z</updated>
    
    <content type="html"><![CDATA[<p>Denoising Diffusion Probabilistic Models</p><p>https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf</p><p>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</p><p>http://proceedings.mlr.press/v37/sohl-dickstein15.pdf</p><h2 id="生成模型">1 生成模型</h2><ul><li>seq2seq</li><li>gan</li><li>flow（数学推理严谨）</li><li>VAE</li><li>diffusion model</li></ul><h2 id="条件概率公式与高斯分布的kl散度">2 条件概率公式与高斯分布的KL散度</h2><h3 id="条件概率的一般形式">2.1 条件概率的一般形式</h3><p><span class="math display">\[P(A,B,C)=P(C|B,A)P(B,A) = P(C|B,A)P(B|A)P(A) \\P(B,C|A) = P(B|A)P(C|A,B)\]</span></p><h3 id="基于马尔科夫假设的条件概率">2.2 基于马尔科夫假设的条件概率</h3><p>如果满足马尔科夫链关系 A-&gt;B-&gt;C,那么有： <span class="math display">\[P(A,B,C)=P(C|B,A)P(B,A)=P(C|B)P(B|A)P(A) \\P(B,C|A)=P(B|A)P(C|B)\]</span></p><h3 id="高斯分布的kl散度公式">2.3 高斯分布的KL散度公式</h3><p>对于两个单一变量的高斯分布p和q而言，他们的KL散度为 <span class="math display">\[KL(p,q) = log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}\]</span></p><h3 id="参数重整化">2.4 参数重整化</h3><p>若希望从高斯分布<span class="math inline">\(N(\mu,\sigma^2)\)</span>中采样，可以先从标准分布<span class="math inline">\(N(0,1)\)</span>采样出<span class="math inline">\(z\)</span>，再得到<span class="math inline">\(\sigma*z+\mu\)</span>。这样做的好处是将随机性转移到了<span class="math inline">\(z\)</span>这个变量上，而<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>则当做仿射变换网络的一部分</p><h2 id="vae与多层vae回顾">3 VAE与多层VAE回顾</h2><p>https://zhuanlan.zhihu.com/p/34998569</p><h3 id="单层vae的原理公式与置信下界">3.1 单层VAE的原理公式与置信下界</h3><p><span class="math display">\[p(x)=\int_z p_{\theta}(x|z)p(z)\\p(x)=\int_z q_{\phi}(z|x) \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \\\log p(x)=\log \mathbb{E}_{z \sim q_{\phi}(z \mid x)}\left[\frac{p_{\theta}(x \mid z) p(z)}{q_{\phi}(z \mid x)}\right] \\\log p(x) \geq \mathbb{E}_{z \sim q_{\phi}(z \mid x)}\left[\log \frac{p_{\theta}(x \mid z) p(z)}{q_{\phi}(z \mid x)}\right]\]</span></p><h3 id="多层vae的原理公式与置信下界">3.2 多层VAE的原理公式与置信下界</h3><p><span class="math display">\[\begin{array}{c}p(x)=\int_{z_{1}} \int_{z_{2}} p_{\theta}\left(x, z_{1}, z_{2}\right) d z_{1}, d z_{2} \\p(x)=\iint q_{\phi}\left(z_{1}, z_{2} \mid x\right) \frac{p_{\theta}\left(x, z_{1}, z_{2}\right)}{q_{\phi}\left(z_{1}, z_{2} \mid x\right)} \\p(x)=\mathbb{E}_{z_{1}, z_{2} \sim q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\left[\frac{p_{\theta}\left(x, z_{1}, z_{2}\right)}{q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\right] \\\log p(x) \geq \mathbb{E}_{z_{1}, z_{2} \sim q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\left[\log \frac{p_{\theta}\left(x, z_{1}, z_{2}\right)}{q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\right] \\p\left(x, z_{1}, z_{2}\right)=p\left(x \mid z_{1}\right) p\left(z_{1} \mid z_{2}\right) p\left(z_{2}\right) \\q\left(z_{1}, z_{2} \mid x\right)=q\left(z_{1} \mid x\right) q\left(z_{2} \mid z_{1}\right) \\\mathcal{L}(\theta, \phi)=\mathbb{E}_{q(21,2 q \mid x)}\left[\log p\left(x \mid z_{1}\right)-\log q\left(z_{1} \mid x\right)+\log p\left(z_{1} \mid z_{2}\right)-\log q\left(z_{2} \mid z_{1}\right)+\log p\left(z_{2}\right)\right]\end{array}\]</span></p><h2 id="diffusion-model">4 diffusion model</h2><p>从目标分布中，希望能找到逆扩散过程的规律，然后利用这个规律将任意噪声恢复。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305032040155.png" alt="image-20221112164034826" /><figcaption aria-hidden="true">image-20221112164034826</figcaption></figure><h3 id="前向过程">4.1 前向过程</h3><p>不断往数据中加入噪声，最后得到了纯噪声（扩散过程）</p><p>每个时刻都要添加高斯噪声，后一时刻都是有前一时刻增加噪声得到的；</p><p>其实这个过程可以看作不断<strong>构建标签</strong>（噪声）的过程</p><ol type="1"><li>给定初始数据分布<span class="math inline">\(x_0 \thicksim q(x)\)</span>,可以不断向分布中添加高斯噪声，该噪声的标准差是以固定值<span class="math inline">\(\beta_t\)</span>而确定的，均值是以固定值值<span class="math inline">\(\beta_t\)</span>和当前t时刻的数据<span class="math inline">\(x_t\)</span>决定的。这个过程是一个马尔科夫链过程。</li><li>随着t的不断增大，最终数据分布<span class="math inline">\(x_T\)</span>变成了一个各向独立的高斯分布。</li></ol><p>Given a data point sampled from a real data distribution <span class="math inline">\(x_0 \thicksim q(x)\)</span>,let us define a forward diffusion process in which we add small amount of Gaussian noise to the sample in <span class="math inline">\(T\)</span> steps, producing a sequence of noisy samples <span class="math inline">\(x_1,\dots,x_T\)</span>. The step sizes are controlled by a variance schedule <span class="math inline">\({\beta_t \in (0,1)}^t_{t=1}\)</span></p><p>给定一个从真实数据分布中采样的数据点<span class="math inline">\(x_0 \thicksim q(x)\)</span>，让我们定义一个向前扩散过程，在该过程中，我们在<span class="math inline">\(T\)</span>步中向样本添加少量的高斯噪声，产生一系列噪声样本<span class="math inline">\(x_1，\dots，x_T\)</span>。步长由方差时间表<span class="math inline">\({\beta_t \in (0,1)}^t_{t=1}\)</span>控制。 <span class="math display">\[q(X_t|X_{t-1})=\N(X_t;\sqrt{1-\beta_t}X_{t-1},\beta_t I) \\q(X_{1:T}|X_0) = \prod^T_{t=1} q(X_t|X_{t-1})\]</span> The data sample <span class="math inline">\(x_0\)</span> gradually loses its distinguishable features as the step <span class="math inline">\(t\)</span> becomes larger. Eventually when <span class="math inline">\(T \rightarrow \infty,X_T\)</span> is euivalent to an isotropic Gaussian distribution</p><p>随着步骤<span class="math inline">\(t\)</span>变得更大，数据样本<span class="math inline">\(x_0\)</span>逐渐失去其可区分的特征。最终当<span class="math inline">\(T \rightarrow \infty\)</span>时，<span class="math inline">\(X_T\)</span>等同于各向同性的高斯分布。</p><ol start="3" type="1"><li>任何时刻的<span class="math inline">\(q(X_t)\)</span>推导也可以完全基于<span class="math inline">\(x_0\)</span>和<span class="math inline">\(\beta _t\)</span>来计算出来，而不需要做迭代</li></ol><p>注意这里，两个正态分布 <span class="math inline">\(X\thicksim N(\mu_1,\sigma_1)\)</span> 和 <span class="math inline">\(Y\thicksim N(\mu_2,\sigma_2)\)</span>的叠加后的分布<span class="math inline">\(aX+bY\)</span>的均值为<span class="math inline">\(a\mu_1+b\mu_2\)</span>，方差为<span class="math inline">\(a^2\sigma_1^2+b^2\sigma_2^2\)</span>，所以<span class="math inline">\(\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}z_{t-2}+\sqrt{1-\alpha_tz_{t-1}}z_{t-1}\)</span>可以参数重整化为只含有一个随机变量<span class="math inline">\(z\)</span>构成的<span class="math inline">\(\sqrt{1-\alpha_t\alpha_{t-1}}z\)</span>的形式</p><h4 id="第一个公式如何得到x_t时刻的分布前向过程">第一个公式，如何得到<span class="math inline">\(X_t\)</span>时刻的分布（前向过程）？</h4><p><span class="math display">\[\alpha _t = 1- \beta_t\]</span></p><p>式（6）中<span class="math inline">\(\beta_t\)</span>表示图像的加入噪声的程度，随着训练的进行要逐渐增大，论文中从0.0001到0.002扩大。 <span class="math display">\[x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_1\]</span> 式(7)中<span class="math inline">\(z_1\)</span>表示噪声（高斯噪声） <span class="math display">\[\begin{align}          x_t &amp; = \sqrt{\alpha _t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}z_{2})+\sqrt{1-\alpha_t}z_1 \\           &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\bar z_2 \\           &amp; = \sqrt{\bar \alpha_t}x_0+\sqrt{1-\bar \alpha_t} z_t          \end{align}\]</span> 其中<span class="math inline">\(z_1和z_2\)</span>分别表示<span class="math inline">\(N(0,1-\alpha_t)和N(0,\alpha_t(1-\alpha_{t-1}))\)</span>，独立同分布可相加得到，最后的<span class="math inline">\(\bar z_2\)</span></p><p>其中<span class="math inline">\(\bar \alpha_t\)</span>表示累乘<span class="math inline">\(\bar \alpha_t=\alpha_1\cdot \alpha_2 \cdot \dots \alpha_{t-1}\cdot \alpha_{t}\)</span> <span class="math display">\[q(X_t|X_0) = \N(X_t;\sqrt{\bar \alpha_t}X_0,(1-\bar \alpha_t)I)\]</span> Usually, we can afford a larger update step when the sample gets noisier, so <span class="math inline">\(\beta_1&lt;\beta_2&lt;...&lt;\beta_T\)</span> and therfore <span class="math inline">\(\bar \alpha_1 &gt; \dots \bar \alpha_T\)</span></p><h3 id="逆向过程">4.2 逆向过程</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305032044361.png" alt="image-20221112171756756" /><figcaption aria-hidden="true">image-20221112171756756</figcaption></figure><p>逆过程是从高斯噪声中恢复原始数据，我们可以假设它也是一个高斯分布，但是无法逐步地去拟合分布，所以需要构建一个参数分布来去做估计。逆扩散过程仍然是一个马尔科夫链过程。 <span class="math display">\[p_\theta(X_{0:T}) = p(X_T)\prod^T_{t=1}p_\theta(X_{t-1}|X_t)\\p_\theta(X_{t-1}|X_t) = \N (X_{t-1};\mu_{\theta}(X_t,t),\Sigma_{\theta}(x_t,t))\]</span> If we can reverse the above process and sample from <span class="math inline">\(q(X_{t-1}|X_t)\)</span>, we will be able to recreate the true sample from a Gaussian noise input, <span class="math inline">\(X_T \thicksim \mathcal{N}(0,I)\)</span>. Note that if <span class="math inline">\(\beta_t\)</span> is small enough, <span class="math inline">\(q(X_{t-1}|X_t)\)</span> will also be Gaussian. Unfortunately, we cannot easily estimate <span class="math inline">\(q(X_{t-1}|X_t)\)</span> because it needs to use the entire dataset adn therefore we need to learn a model <span class="math inline">\(p_\theta\)</span> to approximate these conditional probabilities in order to run reverse diffusion process.</p><p>如果我们能够逆转上述过程并从<span class="math inline">\(q(X_{t-1}|X_t)\)</span>中抽样，我们将能够从高斯噪声输入<span class="math inline">\(X_T \thicksim \mathcal{N}(0,I)\)</span>中重建真实样本。请注意，如果<span class="math inline">\(\beta_t\)</span>足够小，<span class="math inline">\(q(X_{t-1}|X_t)\)</span>也将是高斯分布。不幸的是，我们无法轻易估计<span class="math inline">\(q(X_{t-1}|X_t)\)</span>，因为它需要使用整个数据集，因此我们需要学习一个模型<span class="math inline">\(p_\theta\)</span>来逼近这些条件概率，以便运行反向扩散过程。</p><h3 id="后验的扩散条件概率">4.3 后验的扩散条件概率</h3><p><span class="math inline">\(q(X_{t-1}|X_t,X_0)\)</span>分布式可以用公式表示的，可就是说给定<span class="math inline">\(X_t,X_0\)</span>，就能够计算出<span class="math inline">\(X_{t-1}\)</span> <span class="math display">\[q(X_{t-1}|X_t) = \N(X_{t-1};\bar \mu(X_t,X_0),\bar \beta_t I)\]</span></p><p><span class="math display">\[q(X_{t-1}|X_t,X_0)=q(X_t|X_{t-1},X_0)\frac{q(X_{t-1}|X_0)}{q((X_t|X_0))}\]</span></p><p><span class="math display">\[q(X_{t-1}|X_0)\Longrightarrow    \sqrt{\bar \alpha_{t-1}}x_0+\sqrt{1-\bar \alpha_{t-1}}z \sim N(\sqrt{\bar \alpha_{t-1}}x_0,1-\bar \alpha_{t-1})\]</span></p><p><span class="math display">\[q(X_{t}|X_0)\Longrightarrow  \sqrt{\bar \alpha_{t}}x_0+\sqrt{1-\bar \alpha_{t}}z \sim N(\sqrt{\bar \alpha_{t}}x_0,1-\bar \alpha_{t})\]</span></p><p><span class="math display">\[q(X_{t}|X_{t-1},X_0)\Longrightarrow  \sqrt{ \alpha_{t}}x_{t-1}+\sqrt{1- \alpha_{t}}z \sim N(\sqrt{ \alpha_{t}}x_0,1- \alpha_{t})\]</span></p><p>将式(16~18)代入式(15)得： <span class="math display">\[q(X_{t-1}|X_t,X_0)\propto \exp \left(-\frac{1}{2}\left(\frac{\left(\mathbf{x}_{t}-\sqrt{\alpha_{t}} \mathbf{x}_{t-1}\right)^{2}}{\beta_{t}}+\frac{\left(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t}}\right)\right)\]</span></p><p><span class="math display">\[\begin{array}{l}=\exp \left(-\frac{1}{2}\left(\frac{\mathbf{x}_{t}^{2}-2 \sqrt{\alpha_{t}} \mathbf{x}_{t} \mathbf{x}_{t-1}+\alpha_{t} \mathbf{x}_{t-1}^{2}}{\beta_{t}}+\frac{\mathbf{x}_{t-1}^{2}-2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} \mathbf{x}_{t-1}+\bar{\alpha}_{t-1} \mathbf{x}_{0}^{2}}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t}}\right)\right) \\=\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^{2}-\left(\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{2 \sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) \mathbf{x}_{t-1}+C\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)\right)\right) \end{array}\]</span></p><p><span class="math display">\[\exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)=\exp \left(-\frac{1}{2}\left(\frac{1}{\sigma^{2}} x^{2}-\frac{2 \mu}{\sigma^{2}} x+\frac{\mu^{2}}{\sigma^{2}}\right)\right)\]</span></p>由式(20)可知： $$<span class="math display">\[\begin{cases}\bar \beta_t = \frac{1}{\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar \alpha_{t-1}}} = \frac{1-\bar \alpha_{t-1}}{1-\bar \alpha_t} \cdot \beta_t  \\\bar\mu_t(X_t,X_0)=\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0} \\\end{cases}\]</span><p>$$</p><p>其中: <span class="math display">\[x_0=\frac{1}{\sqrt{\bar\alpha_t}}(x_t-\sqrt{1-\bar \alpha_t}z_t)\]</span> 将<span class="math inline">\(x_0\)</span>代入式（22）最终得到<span class="math inline">\(\tilde{\mu}_{t}\)</span></p><p><span class="math display">\[\tilde{\mu}_{t}=\frac{1}{\sqrt{a_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{a}_{t}}} {z}_{t}\right)\]</span> 其中<span class="math inline">\(z_t\)</span>是什么？是估计的每个时刻的噪声，即<span class="math inline">\(X_T\)</span>时刻的噪声</p><ul><li>虽然无法直接求解，但是能通过训练一个模型进行计算</li><li>采用Unet模型</li><li>模型的输入参数为当前时刻的分布和时刻t</li><li>模型的真实结果其实是由前向过程中添加的noise标签（显然是已知的）</li></ul><h3 id="目标数据分布的似然函数">4.4 目标数据分布的似然函数</h3><p>我们可以在负对数似然函数的基础上加上一个KL散度，于是就构成了负对数似然的上界，上界越小，负对数似然自然也就越小，那么对数似然就越大了 <span class="math display">\[\begin{aligned}-\log p_{\theta}({\mathbf x}_{0})&amp; \leq-\log p_\theta(\mathbf{x}_0)+D_{\mathrm{KL}}(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\|p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0))  \\&amp;=-\log p_\theta(\mathbf{x}_0)+\mathbb{E}_{\mathbf{x}_1:T\sim q(\mathbf{x}_1:T|\mathbf{x}_0)}\left[\log\dfrac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})/p_\theta(\mathbf{x}_0)}\right] \\&amp;=-\log p_\theta\mathfrak{(x}_0)+\mathbb{E}_q\Big[\log\dfrac{q(\mathbf{x}_1:T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0:T)}+\log p_\theta(\mathbf{x}_0)\Big] \\&amp;=\mathbb{E}_q\Big[\log\dfrac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})}\Big] \\\mathrm{Let}~L_{\mathrm{VLB}}&amp; =\mathbb{E}_{q(\mathbf{x}_0:T)}\Big[\log\dfrac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0:T)}\Big]\geq-\mathbb{E}_{q(\mathbf{x}_0)}\log p_\theta(\mathbf{x}_0) \end{aligned}\]</span> 进一步可以写出如上公式的交叉熵的上界，接下来，对交叉熵的上界进行化简 <span class="math display">\[q(X_t|X_{t-1}) =q(X_t|X_{t-1},X_0) = \frac{q(X_t,X_{t-1},X_0)}{q(X_{t-1},X_0)}=\frac{q(X_{t-1}|X_t,X_0)q(X_t|X_0)q(X_0)}{q(X_{t-1},X_0)}=\frac{q(X_{t-1}|X_t,X_0)q(X_t|X_0)}{q(X_{t-1}|X_0)}\]</span></p><p><span class="math display">\[\begin{aligned}L_{\mathrm{VLB}}&amp; =\mathbb{E}_{q(\mathbf{x}_{0:T)}}\Big[\log\frac{q(\mathbf{x}_{1:T}|\mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0:T})}\Big]  \\&amp;=\mathbb{E}_{q}\Big[\log{\frac{\prod_{t=1}^{T}q(\mathbf{x}_{t}|\mathbf{x}_{t-1})}{p_{\theta}(\mathbf{x}_{T})\prod_{t=1}^{T}p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})}}\Big] \\&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=1}^T\log\dfrac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}\Big] \\&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=2}^T\log\dfrac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x_k})}\Big] \\&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=2}^T\log\Big(\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}\cdot\dfrac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}\Big)+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}\Big] \\&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=2}^T\log\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+\sum_{t=2}^T\log\dfrac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}\Big] \\&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum\limits_{=2}^T\log\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+\log\dfrac{q(\mathbf{x}_T|\mathbf{x}_0)}{q(\mathbf{x}_1|\mathbf{x}_0)}+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}\Big] \\&amp;=\mathbb{E}_q\Big[\log\dfrac{q(\mathbf{x}_T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_T)}+\sum\limits_{t=2}^T\log\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)\Big] \\&amp;=\mathbb{E}_q[\underbrace{D_{\mathrm{KL}}(q(\mathbf{x}_T|\mathbf{x}_0)\parallel p_\theta(\mathbf{x}_T))}_{L_T}+\sum_{t=2}^T\underbrace{D_{\mathrm{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\parallel p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))}_{L_{t-1}}-\underbrace{\log p_0(\mathbf{x}_0|\mathbf{x}_1)]}_{L_0}\end{aligned}\]</span></p><p>这里论文将<span class="math inline">\(p_{\theta}(X_{t-1}|X_t)\)</span>分布的方差设置为一个与<span class="math inline">\(\beta\)</span>相关的常数，因此可训练的参数值存在于其均值中</p><p>对于两个单一变量的高斯分布p和q而言，他们的kl散度如式（3） <span class="math display">\[KL(p,q) = log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}\]</span></p><p><span class="math display">\[\begin{gathered}L_{t-1}=\mathbb{E}_{q}\left[{\frac{1}{2\sigma_{t}^{2}}}\|{\tilde{\mu}}_{t}(\mathbf{x}_{t},\mathbf{x}_{0})-{\boldsymbol{\mu}}_{\theta}(\mathbf{x}_{t},t)\|^{2}\right]+C \\ r_{t-1}-C=E_{x_{0},\kappa}\left[{\frac{1}{2\sigma_{t}^{2}}}\left\|{\tilde{\mu}}_{t}\left(\mathbf{x}_{t}(\mathbf{x}_{0},\epsilon),{\frac{1}{\sqrt{\alpha t}}}(\mathbf{x}_{0},\epsilon)-{\sqrt{1-{\bar{\alpha}}_{t}}}\epsilon)\right)-\mu_{0}(\mathbf{x_t}(\mathbf{x}_{0},\epsilon),t)\right\|^{2}\right] \\=\mathbb{E}_{\mathbf{x}_{0},\epsilon}\left[\dfrac{1}{2\sigma_{t}^{2}}\left\|\dfrac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}(\mathbf{x}_{0},\epsilon)-\dfrac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon\right)-\mu_{\theta}(\mathbf{x}_{t}(\mathbf{x}_{0},\epsilon),t)\right\|^{2}\right] \\ \mu_{\theta}(\mathbf{x}_{t},t)={\bar{\mu}}_{t}\left(\mathbf{x}_{t},{\frac{1}{\sqrt{\bar{\alpha}t}}}(\mathbf{x}_{t}-{\sqrt{1-{\bar{\alpha}}_{t}}}\epsilon_{\theta}(\mathbf{x}_{t}))\right)={\frac{1}{\sqrt{\alpha_{t}}}}\left(\mathbf{x}_{t}-{\frac{\beta_{t}}{\sqrt{1-{\bar{\alpha}}_{t}}}}\epsilon_{\theta}(\mathbf{x}_{t},t)\right) \\\mathbb{E}_{\mathbf{x}_{0},\epsilon}\left[\frac{\beta_{t}^{2}}{2\sigma_{t}^{2}\alpha_{t}(1-\bar{\alpha}_{t})}\left\|\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,t)\right\|^{2}\right] \\L_{\mathrm{simple}}(\theta):=\mathbb{E}_{t,\mathbf{x}_{0},\epsilon}\Big[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}(\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon},t)\right\|^{2}\Big] \end{gathered}\]</span></p><ul><li><span class="math inline">\(\tilde{\mu}_{t}\)</span>是可以计算得到的，所以可以用<span class="math inline">\(\mu_{\theta}\)</span>去预测<span class="math inline">\(\tilde{\mu}_{t}\)</span>网络，进而得到 loss function</li><li>预测均值的方法不太合理，预测噪声的方式更加合理。预测目标转移成了<span class="math inline">\(\varepsilon\)</span></li></ul><p><span class="math display">\[\mathbf x_t(\mathbf x_0,\varepsilon ) = \sqrt {\bar {\alpha_n} } \mathbf x_0 + \sqrt{1-\bar{\alpha_n}}\varepsilon \\ \varepsilon \sim\mathcal{N}(0,I)\]</span></p><p>然后式（24）可以转换为： <span class="math display">\[{\mu}_{\theta}(\mathbf x_t,t)=\frac{1}{\sqrt{a_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{a}_{t}}} \varepsilon_{\theta}(\mathbf x_t,t)\right)\]</span></p><h2 id="算法代码介绍">5 算法代码介绍</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305032258822.png" alt="image-20230503225817264" /><figcaption aria-hidden="true">image-20230503225817264</figcaption></figure><p>使用jupyter notebook 进行演示</p><h3 id="选择一个数据集">5.1 选择一个数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_s_curve</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">s_curve, _ = make_s_curve(<span class="number">10</span>**<span class="number">4</span>,noise=<span class="number">0.1</span>)</span><br><span class="line">s_curve = s_curve[:,[<span class="number">0</span>,<span class="number">2</span>]] / <span class="number">10.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;shape of moons:&quot;</span>,np.shape(s_curve))</span><br><span class="line"></span><br><span class="line">data = s_curve.T</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(*data,color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">datasets = torch.Tensor(s_curve).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure><h3 id="确定超参数的值">5.2 确定超参数的值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">100</span> <span class="comment"># 对于步骤，一开始可以由beta，分布的均值和标准差来共同确定</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定每一步的beta</span></span><br><span class="line">betas = torch.linspace(-<span class="number">6</span>,<span class="number">6</span>,num_steps)</span><br><span class="line">betas = torch.sigmoid(betas) * (<span class="number">0.5e-2</span> - <span class="number">1e-5</span>) + <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 alpha，alpha_prod,alpha_prod_previous,alpha_bar_sqrt等变量的值</span></span><br><span class="line">alphas = <span class="number">1</span> - betas</span><br><span class="line">alphas_prod = torch.cumprod(alphas,<span class="number">0</span>)</span><br><span class="line">alphas_prod_p = torch.cat([torch.tensor([<span class="number">1</span>]).<span class="built_in">float</span>(),alphas_prod[:-<span class="number">1</span>]], <span class="number">0</span>) <span class="comment"># p表示previous</span></span><br><span class="line">alphas_bar_sqrt = torch.sqrt(alphas_prod)</span><br><span class="line">one_minus_alphas_bar_log = torch.log(<span class="number">1</span> - alphas_prod)</span><br><span class="line">one_minus_alphas_bar_sqrt = torch.sqrt(<span class="number">1</span> - alphas_prod)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> alphas.shape == alphas_prod.shape == alphas_prod_p.shape == \</span><br><span class="line">alphas_bar_sqrt.shape == one_minus_alphas_bar_log.shape == one_minus_alphas_bar_sqrt.shape</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;all the same shape:&quot;</span>,betas.shape)</span><br></pre></td></tr></table></figure><h3 id="确定扩散过程任意时刻的采样值">5.3 确定扩散过程任意时刻的采样值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算任意时刻的x的采样值，基于x_0和参数重整化技巧</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">q_x</span>(<span class="params">x_0,t</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;可以基于x[0]得到任意时刻t的x[t]&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    noise = torch.randn_like(x_0) <span class="comment"># noise 是从正态分布中生成的随机噪声</span></span><br><span class="line">    alphas_t = alphas_bar_sqrt[t]</span><br><span class="line">    alphas_1_m_t = one_minus_alphas_bar_sqrt[t]</span><br><span class="line">    <span class="keyword">return</span> (alphas_t * x_0 + alphas_1_m_t * noise) <span class="comment"># 在x[0]的基础上添加噪声</span></span><br></pre></td></tr></table></figure><h3 id="演示原始数据分布加噪100步后的效果">5.4 演示原始数据分布加噪100步后的效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_shows = <span class="number">20</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">2</span>,<span class="number">10</span>,figsize=(<span class="number">28</span>,<span class="number">3</span>))</span><br><span class="line">plt.rc(<span class="string">&quot;text&quot;</span>,color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 共有10000个点，每个点包含两个坐标</span></span><br><span class="line"><span class="comment"># 生成100步以内每隔5步加噪声后的图像</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_shows):</span><br><span class="line">    j = i // <span class="number">10</span></span><br><span class="line">    k = i % <span class="number">10</span></span><br><span class="line">    q_i = q_x(datasets,torch.tensor([i*num_steps // num_shows])) <span class="comment"># 生成t时刻的采样数据</span></span><br><span class="line"></span><br><span class="line">    axs[j,k].scatter(q_i[:,<span class="number">0</span>],q_i[:,<span class="number">1</span>],color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    axs[j,k].set_axis_off()</span><br><span class="line">    axs[j,k].set_title(<span class="string">&quot;$q(\mathbf&#123;x&#125;_&#123;&quot;</span>+<span class="built_in">str</span>(i*num_steps//num_shows)+<span class="string">&quot;&#125;)$&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="编写拟合逆扩散过程高斯分布的模型">5.5 编写拟合逆扩散过程高斯分布的模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLPDiffusion</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,n_steps,num_units=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MLPDiffusion,self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.linears = nn.ModuleList(</span><br><span class="line">        [</span><br><span class="line">            nn.Linear(<span class="number">2</span>,num_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_units,num_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_units,num_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_units,<span class="number">2</span>),</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        self.step_embeddings = nn.ModuleList([</span><br><span class="line">            nn.Embedding(n_steps,num_units),</span><br><span class="line">            nn.Embedding(n_steps,num_units),</span><br><span class="line">            nn.Embedding(n_steps,num_units),</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x_0,t</span>):</span><br><span class="line">        x = x_0</span><br><span class="line">        <span class="keyword">for</span> idx,embedding_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.step_embeddings):</span><br><span class="line">            t_embedding = embedding_layer(t)</span><br><span class="line">            x = self.linears[<span class="number">2</span>*idx](x)</span><br><span class="line">            x += t_embedding</span><br><span class="line">            x = self.linears[<span class="number">2</span>*idx+<span class="number">1</span>](x)</span><br><span class="line">            </span><br><span class="line">        x = self.linears[-<span class="number">1</span>](x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="编写训练的误差函数">5.6 编写训练的误差函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">diffusion_loss_fn</span>(<span class="params">model,x_0,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,n_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对任意时刻t进行采样计算&quot;&quot;&quot;</span></span><br><span class="line">    batch_size = x_0.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机CIA杨一个时刻t，为了提高训练速度，这里我确保t不重复</span></span><br><span class="line">    </span><br><span class="line">    t = torch.randint(<span class="number">0</span>,n_steps,size=(batch_size//<span class="number">2</span>,))</span><br><span class="line">    t = torch.cat([t,n_steps-<span class="number">1</span>-t],dim=<span class="number">0</span>)</span><br><span class="line">    t = t.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># x0的系数</span></span><br><span class="line">    a = alphas_bar_sqrt[t]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># eps的系数</span></span><br><span class="line">    am1 = one_minus_alphas_bar_sqrt[t]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成随机噪声eps</span></span><br><span class="line">    e = torch.randn_like(x_0)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构造模型的输入</span></span><br><span class="line">    x = x_0 * a + e * am1</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 送入模型，得到t时刻的随机噪声预测值</span></span><br><span class="line">    output = model(x,t.squeeze(-<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 与真实噪声一起计算误差，求平均值</span></span><br><span class="line">    <span class="keyword">return</span> (e - output).square().mean()</span><br></pre></td></tr></table></figure><h3 id="编写逆扩散采样函数inference过程">5.7 编写逆扩散采样函数(inference过程)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_loop</span>(<span class="params">model,shape,n_steps,betas,one_minus_alphas_bar_sqrt</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从x[T]恢复x[T-1],x[T-2],...,x[0]&quot;&quot;&quot;</span></span><br><span class="line">    cur_x = torch.randn(shape)</span><br><span class="line">    x_seq = [cur_x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(n_steps)):</span><br><span class="line">        cur_x = p_sample(model,cur_x,i,betas,one_minus_alphas_bar_sqrt)</span><br><span class="line">        x_seq.append(cur_x)</span><br><span class="line">    <span class="keyword">return</span> x_seq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">model,x,t,betas,one_minus_alphas_bar_sqrt</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从x[T]采样t时刻的重构值&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    t = torch.tensor([t])</span><br><span class="line">    coeff = betas[t] / one_minus_alphas_bar_sqrt[t]</span><br><span class="line">    eps_theta = model(x,t)</span><br><span class="line">    mean = (<span class="number">1</span> / (<span class="number">1</span>-betas[t]).sqrt() * (x - (coeff * eps_theta)))</span><br><span class="line">    </span><br><span class="line">    z = torch.randn_like(x)</span><br><span class="line">    sigma_t = betas[t].sqrt()</span><br><span class="line">    sample = mean + sigma_t * z</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (sample)</span><br></pre></td></tr></table></figure><h3 id="开始训练模型并打印loss及中间的重构效果">5.8开始训练模型，并打印loss及中间的重构效果</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">seed = 42</span><br><span class="line">class EMA():</span><br><span class="line">    &quot;&quot;&quot;构建一个参数平滑器&quot;&quot;&quot;</span><br><span class="line">    def __init__(self,mu=0.01):</span><br><span class="line">        self.mu = mu</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line">    def register(self,name,val):</span><br><span class="line">        self.shadow[name] = val.clone()</span><br><span class="line">        </span><br><span class="line">    def __cell__(self,name,x):</span><br><span class="line">        assert name in self.shadow</span><br><span class="line">        new_average = self.mu * x + (1.0 - self.mu) * self.shadow[name]</span><br><span class="line">        self.shadow[name] = new_average.clone()</span><br><span class="line">        return new_average</span><br><span class="line">    </span><br><span class="line">print(&quot;training model...&quot;)</span><br><span class="line"></span><br><span class="line">batch_size = 128</span><br><span class="line">dataloader = torch.utils.data.DataLoader(datasets,batch_size=batch_size,shuffle=True)</span><br><span class="line">dataloader = dataloader.cuda()</span><br><span class="line">num_epoch = 4000</span><br><span class="line">plt.rc(&quot;text&quot;,color=&quot;blue&quot;)</span><br><span class="line"></span><br><span class="line">model = MLPDiffusion(num_steps).cuda() # 输出维度是2，输入是x和step</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)</span><br><span class="line"></span><br><span class="line">for t in range(num_epoch):</span><br><span class="line">    for idx,batch_x in enumerate(dataloader):</span><br><span class="line">        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(),1.)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    # print loss</span><br><span class="line">    if (t % 100 == 0):</span><br><span class="line">        print(loss)</span><br><span class="line">        x_seq = p_sample_loop(model,datasets.shape,num_steps,betas,one_minus_alphas_bar_sqrt) #共有100个元素</span><br><span class="line">        </span><br><span class="line">        fig, axs = plt.subplots(1,10,figsize=(28,3))</span><br><span class="line">        for i in range(1,11):</span><br><span class="line">            cur_x = x_seq[i * 10].detach()</span><br><span class="line">            axs[i-1].scatter(cur_x[:,0],cur_x[:,1],color=&quot;red&quot;,edgecolor=&quot;white&quot;)</span><br><span class="line">            axs[i-1].set_axis_off()</span><br><span class="line">            axs[i-1].set_title(&quot;$q(\mathbf&#123;x&#125;_&#123;&quot;+str(i*10)+&quot;&#125;)$&quot;)</span><br></pre></td></tr></table></figure><h3 id="动画演示">5.9 动画演示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generating the forward image sequence 生成前向过程，也就是逐步加噪声</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    plt.clf()</span><br><span class="line">    q_i = q_x(datasets,torch.tensor([i]))</span><br><span class="line">    plt.scatter(q_i[:,<span class="number">0</span>],q_i[:,<span class="number">1</span>],color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>,s=<span class="number">5</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    img_buf = io.BytesIO()</span><br><span class="line">    plt.savefig(img_buf,<span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_buf)</span><br><span class="line">    imgs.append(img)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># generating the reverse diffusion sequence</span></span><br><span class="line">reverse = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    plt.clf()</span><br><span class="line">    cur_x = x_seq[i].detach() <span class="comment"># 拿到训练末尾阶段生成的x_seq</span></span><br><span class="line">    plt.scatter(q_i[:,<span class="number">0</span>],q_i[:,<span class="number">1</span>],color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>,s=<span class="number">5</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    img_buf = io.BytesIO()</span><br><span class="line">    plt.savefig(img_buf,<span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_buf)</span><br><span class="line">    reverse.append(img)</span><br><span class="line">    </span><br><span class="line"> imgs = imgs + reverse</span><br><span class="line"> imgs[<span class="number">0</span>].save(<span class="string">&quot;diffusion.gif&quot;</span>,<span class="built_in">format</span>=<span class="string">&quot;GIF&quot;</span>,append_images=imgs,save_all=<span class="literal">True</span>,duration=<span class="number">100</span>,loop=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Denoising Diffusion Probabilistic Models&lt;/p&gt;
&lt;p&gt;https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pd</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="diffusion model" scheme="https://wangtongyouwen.github.io/categories/pytorch/diffusion-model/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="diffusion model" scheme="https://wangtongyouwen.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络12-Unet</title>
    <link href="https://wangtongyouwen.github.io/post/94472398.html"/>
    <id>https://wangtongyouwen.github.io/post/94472398.html</id>
    <published>2023-04-30T06:12:22.000Z</published>
    <updated>2023-04-30T13:07:07.759Z</updated>
    
    <content type="html"><![CDATA[<h1 id="u-net-convolutional-networks-for-biomedical-image-segmentation">U-Net: Convolutional Networks for Biomedical Image Segmentation</h1><p>https://arxiv.org/pdf/1505.04597.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301414602.png" alt="image-20230430141414836" /><figcaption aria-hidden="true">image-20230430141414836</figcaption></figure><ul><li>为什么输入图片和输出图片的大小不同？---因为输入的图片本质也是388*388的，只是通过了一定的填充方式变成了572*572。这是为了让边缘的像素具有更好的上下文信息。</li></ul><p>example：</p><p>https://github.com/yassouali/pytorch-segmentation</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;u-net-convolutional-networks-for-biomedical-image-segmentation&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/h1&gt;</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门24-损失函数</title>
    <link href="https://wangtongyouwen.github.io/post/24351970.html"/>
    <id>https://wangtongyouwen.github.io/post/24351970.html</id>
    <published>2023-04-27T09:28:25.000Z</published>
    <updated>2023-04-30T13:12:16.502Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># logits shape: [BS,NC]</span></span><br><span class="line">batchsize = <span class="number">2</span></span><br><span class="line">num_class = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">logits = torch.randn(batchsize,num_class)</span><br><span class="line">target = torch.randint(num_class,size=(batchsize,)) <span class="comment"># delta目标分布</span></span><br><span class="line">target_logits = torch.randn(batchsize,num_class) <span class="comment"># 非delta目标分布</span></span><br></pre></td></tr></table></figure><h1 id="crossentropyloss">CROSSENTROPYLOSS</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304271739035.png" alt="image-20230427173911712" /><figcaption aria-hidden="true">image-20230427173911712</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304271743417.png" alt="image-20230427174342229" /><figcaption aria-hidden="true">image-20230427174342229</figcaption></figure><p>https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.调用 cross entropy loss (CE loss)</span></span><br><span class="line"><span class="comment"># 第一种方法调用CEloss</span></span><br><span class="line">ce_loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">ce_loss = ce_loss_fn(logits,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;cross entropy loss: <span class="subst">&#123;ce_loss&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 第二种方法</span></span><br><span class="line">ce_loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">ce_loss = ce_loss_fn(logits,target_logits)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;cross entropy loss: <span class="subst">&#123;ce_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="nllloss">NLLLOSS</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304271754467.png" alt="image-20230427175450218" /><figcaption aria-hidden="true">image-20230427175450218</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.调用Negative Log Likelihood Loss (NLL loss)</span></span><br><span class="line">nll_fn = nn.NLLLoss()</span><br><span class="line">nll_loss = nll_fn(torch.log(torch.softmax(logits,-<span class="number">1</span>)),target_indices)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;neagative log-likelihood loss: <span class="subst">&#123;nll_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>如果得到的logits是没有归一化的值，那么使用CEloss</li><li>如果得到的logits是已经softmax后的log概率值值，则使用NLLloss</li><li>总之，两个损失函数都是用来计算分类问题的</li></ul><p>In classification problems we want to estimate the probability of different outcomes. Let the estimated probability of outcome<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" alt="i" /> be <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/187f4094cdfe699a627b3166c870d4e80a3ddbc9" alt="{q_{}(X=i)}" /> with to-be-optimized parameters <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="" /> and let the frequency (empirical probability) of outcome in the training set be <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4adae4de2fee09ec86818003df233cee809e070" alt="{p(X=i)}" />. Given N <a href="https://en.wikipedia.org/wiki/Conditionally_independent">conditionally independent</a> samples in the training set, then the <a href="https://en.wikipedia.org/wiki/Likelihood">likelihood</a> of the parameters <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="" /> of the model <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ab8eaa7d05b1279a2c3bb4c4451a2305b5de380" alt="{q_{}(X=x)}" /> on the training set is</p><figure><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d452937ed5a5bdfdac361ad273d0c55868f0cdd3" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p>where the last expression is due to the definition of the multinomial PMF. Therefore, the log-likelihood, divided by <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3" alt="N" /> is</p><figure><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9601b9fef9a3e4c9bb43553b1b3a1d523c0f3dfa" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p>so that <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximizing the likelihood</a> with respect to the parameters <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="" /> is the same as minimizing the cross-entropy.</p><h1 id="kullback-leibler-divergence-loss">Kullback-Leibler divergence loss</h1><p>https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301259064.png" alt="image-20230430125950441" /><figcaption aria-hidden="true">image-20230430125950441</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 调用 KL divergence loss(KL loss)</span></span><br><span class="line">KL_loss_fn = nn.KLDivLoss()</span><br><span class="line">kld_loss = KL_loss_fn(torch.log(torch.softmax(logits,-<span class="number">1</span>)),torch.softmax(target_logits,dim=-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;KL divergence loss: <span class="subst">&#123;kld_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 4. CE = IE(信息熵) + KLD </span><br><span class="line">ce_loss_fn_smaple = nn.CrossEntropyLoss(reduction=&quot;none&quot;)</span><br><span class="line">ce_loss_sample = ce_loss_fn_smaple(logits,torch.softmax(target_logits,dim=-1))</span><br><span class="line">print(f&quot;cross entropy loss sample:&#123;ce_loss_sample&#125;&quot;)</span><br><span class="line"></span><br><span class="line">kld_loss_fn_sample = nn.KLDivLoss(reduction=&quot;none&quot;)</span><br><span class="line">kld_loss_sample = kld_loss_fn_sample(torch.log(torch.softmax(logits,-1)),torch.softmax(target_logits,dim=-1)).sum(-1)</span><br><span class="line">print(f&quot;KL divergence loss sample:&#123;kld_loss_sample&#125;&quot;)</span><br><span class="line"></span><br><span class="line">target_information_entropy = torch.distributions.Categorical(probs=torch.softmax(target_logits,dim=-1)).entropy()</span><br><span class="line">print(f&quot;information entropy sample:&#123;target_information_entropy&#125;&quot;)</span><br><span class="line"></span><br><span class="line">torch.allclose(ce_loss_sample,kld_loss_sample+target_information_entropy)</span><br></pre></td></tr></table></figure><h1 id="bceloss">BCELOSS</h1><p>https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. 调用Binary Cross Entropy loss （BCE loss）</span></span><br><span class="line">bce_loss_fn = nn.BCELoss()</span><br><span class="line">logits = torch.randn(batchsize)</span><br><span class="line">prob_1 = torch.sigmoid(logits)</span><br><span class="line">target = torch.randint(<span class="number">2</span>,size=(batchsize,))</span><br><span class="line">bce_loss = bce_loss_fn(prob_1,target.<span class="built_in">float</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;binary cross entropy loss: <span class="subst">&#123;bce_loss&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 用 NLL loss 代替 BCE loss</span></span><br><span class="line">prob_0 = <span class="number">1</span> - prob_1.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">prob = torch.cat([prob_0,prob_1.unsqueeze(-<span class="number">1</span>)],dim=-<span class="number">1</span>)</span><br><span class="line">nll_loss_binary = nll_fn(torch.log(prob),target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;negative likelihood loss binary:<span class="subst">&#123;nll_loss_binary&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="cosineembeddingloss">COSINEEMBEDDINGLOSS</h1><p>https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301347056.png" alt="image-20230430134714165" /><figcaption aria-hidden="true">image-20230430134714165</figcaption></figure><ul><li>计算相似度匹配（图片检索任务：找出相似度最相似的）</li><li>对比学习，自监督学习</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络3-transformer</title>
    <link href="https://wangtongyouwen.github.io/post/2fee727b.html"/>
    <id>https://wangtongyouwen.github.io/post/2fee727b.html</id>
    <published>2023-04-27T09:28:25.000Z</published>
    <updated>2023-04-30T13:07:07.761Z</updated>
    
    <content type="html"><![CDATA[<p>https://arxiv.org/pdf/1706.03762.pdf</p><p>https://nlp.seas.harvard.edu/2018/04/03/attention.html</p><h3 id="attention-is-all-you-need-----transformer">attention is all you need ---&gt; transformer</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725197.png" alt="image-20230408205122663" style="zoom: 80%;" /></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725367.png" alt="image-20230408205258220" style="zoom:50%;" /></p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082110329.jpg" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082209301.png" alt="image-20230408220929314" style="zoom:80%;" /></p><h3 id="pytorch-源码">1 pytorch 源码</h3><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span><br><span class="line">        &gt;&gt;&gt; src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">             num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">             activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">             custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">             device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">    <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.encoder = custom_encoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.decoder = custom_decoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">    self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.nhead = nhead</span><br><span class="line"></span><br><span class="line">    self.batch_first = batch_first</span><br></pre></td></tr></table></figure><p>其中初始化部分最为重要的四部分：TransformerEncoderLayer（通过encoder_layer连接），TransformerDecoderLayer（通过decoder_layer连接）</p><h4 id="transformerencoderlayer">1.1 TransformerEncoderLayer</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Args:</span><br><span class="line">    d_model: the number of expected features <span class="keyword">in</span> the input (required).</span><br><span class="line">    nhead: the number of heads <span class="keyword">in</span> the multiheadattention models (required).</span><br><span class="line">    dim_feedforward: the dimension of the feedforward network model (default=2048).</span><br><span class="line">    dropout: the dropout value (default=0.1).</span><br><span class="line">    activation: the activation <span class="keyword">function</span> of the intermediate layer, can be a string</span><br><span class="line">        (<span class="string">&quot;relu&quot;</span> or <span class="string">&quot;gelu&quot;</span>) or a unary callable. Default: relu</span><br><span class="line">    layer_norm_eps: the eps value <span class="keyword">in</span> layer normalization components (default=1e-5).</span><br><span class="line">    batch_first: If ``True``, <span class="keyword">then</span> the input and output tensors are provided</span><br><span class="line">        as (batch, <span class="built_in">seq</span>, feature). Default: ``False`` (<span class="built_in">seq</span>, batch, feature).</span><br><span class="line">    norm_first: <span class="keyword">if</span> ``True``, layer norm is <span class="keyword">done</span> prior to attention and feedforward</span><br><span class="line">        operations, respectively. Otherwise it<span class="string">&#x27;s done after. Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Examples::</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">              activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">              layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">              device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">     factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">     <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">     self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                         **factory_kwargs)</span><br><span class="line">     <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">     self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">     self.dropout = Dropout(dropout)</span><br><span class="line">     self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">     self.norm_first = norm_first</span><br><span class="line">     self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.dropout1 = Dropout(dropout)</span><br><span class="line">     self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">         activation = _get_activation_fn(activation)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># We can&#x27;t test self.activation in forward() in TorchScript,</span></span><br><span class="line">     <span class="comment"># so stash some information about it instead.</span></span><br><span class="line">     <span class="keyword">if</span> activation <span class="keyword">is</span> F.relu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.ReLU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">1</span></span><br><span class="line">     <span class="keyword">elif</span> activation <span class="keyword">is</span> F.gelu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.GELU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">2</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">0</span></span><br><span class="line">     self.activation = activation</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">x = src</span><br><span class="line">     <span class="keyword">if</span> self.norm_first:</span><br><span class="line">         x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">         x = x + self._ff_block(self.norm2(x))</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">         x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> x</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h4 id="transformerencoder">1.2 TransformerEncoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers. Users can build the</span></span><br><span class="line"><span class="string">    BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        encoder_layer: an instance of the TransformerEncoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-encoder-layers in the encoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string">        enable_nested_tensor: if True, input will automatically convert to nested tensor</span></span><br><span class="line"><span class="string">            (and convert back on output). This will improve the overall performance of</span></span><br><span class="line"><span class="string">            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span>, enable_nested_tensor=<span class="literal">True</span>, mask_check=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">        self.enable_nested_tensor = enable_nested_tensor</span><br><span class="line">        self.mask_check = mask_check</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="transformerdecoderlayer">1.3 TransformerDecoderLayer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of the intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, layer norm is done prior to self attention, multihead</span></span><br><span class="line"><span class="string">            attention and feedforward operations, respectively. Otherwise it&#x27;s done after.</span></span><br><span class="line"><span class="string">            Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">        memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">        tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">        memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">        tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">        memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        see the docs in Transformer class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">    x = tgt</span><br><span class="line">    <span class="keyword">if</span> self.norm_first:</span><br><span class="line">        x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">        x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">        x = x + self._ff_block(self.norm3(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">        x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">        x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="transformerdecoder">1.4 TransformerDecoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        decoder_layer: an instance of the TransformerDecoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-decoder-layers in the decoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><em>Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.</em></p><figure><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption></figure><p>在进行解码过程中，第一个词的预测只与第一个词有关，因此最后的的attention机制是个上三角的形式，如上图所示。</p><h4 id="attention">1.5 Attention</h4><figure><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption></figure><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span>, and values of dimension <span class="math inline">\(d_v\)</span>. We compute the dot products of the query with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax function to obtain the weights on the values.</p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We compute the matrix of outputs as: <span class="math display">\[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304100940797.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="encoder-细节">2 Encoder 细节</h3><h4 id="word-embedding">2.1 word embedding</h4><p>考虑 source sentence 和 target sentence 构建序列，序列的字符以其词表中的索引的形式表示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source sentence 和 target sentence 的初始长度</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 单词表大小</span></span><br><span class="line">max_num_src_words = <span class="number">8</span></span><br><span class="line">max_num_tgt_words = <span class="number">8</span></span><br><span class="line">model_dim = <span class="number">8</span> <span class="comment"># 特征大小，原文是512</span></span><br><span class="line"><span class="comment"># 序列最大长度</span></span><br><span class="line">max_src_seq_len = <span class="number">5</span></span><br><span class="line">max_tgt_seq_len = <span class="number">5</span></span><br><span class="line">max_position_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># src_len = torch.randint(2,5,(batch_size,))</span></span><br><span class="line"><span class="comment"># tgt_len = torch.randint(2,5,(batch_size,)) </span></span><br><span class="line">src_len = torch.Tensor([<span class="number">2</span>,<span class="number">4</span>]).to(torch.int32)  <span class="comment"># 句子长度（2个句子）</span></span><br><span class="line">tgt_len = torch.Tensor([<span class="number">4</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line"><span class="built_in">print</span>(src_len,tgt_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 单词索引构成的源句子和目标句子,pad为最大序列长度,unsqueeze 变为2维张量，然后使用cat拼接起来,padding 默认值为0,构建batch</span></span><br><span class="line">src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_src_words,(L,)),(<span class="number">0</span>,max_src_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]) </span><br><span class="line">tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_tgt_words,(L,)),(<span class="number">0</span>,max_tgt_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_seq,<span class="string">&quot;\n&quot;</span>,tgt_seq,end=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2 构造 word embedding</span></span><br><span class="line"><span class="comment"># 第0行是默认padding的0，第1-9行是每个单词的embedding结果</span></span><br><span class="line">src_embedding_table = nn.Embedding(max_num_src_words + <span class="number">1</span>, model_dim)</span><br><span class="line">tgt_embedding_table = nn.Embedding(max_num_tgt_words + <span class="number">1</span>, model_dim)</span><br><span class="line">src_embedding = src_embedding_table(src_seq) <span class="comment"># embedding 的 forward 方法</span></span><br><span class="line">stgt_embedding = tgt_embedding_table(tgt_seq) </span><br><span class="line"><span class="built_in">print</span>(src_embedding_table.weight)</span><br><span class="line"><span class="built_in">print</span>(src_seq)</span><br><span class="line"><span class="built_in">print</span>(src_embedding)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="position-embedding">2.2 position embedding</h4><p><span class="math display">\[\begin{aligned}P E_{(p o s, 2 i)} &amp; =\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\P E_{(p o s, 2 i+1)} &amp; =\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)\end{aligned}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 构建 position embedding</span></span><br><span class="line">pos_mat = torch.arange(max_position_len).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">i_mat = torch.<span class="built_in">pow</span>(<span class="number">10000</span>,torch.arange(<span class="number">0</span>,model_dim,<span class="number">2</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)/model_dim)</span><br><span class="line">pe_embedding_table = torch.zeros(max_position_len,model_dim)</span><br><span class="line"><span class="comment"># element point</span></span><br><span class="line">pe_embedding_table[:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos_mat/i_mat)  <span class="comment"># 偶数行</span></span><br><span class="line">pe_embedding_table[:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos_mat/i_mat)  <span class="comment"># 奇数行</span></span><br><span class="line"><span class="built_in">print</span>(pos_mat,<span class="string">&#x27;\n&#x27;</span>,i_mat,<span class="string">&#x27;\n&#x27;</span>,pe_embedding_table)</span><br><span class="line"></span><br><span class="line">src_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(src_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> src_len]).to(torch.int32)</span><br><span class="line">tgt_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(tgt_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> tgt_len]).to(torch.int32)</span><br><span class="line"></span><br><span class="line">src_pe_embedding = pe_embedding(src_pos)</span><br><span class="line">tgt_pe_embedding = pe_embedding(tgt_pos)</span><br><span class="line"><span class="built_in">print</span>(src_pe_embedding)</span><br><span class="line"><span class="built_in">print</span>(tgt_pe_embedding)</span><br></pre></td></tr></table></figure><p><span class="math inline">\(10000^{2 i / d_{\mathrm{model}}}\)</span> 表示为<span class="math inline">\(\omega_k\)</span>，pos表示为<span class="math inline">\(t\)</span></p><p>解决 out of demain: 如果是超出序列长度，可以通过之前序列长度的线性组合来表示</p><p>For every sine-cosine pair corresponding to frequency <span class="math inline">\(\omega_k\)</span>, there is a linear transformation $ M ^{2 } $(independent of t) where the following equation holds:</p>$$ M <span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega _k \cdot (t+\phi))\\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}\]</span><p>$$ proof:</p>Let <span class="math inline">\(M\)</span> be a <span class="math inline">\(2\times2\)</span> matrix, we want to find <span class="math inline">\(u_1,v_1,u_2\)</span> and <span class="math inline">\(v_2\)</span> so that: $$<span class="math display">\[\begin{bmatrix} u_1 &amp;v_1 \\ u_2 &amp; v_2\end{bmatrix}\]</span><span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega _k \cdot (t+\phi))\\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}\]</span><span class="math display">\[By applying the addition theorem, we can expand the right hand side as follows:\]</span><span class="math display">\[\begin{bmatrix} u_1 &amp;v_1 \\ u_2 &amp; v_2\end{bmatrix}\]</span><span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega_k \cdot t)cos(\omega_k \cdot \phi) + cos(\omega_k \cdot t)sin(\omega_k \cdot \phi)         \\cos(\omega_k \cdot t)cos(\omega_k \cdot \phi)  - sin(\omega_k \cdot t)sin(\omega_k \cdot \phi)\end{bmatrix}\]</span><span class="math display">\[Which result in the following two equations:\]</span><span class="math display">\[\begin{array}{l}u_{1} \sin \left(\omega_{k} \cdot t\right)+v_{1} \cos \left(\omega_{k} \cdot t\right)=\cos \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\sin \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right) \\u_{2} \sin \left(\omega_{k} \cdot t\right)+v_{2} \cos \left(\omega_{k} \cdot t\right)=-\sin \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\cos \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right)\end{array}\]</span><span class="math display">\[By solving above equations, we get:\]</span><span class="math display">\[\begin{aligned}u_{1}=\cos \left(\omega_{k} \cdot \phi\right) ， v_{1}=\sin \left(\omega_{k} \cdot \phi\right) \\u_{2}=-\sin \left(\omega_{k} \cdot \phi\right) ， v_{2}=\cos \left(\omega_{k} \cdot \phi\right)\end{aligned}\]</span><span class="math display">\[So the final transformation matrix M is:\]</span> M_{,k}=<span class="math display">\[\begin{bmatrix} cos(\omega_k,\phi) &amp; sin(\omega_k,\phi) \\- sin(\omega_k,\phi) &amp; cos(\omega_k,\phi)\end{bmatrix}\]</span><p>$$</p><h4 id="构建encoder的self-attention-mask">2.3 构建encoder的self-attention mask</h4><p>mask的shape：[batch_size,max_src_len,max_tgt_len] 值为1或-inf</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 构建encoder的self-attention mask</span></span><br><span class="line">valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(src_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]),<span class="number">2</span>) <span class="comment"># 有效长度</span></span><br><span class="line">valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效矩阵</span></span><br><span class="line">invalid_encoder_pos_matrix = <span class="number">1</span> - valid_encoder_pos_matrix</span><br><span class="line">mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.<span class="built_in">bool</span>) <span class="comment"># 变为bool</span></span><br><span class="line"><span class="built_in">print</span>(valid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,invalid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,mask_encoder_self_attention) <span class="comment">#(batchsize,maxlen after padding,_)</span></span><br><span class="line"><span class="comment"># true 需要 mask</span></span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(src_len),<span class="built_in">max</span>(src_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_score = score.masked_fill(mask_encoder_self_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_len)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="built_in">print</span>(masked_score)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br><span class="line"><span class="comment"># 无需因果的遮掩</span></span><br></pre></td></tr></table></figure><h4 id="scaled-的重要性">2.4 scaled 的重要性</h4><p><span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]</span></p><p>这里的softmax中为什么要除以<span class="math inline">\(\sqrt{d_k}\)</span>?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># when the varience of prob is too big</span></span><br><span class="line">alpha1 = <span class="number">0.1</span></span><br><span class="line">alpha2 = <span class="number">10</span></span><br><span class="line">score = torch.randn(<span class="number">5</span>)</span><br><span class="line">prob1 = F.softmax(score*alpha1,-<span class="number">1</span>)</span><br><span class="line">prob2 = F.softmax(score*alpha2,-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_func</span>(<span class="params">score</span>):</span><br><span class="line">    <span class="keyword">return</span> F.softmax(score)</span><br><span class="line">jaco_mat1 = torch.autograd.functional.jacobian(softmax_func,score*alpha1)</span><br><span class="line">jaco_mat2 = torch.autograd.functional.jacobian(softmax_func,score*alpha2)</span><br><span class="line"><span class="comment"># jaco matrix is close to zero when the varience is too big</span></span><br><span class="line"><span class="comment"># print(score,prob1,prob2)</span></span><br><span class="line"><span class="built_in">print</span>(jaco_mat1,<span class="string">&#x27;\n&#x27;</span>,jaco_mat2)</span><br></pre></td></tr></table></figure><h3 id="decoder-细节">3 decoder 细节</h3><h4 id="intra-attention-的-mask">3.1 intra-attention 的 mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step5 构造intra-attention的mask</span></span><br><span class="line"><span class="comment"># Q @ k^T shape:(batch_size,tgt_seq_len,src_seq_len)</span></span><br><span class="line">valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len]),<span class="number">2</span>)</span><br><span class="line">valid_cross_pos_matrix = torch.bmm(valid_decoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效位置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;源序列有效位置张量：&quot;</span>,valid_encoder_pos,<span class="string">&quot;\n 目标序列有效位置张量：&quot;</span>,valid_decoder_pos,<span class="string">&quot;\n 目标序列对源头序列有效位置张量：&quot;</span>,valid_cross_pos)</span><br><span class="line"></span><br><span class="line">invalid_cross_pos_matrix = <span class="number">1</span> - valid_cross_pos_matrix</span><br><span class="line">mask_cross_attention = invalid_cross_pos_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(mask_cross_attention)</span><br><span class="line"><span class="comment"># print(valid_cross_pos)</span></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_cross_score = score.masked_fill(mask_cross_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob_cross = F.softmax(masked_cross_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(prob_cross)</span><br></pre></td></tr></table></figure><h4 id="decoder-self-attention">3.2 decoder self-attention</h4><p>下三角形的mask：防止因果</p><p>要把答案遮住，如果预测第四个位置，就要把第四个位置以后的所有内容都遮住。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step6 构造 decoder self-attention 的 mask</span></span><br><span class="line">valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L,<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">invalid_decoder_tri_matrix = <span class="number">1</span> - valid_decoder_tri_matrix</span><br><span class="line">invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(valid_decoder_tri_matrix,invalid_decoder_tri_matrix)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line">masked_score = score.masked_fill(invalid_decoder_tri_matrix,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(tgt_len)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br></pre></td></tr></table></figure><p>流式预测的时候，特别需要这个掩码。</p><h4 id="scaled-self-attention">3.3 scaled self-attention</h4><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101603265.png" alt="image-20230410160332412" /><figcaption aria-hidden="true">image-20230410160332412</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q,K,V,attn_mask</span>):</span><br><span class="line">    <span class="comment"># shape pf Q,k,V: (batch_size * num_head,seq_len,model_dim/num_head)</span></span><br><span class="line">    score = torch.bmn(Q,K.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/torch.sqrt(model_dim)</span><br><span class="line">    masked_score = score.masked_fill(attn_mask,-<span class="number">1e9</span>)</span><br><span class="line">    prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line">    context = torch.bmn(prov,V)</span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>源码：D:\0_python-packages.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multi_head_attention_forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    query: Tensor,</span></span><br><span class="line"><span class="params">    key: Tensor,</span></span><br><span class="line"><span class="params">    value: Tensor,</span></span><br><span class="line"><span class="params">    embed_dim_to_check: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    in_proj_weight: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    in_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_k: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_v: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    add_zero_attn: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">    dropout_p: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    out_proj_weight: Tensor,</span></span><br><span class="line"><span class="params">    out_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    training: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    use_separate_proj_weight: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    q_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    k_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    v_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_k: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_v: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    average_attn_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101653337.png" alt="image-20230410165300158" /><figcaption aria-hidden="true">image-20230410165300158</figcaption></figure><h3 id="loss-function">4 Loss function</h3><p>https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101703643.png" alt="image-20230410170322549" /><figcaption aria-hidden="true">image-20230410170322549</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bath_size = <span class="number">2</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">vocab_size = <span class="number">4</span></span><br><span class="line">logits = torch.randn(bath_size,seq_len,vocab_size)      <span class="comment"># bath_size = 2, seq_len = 3, vocab_size = 4</span></span><br><span class="line">label = torch.randint(<span class="number">0</span>,vocab_size,(bath_size,seq_len))</span><br><span class="line">logits = logits.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">F.cross_entropy(logits,label) <span class="comment"># 六个单词的平均交叉熵</span></span><br><span class="line">F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) <span class="comment"># 返回所有单词的交叉熵</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mask</span></span><br><span class="line">tgt_len =torch.Tensor([<span class="number">2</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line">mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len)-L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">cross_entropy = F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) * mask </span><br><span class="line"><span class="built_in">print</span>(cross_entropy)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/p&gt;
&lt;p&gt;https://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;
&lt;h3 id=&quot;attention-is-all-you-nee</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目4-读取Excel/csv文件格式为PyTorch张量</title>
    <link href="https://wangtongyouwen.github.io/post/9358f042.html"/>
    <id>https://wangtongyouwen.github.io/post/9358f042.html</id>
    <published>2023-04-27T08:33:55.000Z</published>
    <updated>2023-05-05T11:59:32.158Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ExcelDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath=<span class="string">&quot;train.xlsx&quot;</span>, sheet_name=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;filepath&#125;</span>, sheet=<span class="subst">&#123;sheet_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath=<span class="string">&quot;train.xlsx&quot;</span>, sheet_name=<span class="number">0</span></span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;filepath&#125;</span>, sheet=<span class="subst">&#123;sheet_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            df = pandas.read_excel(</span><br><span class="line">                <span class="comment"># filepath,header=0,index_col=0,</span></span><br><span class="line">                filepath, header=<span class="number">0</span>,</span><br><span class="line">                names=[<span class="string">&#x27;admit&#x27;</span>, <span class="string">&#x27;gre&#x27;</span>, <span class="string">&#x27;gpa&#x27;</span>, <span class="string">&#x27;prestige&#x27;</span>],</span><br><span class="line">                sheet_name=sheet_name,</span><br><span class="line">                dtype=&#123;<span class="string">&quot;gre&quot;</span>: np.float32, <span class="string">&quot;gpa&quot;</span>: np.float32, <span class="string">&quot;admit&quot;</span>: np.int8, <span class="string">&quot;prestige&quot;</span>: np.string_&#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;the shape of dataframe is <span class="subst">&#123;df.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            feat = df.iloc[:, <span class="number">1</span>:<span class="number">3</span>].values</span><br><span class="line">            label = df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">            self.x = torch.from_numpy(feat)</span><br><span class="line">            self.y = torch.from_numpy(label)</span><br><span class="line">            <span class="comment"># print(feat,label)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">            <span class="keyword">return</span> self.x[index], self.y[index]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Csv2Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath=<span class="string">&quot;train.csv&quot;</span></span>):</span><br><span class="line">        <span class="comment"># there is no sheet name definition in csv format file</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;filepath&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file=filepath,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">        feat = []</span><br><span class="line">        label = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</span><br><span class="line">            values = line.strip().split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            row_feat = [<span class="built_in">float</span>(v) <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span>  <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> values[<span class="number">1</span>:<span class="number">2</span>]]</span><br><span class="line">            row_label = <span class="built_in">int</span>(values[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            feat.append(row_feat)</span><br><span class="line">            label.append(row_label)</span><br><span class="line">        feat = np.array(feat,dtype=np.float32)</span><br><span class="line">        label = np.array(label,dtype=np.int8)</span><br><span class="line"></span><br><span class="line">        self.x = torch.from_numpy(feat)</span><br><span class="line">        self.y = torch.from_numpy(label)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x[index], self.y[index]</span><br></pre></td></tr></table></figure><p>其中对缺省值进行处理，可以使用平均数来代替这个缺省值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目3-基于ResNet的水果蔬菜分类</title>
    <link href="https://wangtongyouwen.github.io/post/4627104a.html"/>
    <id>https://wangtongyouwen.github.io/post/4627104a.html</id>
    <published>2023-04-26T13:21:35.000Z</published>
    <updated>2023-05-06T07:00:06.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据集介绍">1 数据集介绍</h2><p>https://aistudio.baidu.com/aistudio/datasetdetail/119023</p><p>其中的图片有36各类，不同类别在不同的文件夹中，其中文件后缀有"jpg,png,JPG"</p><h2 id="数据集预处理">2 数据集预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Pre_Data</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 对所有图片进行RGB转换，并且统一调整到一致大小，但不让图片发生变形或扭曲,划分训练集和测试集&quot;&quot;&quot;</span></span><br><span class="line">    test_split_ratio = <span class="number">0.05</span></span><br><span class="line">    desired_size =<span class="number">128</span> <span class="comment"># 图片缩放后的同一大小</span></span><br><span class="line">    raw_path = <span class="string">&quot;./raw&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># class files in the raw data</span></span><br><span class="line">    dirs = glob.glob(os.path.join(raw_path,<span class="string">&quot;*&quot;</span>))</span><br><span class="line">    dirs = [d <span class="keyword">for</span> d <span class="keyword">in</span> dirs <span class="keyword">if</span> os.path.isdir(d)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(dirs)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(dirs)&#125;</span> classes: <span class="subst">&#123;dirs&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> path <span class="keyword">in</span> dirs:</span><br><span class="line">        <span class="comment"># 对每个类别单独处理</span></span><br><span class="line">        path = path.split(<span class="string">&quot;\\&quot;</span>)[-<span class="number">1</span>] <span class="comment"># classes</span></span><br><span class="line">        <span class="comment"># print(path)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f&quot;train/<span class="subst">&#123;path&#125;</span>&quot;</span>):</span><br><span class="line">            os.makedirs(<span class="string">f&quot;train/<span class="subst">&#123;path&#125;</span>&quot;</span>,exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>):</span><br><span class="line">            os.makedirs(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>,exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        files = glob.glob(os.path.join(raw_path,path,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">        files += glob.glob(os.path.join(raw_path,path,<span class="string">&quot;*.JPG&quot;</span>))</span><br><span class="line">        files += glob.glob(os.path.join(raw_path,path,<span class="string">&quot;*.png&quot;</span>))</span><br><span class="line">        <span class="comment"># print(files)</span></span><br><span class="line">        random.shuffle(files)</span><br><span class="line"></span><br><span class="line">        boundary = <span class="built_in">int</span>(<span class="built_in">len</span>(files)*test_split_ratio) <span class="comment"># 测试集和训练集的边界</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i,file <span class="keyword">in</span> <span class="built_in">enumerate</span>(files):</span><br><span class="line">            img = Image.<span class="built_in">open</span>(file).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">            <span class="comment"># print(img)</span></span><br><span class="line">            old_size = img.size <span class="comment"># old_size[0] is in (width,height) format</span></span><br><span class="line">            <span class="comment"># print(old_size)</span></span><br><span class="line"></span><br><span class="line">            ratio = <span class="built_in">float</span>(desired_size)/<span class="built_in">max</span>(old_size)</span><br><span class="line"></span><br><span class="line">            new_size = <span class="built_in">tuple</span>([<span class="built_in">int</span>(x*ratio) <span class="keyword">for</span> x <span class="keyword">in</span> old_size])</span><br><span class="line"></span><br><span class="line">            im = img.resize(new_size,Image.LANCZOS) <span class="comment"># 无模糊</span></span><br><span class="line"></span><br><span class="line">            new_im = Image.new(<span class="string">&quot;RGB&quot;</span>,(desired_size,desired_size))</span><br><span class="line">            new_im.paste(im,((desired_size-new_size[<span class="number">0</span>])//<span class="number">2</span>,</span><br><span class="line">                             (desired_size-new_size[<span class="number">1</span>])//<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">assert</span> new_im.mode == <span class="string">&quot;RGB&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &lt;= boundary:</span><br><span class="line">                new_im.save(os.path.join(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>,file.split(<span class="string">&quot;\\&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]+ <span class="string">&quot;.jpg&quot;</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">str</span>(get_filename(file)+<span class="string">&quot;.jpg&quot;</span>) <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(os.path.join(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>)):</span><br><span class="line">                    new_im.save(os.path.join(<span class="string">f&quot;train/<span class="subst">&#123;path&#125;</span>&quot;</span>,file.split(<span class="string">&quot;\\&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]+ <span class="string">&quot;.jpg&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;classes <span class="subst">&#123;path&#125;</span> is done !&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    test_files = glob.glob(os.path.join(<span class="string">&quot;test&quot;</span>,<span class="string">&quot;*&quot;</span>,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">    train_files = glob.glob(os.path.join(<span class="string">&quot;train&quot;</span>,<span class="string">&quot;*&quot;</span>,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(test_files)&#125;</span> files for testing&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(train_files)&#125;</span> files for training&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_mean_std</span>():</span><br><span class="line">    train_files = glob.glob(os.path.join(<span class="string">&quot;train&quot;</span>,<span class="string">&quot;*&quot;</span>,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(train_files)&#125;</span> files for training&quot;</span>)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> train_files:</span><br><span class="line">        img = Image.<span class="built_in">open</span>(file).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        img = np.array(img).astype(np.uint8)</span><br><span class="line">        img = img / <span class="number">255</span></span><br><span class="line">        result.append(img)</span><br><span class="line">    <span class="built_in">print</span>(np.shape(result)) <span class="comment"># [bs,H,W,C]</span></span><br><span class="line">    mean = np.mean(result,axis=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    std = np.std(result,axis=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(mean,std</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="eval">3 eval</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">data_loader, model, device</span>):</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    metric_logger = misc.MetricLogger(delimiter=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    header = <span class="string">&quot;test:&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># switch to evaluation mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> metric_logger.log_every(data_loader, <span class="number">10</span>, header):</span><br><span class="line">        <span class="comment"># print(batch)</span></span><br><span class="line">        images = batch[<span class="number">0</span>]</span><br><span class="line">        target = batch[-<span class="number">1</span>]</span><br><span class="line">        images = images.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">        target = target.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute output</span></span><br><span class="line">        output = model(images)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">        output = F.softmax(output, dim=-<span class="number">1</span>)</span><br><span class="line">        acc1, acc5 = accuracy(output, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">        batch_size = images.shape[<span class="number">0</span>]</span><br><span class="line">        metric_logger.update(loss=loss.item())</span><br><span class="line">        metric_logger.meters[<span class="string">&#x27;acc1&#x27;</span>].update(acc1.item(), n=batch_size)</span><br><span class="line">        metric_logger.meters[<span class="string">&#x27;acc5&#x27;</span>].update(acc5.item(), n=batch_size)</span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;* Acc@1 &#123;top1.global_avg:.3f&#125; Acc@5 &#123;top5.global_avg:.3f&#125; loss &#123;losses.global_avg:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))</span><br><span class="line">    <span class="keyword">return</span> &#123;k: meter.global_avg <span class="keyword">for</span> k, meter <span class="keyword">in</span> metric_logger.meters.items()&#125;</span><br></pre></td></tr></table></figure><h2 id="train">4 train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model: nn.Module, criterion: nn.Module, data_loader: Iterable, optimizer: torch.optim.Optimizer,</span></span><br><span class="line"><span class="params">                    device: torch.device, epoch: <span class="built_in">int</span>, loss_scaler, max_norm: <span class="built_in">float</span> = <span class="number">0</span>, log_writer=<span class="literal">None</span>, args=<span class="literal">None</span></span>):</span><br><span class="line">    model.train(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    accum_iter = args.accum_iter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;log_dir:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_writer.log_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data_iter_step, (samples, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        samples = samples.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">        targets = targets.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        outputs = model(samples)</span><br><span class="line"></span><br><span class="line">        warmup_lr = args.lr</span><br><span class="line">        optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>] = warmup_lr</span><br><span class="line"></span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line">        loss /= accum_iter</span><br><span class="line"></span><br><span class="line">        loss_scaler(loss, optimizer, clip_grad=max_norm,</span><br><span class="line">                    parameters=model.parameters(), create_graph=<span class="literal">False</span>,</span><br><span class="line">                    update_grad=(data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        loss_value = loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> math.isfinite(loss_value):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Loss is &#123;&#125;, stopping training&quot;</span>.<span class="built_in">format</span>(loss_value))</span><br><span class="line">            sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            <span class="string">&quot;&quot;&quot; We use epoch_1000x as the x-axis in tensorboard.</span></span><br><span class="line"><span class="string">            This calibrates different curves when batch size changes.</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            epoch_1000x = <span class="built_in">int</span>((data_iter_step / <span class="built_in">len</span>(data_loader) + epoch) * <span class="number">1000</span>)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;loss&#x27;</span>, loss_value, epoch_1000x)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;lr&#x27;</span>, warmup_lr, epoch_1000x)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch:<span class="subst">&#123;epoch&#125;</span>,Step: <span class="subst">&#123;data_iter_step&#125;</span>,loss: <span class="subst">&#123;loss&#125;</span>,lr: <span class="subst">&#123;warmup_lr&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="dataset">5 dataset</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def build_transform(is_train, args):</span><br><span class="line">    if is_train:</span><br><span class="line">        # this should always dispatch to transforms_imagenet_train</span><br><span class="line">        print(&quot;train transform&quot;)</span><br><span class="line">        return torchvision.transforms.Compose([</span><br><span class="line">            torchvision.transforms.Resize((args.input_size, args.input_size)),</span><br><span class="line">            torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">            torchvision.transforms.RandomVerticalFlip(),</span><br><span class="line">            torchvision.transforms.RandomPerspective(distortion_scale=0.6, p=1.0),</span><br><span class="line">            torchvision.transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),</span><br><span class="line">            torchvision.transforms.ToTensor(),</span><br><span class="line">        ])</span><br><span class="line">    # eval transform</span><br><span class="line">    print(&quot;eval transform&quot;)</span><br><span class="line">    return torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.Resize((args.input_size, args.input_size)),</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def build_dataset(is_train, args):</span><br><span class="line">    transform = build_transform(is_train, args)</span><br><span class="line">    path = os.path.join(args.root_path, &quot;train&quot; if is_train else &quot;test&quot;)</span><br><span class="line">    dataset = torchvision.datasets.ImageFolder(path, transform=transform)</span><br><span class="line">    info = dataset.find_classes(path)</span><br><span class="line">    print(f&quot;finding classes from &#123;path&#125;:\t &#123;info[0]&#125;&quot;)</span><br><span class="line">    print(f&quot;mapping classes from &#123;path&#125; to indexes:\t &#123;info[1]&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    return dataset</span><br></pre></td></tr></table></figure><h2 id="argparse">6 argparse</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_args_parser</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;MAE pre-training&#x27;</span>, add_help=<span class="literal">False</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">72</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, default=<span class="number">400</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--accum_iter&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Accumulate gradient iterations (for increasing the effective batch size under memory constraints)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Model parameters</span></span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--input_size&#x27;</span>, default=<span class="number">128</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;images input size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimizer parameters</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0001</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;weight decay (default: 0.0001)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0001</span>, metavar=<span class="string">&#x27;LR&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;learning rate (absolute lr)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dataset parameters</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--root-path&#x27;</span>, default=<span class="string">&#x27;./dataset_fruit_veg/&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where the train test pic is &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output_dir&#x27;</span>, default=<span class="string">&#x27;./output_dir_pretrained&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to save, empty for no saving&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--log_dir&#x27;</span>, default=<span class="string">&#x27;./output_dir_pretrained&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to tensorboard log&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--resume&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;resume from checkpoint&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--start_epoch&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;start epoch&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_workers&#x27;</span>, default=<span class="number">5</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--pin_mem&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--no_pin_mem&#x27;</span>, action=<span class="string">&#x27;store_false&#x27;</span>, dest=<span class="string">&#x27;pin_mem&#x27;</span>)</span><br><span class="line">    parser.set_defaults(pin_mem=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># distributed training parameters</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--world_size&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of distributed processes&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--local_rank&#x27;</span>, default=-<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dist_on_itp&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dist_url&#x27;</span>, default=<span class="string">&#x27;env://&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;url used to set up distributed training&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure><h2 id="主要代码">7 主要代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args, mode=<span class="string">&quot;train&quot;</span>, test_image_path=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;mode&#125;</span> mode...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建数据批次</span></span><br><span class="line">        dataset_train = build_dataset(is_train=<span class="literal">True</span>, args=args)</span><br><span class="line">        dataset_val = build_dataset(is_train=<span class="literal">False</span>, args=args)</span><br><span class="line"></span><br><span class="line">        sampler_train = torch.utils.data.RandomSampler(dataset_train)</span><br><span class="line">        sampler_val = torch.utils.data.SequentialSampler(dataset_val)</span><br><span class="line"></span><br><span class="line">        data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">            dataset_train, sampler=sampler_train,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">            pin_memory=args.pin_mem,</span><br><span class="line">            drop_last=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">            dataset_val, sampler=sampler_val,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            <span class="comment"># batch_size = 1</span></span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">            pin_memory=args.pin_mem,</span><br><span class="line">            drop_last=<span class="literal">False</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 构建模型</span></span><br><span class="line">        model = timm.create_model(<span class="string">&quot;resnet18&quot;</span>, pretrained=<span class="literal">True</span>, num_classes=<span class="number">36</span>, drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        n_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;number of trainable params (M):%.2f&#x27;</span> % (n_parameters / <span class="number">1.e6</span>))</span><br><span class="line"></span><br><span class="line">        criterion = nn.CrossEntropyLoss()</span><br><span class="line">        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log dir</span></span><br><span class="line">        os.makedirs(args.log_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        log_writer = SummaryWriter(log_dir=args.log_dir)</span><br><span class="line">        loss_scaler = NativeScaler()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读入已有的模型</span></span><br><span class="line">        misc.load_model(args=args, model_without_ddp=model, optimizer=optimizer, loss_scaler=loss_scaler)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch<span class="subst">&#123;epoch&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;length of data_loader_train is <span class="subst">&#123;<span class="built_in">len</span>(data_loader_train)&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Evaluating...&quot;</span>)</span><br><span class="line">                model.<span class="built_in">eval</span>()</span><br><span class="line">                test_stats = evaluate(data_loader_val, model, device)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Accuracy of the network on the <span class="subst">&#123;<span class="built_in">len</span>(dataset_val)&#125;</span> test images: <span class="subst">&#123;test_stats[<span class="string">&#x27;acc1&#x27;</span>]:<span class="number">.1</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    log_writer.add_scalar(<span class="string">&quot;perf/test_acc1&quot;</span>, test_stats[<span class="string">&quot;acc1&quot;</span>], epoch)</span><br><span class="line">                    log_writer.add_scalar(<span class="string">&quot;perf/test_acc5&quot;</span>, test_stats[<span class="string">&quot;acc5&quot;</span>], epoch)</span><br><span class="line">                    log_writer.add_scalar(<span class="string">&quot;perf/test_loss&quot;</span>, test_stats[<span class="string">&quot;loss&quot;</span>], epoch)</span><br><span class="line">                model.train()</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;training...&quot;</span>)</span><br><span class="line">            train_stats = train_one_epoch(</span><br><span class="line">                model, criterion, data_loader_train,</span><br><span class="line">                optimizer, device, epoch + <span class="number">1</span>, loss_scaler, <span class="literal">None</span>, log_writer=log_writer, args=args</span><br><span class="line">            )</span><br><span class="line">            <span class="built_in">print</span>(train_stats)</span><br><span class="line">            <span class="keyword">if</span> args.output_dir:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;saving checkpoint...&quot;</span>)</span><br><span class="line">                misc.save_model(</span><br><span class="line">                    args=args,model=model,model_without_ddp=model,optimizer=optimizer,</span><br><span class="line">                    loss_scaler=loss_scaler,epoch=epoch</span><br><span class="line">                )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model = timm.create_model(<span class="string">&quot;resnet18&quot;</span>, pretrained=<span class="literal">True</span>, num_classes=<span class="number">36</span>, drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        class_dict = &#123;<span class="string">&#x27;apple&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;banana&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;beetroot&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;bell pepper&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;cabbage&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;capsicum&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">                      <span class="string">&#x27;carrot&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;cauliflower&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;chilli pepper&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;corn&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;cucumber&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;eggplant&#x27;</span>: <span class="number">11</span>,</span><br><span class="line">                      <span class="string">&#x27;garlic&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;ginger&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;grapes&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;jalepeno&#x27;</span>: <span class="number">15</span>, <span class="string">&#x27;kiwi&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;lemon&#x27;</span>: <span class="number">17</span>, <span class="string">&#x27;lettuce&#x27;</span>: <span class="number">18</span>,</span><br><span class="line">                      <span class="string">&#x27;mango&#x27;</span>: <span class="number">19</span>, <span class="string">&#x27;onion&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;orange&#x27;</span>: <span class="number">21</span>, <span class="string">&#x27;paprika&#x27;</span>: <span class="number">22</span>, <span class="string">&#x27;pear&#x27;</span>: <span class="number">23</span>, <span class="string">&#x27;peas&#x27;</span>: <span class="number">24</span>, <span class="string">&#x27;pineapple&#x27;</span>: <span class="number">25</span>,</span><br><span class="line">                      <span class="string">&#x27;pomegranate&#x27;</span>: <span class="number">26</span>, <span class="string">&#x27;potato&#x27;</span>: <span class="number">27</span>, <span class="string">&#x27;raddish&#x27;</span>: <span class="number">28</span>, <span class="string">&#x27;soy beans&#x27;</span>: <span class="number">29</span>, <span class="string">&#x27;spinach&#x27;</span>: <span class="number">30</span>, <span class="string">&#x27;sweetcorn&#x27;</span>: <span class="number">31</span>,</span><br><span class="line">                      <span class="string">&#x27;sweetpotato&#x27;</span>: <span class="number">32</span>, <span class="string">&#x27;tomato&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;turnip&#x27;</span>: <span class="number">34</span>, <span class="string">&#x27;watermelon&#x27;</span>: <span class="number">35</span>&#125;</span><br><span class="line"></span><br><span class="line">        n_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;number of trainable params (M):%.2f&#x27;</span> % (n_parameters / <span class="number">1.e6</span>))</span><br><span class="line"></span><br><span class="line">        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">        os.makedirs(args.log_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        loss_scaler = NativeScaler()</span><br><span class="line"></span><br><span class="line">        misc.load_model(args=args, model_without_ddp=model, optimizer=optimizer, loss_scaler=loss_scaler)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        image = Image.<span class="built_in">open</span>(test_image_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        image = image.resize((args.input_size, args.input_size), Image.ANTIALIAS)</span><br><span class="line">        image = torchvision.transforms.ToTensor()(image).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model(image)</span><br><span class="line"></span><br><span class="line">        output = F.softmax(output, dim=-<span class="number">1</span>)</span><br><span class="line">        class_idx = torch.argmax(output, dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        score = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;image path is <span class="subst">&#123;test_image_path&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;score is <span class="subst">&#123;score.item()&#125;</span>, class id is <span class="subst">&#123;class_idx.item()&#125;</span>,  &quot;</span></span><br><span class="line">            <span class="string">f&quot;class name is <span class="subst">&#123;<span class="built_in">list</span>(class_dict.keys())[<span class="built_in">list</span>(class_dict.values()).index(class_idx)]&#125;</span>&quot;</span>)</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>分为两大部分：</p><ul><li>首先进行eval</li><li>然后进行训练</li></ul><h2 id="infer">8 infer</h2><ul><li>修改resume</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&#x27;--resume&#x27;, default=&#x27;./output_dir_pretrained/checkpoint-24.pth&#x27;,help=&#x27;resume from checkpoint&#x27;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;数据集介绍&quot;&gt;1 数据集介绍&lt;/h2&gt;
&lt;p&gt;https://aistudio.baidu.com/aistudio/datasetdetail/119023&lt;/p&gt;
&lt;p&gt;其中的图片有36各类，不同类别在不同的文件夹中，其中文件后缀有&quot;jpg,png,JPG</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络11-GAN</title>
    <link href="https://wangtongyouwen.github.io/post/cad0a038.html"/>
    <id>https://wangtongyouwen.github.io/post/cad0a038.html</id>
    <published>2023-04-24T11:36:29.000Z</published>
    <updated>2023-04-30T13:07:07.758Z</updated>
    
    <content type="html"><![CDATA[<h1 id="generative-adversarial-nets">Generative Adversarial Nets</h1><p>https://arxiv.org/pdf/1406.2661.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241952572.png" alt="image-20230424195239653" /><figcaption aria-hidden="true">image-20230424195239653</figcaption></figure><p>Pearson散度和Jensen-Shannon散度都是衡量两个概率分布之间差异的方法，但它们具有不同的计算方式和特点。</p><ol type="1"><li>Pearson散度（Pearson Divergence）： Pearson散度是衡量两个概率分布之间的差异的一种方法，主要基于卡方统计量（Chi-Square statistic）。对于两个概率分布P和Q，Pearson散度的计算公式如下：</li></ol><p>D_Pearson(P, Q) = Σ[(P(x) - Q(x))^2 / Q(x)]</p><p>其中，x表示数据空间中的元素，P(x)和Q(x)分别表示概率分布P和Q在x处的概率密度。</p><p>Pearson散度的值越大，表示两个概率分布之间的差异越大。需要注意的是，Pearson散度不是一种距离度量，因为它不满足三角不等式。</p><ol type="1"><li>Jensen-Shannon散度（Jensen-Shannon Divergence）： Jensen-Shannon散度是一种对称的、有界的散度度量方法，用于衡量两个概率分布之间的差异。它是基于Kullback-Leibler散度（KL散度）的改进版本。对于两个概率分布P和Q，Jensen-Shannon散度的计算公式如下：</li></ol><p>D_JS(P, Q) = (1/2) * D_KL(P, M) + (1/2) * D_KL(Q, M)</p><p>其中，M = (1/2) * (P + Q)，D_KL(P, M)和D_KL(Q, M)分别表示P和Q相对于M的Kullback-Leibler散度。</p><p>Jensen-Shannon散度的值在0到1之间，值越大表示两个概率分布之间的差异越大。当两个分布完全相同时，Jensen-Shannon散度为0；当两个分布完全不相交时，Jensen-Shannon散度接近1。可以通过计算Jensen-Shannon散度的平方根得到Jensen-Shannon距离，它满足距离度量的性质。</p><p>总结一下，Pearson散度和Jensen-Shannon散度都是衡量两个概率分布之间差异的方法，但它们的计算方式和特点不同。Pearson散度基于卡方统计量，而Jensen-Shannon散度基于Kullback-Leibler散度。Jensen-Shannon散度是对称的、有界的，可以通过计算其平方根得到满足距离度量性质的Jensen-Shannon距离。</p><h1 id="code">code</h1><h2 id="generator">1 Generator</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;基于 MNist 实现对抗生成网络(GAN)&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(latent_dim, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, np.prod(image_size)),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        output = self.model(z)</span><br><span class="line">        image = torch.reshape(output, (z.shape[<span class="number">0</span>], *image_size))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="discriminator">2 Discriminator</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(np.prod(image_size), <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image</span>):</span><br><span class="line">        prob = self.model(torch.reshape(image, (image.shape[<span class="number">0</span>], -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prob</span><br></pre></td></tr></table></figure><h2 id="training">3 training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line">image_size = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">latent_dim = <span class="number">100</span></span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.MNIST(<span class="string">&quot;mnist_data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                     transform=torchvision.transforms.Compose([</span><br><span class="line">                                         torchvision.transforms.Resize(<span class="number">28</span>),</span><br><span class="line">                                         torchvision.transforms.ToTensor(),</span><br><span class="line">                                         torchvision.transforms.Normalize(mean=[<span class="number">0.5</span>], std=[<span class="number">0.5</span>])</span><br><span class="line">                                     ]))</span><br><span class="line"></span><br><span class="line">DataLoader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">generator = Generator().cuda()</span><br><span class="line">discriminator = Discriminator().cuda()</span><br><span class="line"></span><br><span class="line">g_optimizer = torch.optim.Adam(params=generator.parameters(), lr=<span class="number">0.0002</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">d_optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=<span class="number">0.0002</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">raw = os.path.abspath(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">output_dir = os.path.join(raw, <span class="string">&quot;mnist_data/MNIST/result&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">    os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否存在预训练的权重文件</span></span><br><span class="line">generator_weights_path = os.path.join(output_dir, <span class="string">&#x27;generator.pth&#x27;</span>)</span><br><span class="line">discriminator_weights_path = os.path.join(output_dir, <span class="string">&#x27;discriminator.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(generator_weights_path) <span class="keyword">and</span> os.path.exists(discriminator_weights_path):</span><br><span class="line">    generator.load_state_dict(torch.load(generator_weights_path))</span><br><span class="line">    discriminator.load_state_dict(torch.load(discriminator_weights_path))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> i, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(DataLoader):</span><br><span class="line">        gt_images, _ = mini_batch</span><br><span class="line">        gt_images = gt_images.cuda()</span><br><span class="line">        z = torch.randn(batch_size, latent_dim)</span><br><span class="line">        z = z.cuda()</span><br><span class="line">        pred_images = generator(z)</span><br><span class="line">        pred_images = pred_images.cuda()</span><br><span class="line">        g_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        target_ones = torch.ones(batch_size, <span class="number">1</span>)</span><br><span class="line">        target_ones = target_ones.cuda()</span><br><span class="line">        target_zeros = torch.zeros(batch_size, <span class="number">1</span>)</span><br><span class="line">        target_zeros = target_zeros.cuda()</span><br><span class="line">        g_loss = loss_fn(discriminator(pred_images), target_ones)  <span class="comment"># 生成器</span></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        g_optimizer.step()</span><br><span class="line"></span><br><span class="line">        d_optimizer.zero_grad()</span><br><span class="line">        d_loss = loss_fn(discriminator(gt_images), target_ones) + loss_fn(discriminator(pred_images.detach()), target_zeros)  <span class="comment"># 判别器</span></span><br><span class="line">        d_loss.backward()</span><br><span class="line">        d_optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            raw = os.path.abspath(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">            output_dir = os.path.join(raw, <span class="string">&quot;mnist_data/MNIST/result&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">                os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">            torchvision.utils.save_image(pred_images, os.path.join(output_dir, <span class="string">f&quot;image_<span class="subst">&#123;epoch&#125;</span>_<span class="subst">&#123;i // <span class="number">1000</span>&#125;</span>.png&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch&#125;</span>, Batch: <span class="subst">&#123;i&#125;</span>, Generator Loss: <span class="subst">&#123;g_loss.item()&#125;</span>, Discriminator Loss: <span class="subst">&#123;d_loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型权重</span></span><br><span class="line">    torch.save(generator.state_dict(), generator_weights_path)</span><br><span class="line">    torch.save(discriminator.state_dict(), discriminator_weights_path)</span><br></pre></td></tr></table></figure><h1 id="cgan">CGAN</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304262015876.png" alt="image-20230426201511591" /><figcaption aria-hidden="true">image-20230426201511591</figcaption></figure><p>CGAN（条件生成对抗网络，Conditional GAN）是在GAN的基础上引入条件信息的一种变体。与普通GAN相比，CGAN的生成器和判别器都接收额外的条件信息（如类别标签、文本描述等），并根据这些条件信息生成特定类别的数据。这使得CGAN能够有更好的控制生成数据的特征。</p><p>CGAN的特点如下：</p><ol type="1"><li>控制生成数据特征：CGAN可以根据输入的条件信息，生成具有特定特征的数据。这使得CGAN在生成数据时具有更高的可控性。</li><li>引入条件信息：CGAN的生成器和判别器都接收额外的条件信息，使得网络可以在训练过程中学习如何利用这些条件信息来生成和识别数据。</li><li>更好的生成性能：由于条件信息的引入，CGAN可以在生成数据时更好地捕捉数据的特征和结构，从而提高生成数据的质量。</li></ol><p>总之，与普通GAN相比，CGAN通过引入条件信息，实现了对生成数据特征的控制，使得生成数据更具有可控性和更高的质量。在很多应用场景中，如图像生成、文本生成等，CGAN已经表现出很好的性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">10</span>,label_emb_dim)</span><br><span class="line">       ...</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z,labels</span>):</span><br><span class="line">        <span class="comment"># shape of z: [batchsize,latent_dim]</span></span><br><span class="line">        label_embedding = self.embedding(labels)</span><br><span class="line">        z = torch.cat([z,label_embedding], axis=-<span class="number">1</span>)</span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">10</span>,label_emb_dim)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image,labels</span>):</span><br><span class="line">        <span class="comment"># shape of image [batchsize,1,28,28]</span></span><br><span class="line">        label_embedding = self.embedding(labels)</span><br><span class="line">        prob = self.model(torch.cat([torch.reshape(image, (image.shape[<span class="number">0</span>], -<span class="number">1</span>)),label_embedding],axis=-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在训练过程中需要对generator和discriminator的调用中引入参数label</span></span><br><span class="line">gt_images, labels = mini_batch</span><br></pre></td></tr></table></figure><h1 id="least-squares-gan">least-squares-GAN</h1><p>Least Squares Generative Adversarial Networks（LSGAN）是一种生成对抗网络（GAN）的变体，它主要针对传统GAN在训练过程中容易出现的不稳定性和梯度消失问题进行了改进。LSGAN的核心思想是将判别器（Discriminator）的损失函数从交叉熵损失（Cross-Entropy Loss）替换为最小二乘损失（Least Squares Loss），从而提高训练的稳定性和生成数据的质量。</p><p>LSGAN的主要特点如下：</p><ol type="1"><li>稳定性：与传统GAN相比，LSGAN在训练过程中表现出更高的稳定性。这主要归功于最小二乘损失函数的平滑性，它能够减少梯度消失问题的发生，使得生成器（Generator）和判别器（Discriminator）在训练过程中保持更好的平衡。</li><li>生成质量：LSGAN生成的数据质量通常优于传统GAN。最小二乘损失有助于判别器更好地区分真实数据和生成数据，从而为生成器提供更有效的梯度指导，使得生成器能够生成更高质量的数据。</li><li>更低的梯度消失风险：在传统GAN中，由于交叉熵损失在判别器接近最优时可能导致梯度消失问题，这使得生成器的训练变得困难。然而，在LSGAN中，最小二乘损失能够减轻梯度消失问题，使得生成器在整个训练过程中都能够收到有效的梯度信息。</li><li>容易实现：LSGAN的实现非常简单，只需将传统GAN的损失函数替换为最小二乘损失即可。这意味着在现有的GAN架构中，很容易将其升级为LSGAN。</li></ol><p>总之，LSGAN通过使用最小二乘损失改进了传统GAN的训练稳定性和生成数据质量。这使得LSGAN在各种生成任务中，如图像生成、文本生成等，都能取得更好的性能。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304262051780.png" alt="image-20230426205146063" /><figcaption aria-hidden="true">image-20230426205146063</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">logits = torch.linspace(-<span class="number">10</span>,<span class="number">10</span>,<span class="number">2000</span>)</span><br><span class="line">loss = []</span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"><span class="keyword">for</span> lgs <span class="keyword">in</span> logits:</span><br><span class="line">    loss.append(loss_fn(torch.sigmoid(lgs),torch.ones_like(lgs)))</span><br><span class="line"></span><br><span class="line">plt.plot(logits,loss)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304262107840.png" alt="image-20230426210738626" /><figcaption aria-hidden="true">image-20230426210738626</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;generative-adversarial-nets&quot;&gt;Generative Adversarial Nets&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/pdf/1406.2661.pdf&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门12-位置编码</title>
    <link href="https://wangtongyouwen.github.io/post/f94e9029.html"/>
    <id>https://wangtongyouwen.github.io/post/f94e9029.html</id>
    <published>2023-04-24T08:26:35.000Z</published>
    <updated>2023-04-30T06:10:14.229Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241635006.png" alt="image-20230424163501351" /><figcaption aria-hidden="true">image-20230424163501351</figcaption></figure><h1 id="transformer">1 transformer</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241637416.png" alt="image-20230424163709530" /><figcaption aria-hidden="true">image-20230424163709530</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.1d absolute sincos constant embedding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_1d_absolute_sincos_embeddings</span>(<span class="params">n_pos_vec,dim</span>):</span><br><span class="line">    <span class="comment"># pos_vec: torch.arrange(n_pos)</span></span><br><span class="line">    <span class="keyword">assert</span> dim % <span class="number">2</span> ==<span class="number">0</span>,<span class="string">&quot;wrong dimension&quot;</span></span><br><span class="line">    position_embedding = torch.zeros(n_pos_vec.numel(),dim,dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    omega = torch.arange(dim//<span class="number">2</span>,dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    omega /= dim / <span class="number">2.</span></span><br><span class="line">    omega = <span class="number">1.</span> / (<span class="number">10000</span> ** omega)</span><br><span class="line"></span><br><span class="line">    out = n_pos_vec[:,<span class="literal">None</span>] @ omega[<span class="literal">None</span>,:] <span class="comment"># [n_pos_vec,1]*[1,dim//2]</span></span><br><span class="line"></span><br><span class="line">    emb_sin = torch.sin(out)</span><br><span class="line">    emb_cos = torch.cos(out)</span><br><span class="line"></span><br><span class="line">    position_embedding[:,<span class="number">0</span>::<span class="number">2</span>] = emb_sin</span><br><span class="line">    position_embedding[:,<span class="number">1</span>::<span class="number">2</span>] = emb_cos</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>  position_embedding</span><br></pre></td></tr></table></figure><h1 id="swin-transformer">2 Swin transformer</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241914252.png" alt="image-20230424191316681" /><figcaption aria-hidden="true">image-20230424191316681</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.2d relative bias trainable embedding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_2d_relative_bias_trainable_embeddings</span>(<span class="params">n_head,height,width</span>):</span><br><span class="line">    <span class="comment"># width:5   bias=[-width+1, width-1]  2*width-1</span></span><br><span class="line">    <span class="comment"># height:5                            2*height-1</span></span><br><span class="line">    position_embedding = nn.Embedding((<span class="number">2</span>*width-<span class="number">1</span>)*(<span class="number">2</span>*height-<span class="number">1</span>),n_head)</span><br><span class="line">    nn.init.constant_(position_embedding,<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_relative_position_index</span>(<span class="params">height,width</span>):</span><br><span class="line">        cords = torch.stack(torch.meshgrid(torch.arange(height),torch.arange(width))) <span class="comment">#[2,height,width]</span></span><br><span class="line">        cords_flatten = torch.flatten(cords,<span class="number">1</span>) <span class="comment"># [2,height*width]</span></span><br><span class="line">        relative_cords_bias = cords_flatten[:,:,<span class="literal">None</span>] - cords_flatten[:,<span class="literal">None</span>,:] <span class="comment">#[2,height*width,height*width]</span></span><br><span class="line"></span><br><span class="line">        relative_cords_bias[<span class="number">0</span>,:,:] += height - <span class="number">1</span></span><br><span class="line">        relative_cords_bias[<span class="number">1</span>,:,:] += width - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># A:2d,B:1d B[i*cols+j] = a[i,j]</span></span><br><span class="line"></span><br><span class="line">        relative_cords_bias[<span class="number">0</span>,:,:] *= relative_cords_bias[<span class="number">1</span>,:,:].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  relative_cords_bias.<span class="built_in">sum</span>(<span class="number">0</span>) <span class="comment"># [height*width,height*width]</span></span><br><span class="line">    relative_position_bias = get_relative_position_index(height,width)</span><br><span class="line">    bias_embedding = position_embedding(torch.flatten(relative_position_bias)).reshape(height*width,height*width,n_head) <span class="comment"># [height*width,height*width,n_head]</span></span><br><span class="line"></span><br><span class="line">    bias_embedding.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>).unsqueeze(<span class="number">0</span>) <span class="comment"># [1,n_head,height*width,height*width]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bias_embedding</span><br></pre></td></tr></table></figure><h1 id="masked-ae">3 Masked AE</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.2d absolute constant sincos embedding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_2d_absolute_sincos_embeddings</span>(<span class="params">height,width,dim</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> dim % <span class="number">4</span> == <span class="number">0</span>, <span class="string">&quot;wrong dimension&quot;</span></span><br><span class="line">    position_embedding = torch.zeros(height*width,dim)</span><br><span class="line">    cords = torch.stack(torch.meshgrid(torch.arange(height,dtype=torch.<span class="built_in">float</span>), torch.arange(width,dtype=torch.<span class="built_in">float</span>)))  <span class="comment"># [2,height,width]</span></span><br><span class="line">    height_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(cords[<span class="number">0</span>]),dim//<span class="number">2</span>) <span class="comment"># [height*width,dim//2]</span></span><br><span class="line">    width_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(cords[<span class="number">1</span>]),dim//<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    position_embedding[:,:dim//<span class="number">2</span>] = height_embedding</span><br><span class="line">    position_embedding[:,dim//<span class="number">2</span>:] = width_embedding</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> position_embedding</span><br></pre></td></tr></table></figure><h1 id="section"></h1>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241635006.png&quot; alt=&quot;image-20230424163501351&quot; /&gt;&lt;fig</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门11-Normalization</title>
    <link href="https://wangtongyouwen.github.io/post/87a49949.html"/>
    <id>https://wangtongyouwen.github.io/post/87a49949.html</id>
    <published>2023-04-20T10:24:36.000Z</published>
    <updated>2023-04-30T06:10:05.146Z</updated>
    
    <content type="html"><![CDATA[<h1 id="layer-normalization">Layer Normalization</h1><p><a href="https://arxiv.org/pdf/1607.06450.pdf">1607.06450.pdf (arxiv.org)</a></p><p>这篇文章讨论了一种称为层归一化（Layer Normalization）的方法，用于稳定神经网络的训练过程，并减少训练时间。层归一化的核心思想是通过对单个训练样本的神经元输入求和来计算归一化所需的均值和方差。这与之前的批量归一化（Batch Normalization）方法不同，后者依赖于一个 mini-batch 中所有样本的输入分布。</p><p>以下是该论文的主要观点：</p><ol type="1"><li>层归一化将批量归一化的概念从 mini-batch 级别扩展到层级别，计算用于归一化的均值和方差来自单个训练样本中的所有神经元输入求和。</li><li>与批量归一化类似，层归一化也为每个神经元提供自适应的偏置和增益，这些参数在归一化之后但在非线性激活函数之前应用。</li><li>与批量归一化不同，层归一化在训练和测试阶段执行相同的计算，因此不依赖于 mini-batch 大小。</li><li>层归一化很容易应用于循环神经网络（RNN），因为可以在每个时间步骤单独计算归一化统计数据。</li><li>层归一化在稳定循环神经网络的隐藏状态动态方面非常有效。</li><li>实证研究表明，与以前发布的技术相比，层归一化可以显著减少训练时间。</li></ol><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202124984.png" alt="image-20230420212422293" /><figcaption aria-hidden="true">image-20230420212422293</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202123289.png" alt="image-20230420212333495" /><figcaption aria-hidden="true">image-20230420212333495</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202229447.png" alt="image-20230420222926884" /><figcaption aria-hidden="true">image-20230420222926884</figcaption></figure><h1 id="五种归一化的代码实现">五种归一化的代码实现</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202236940.png" alt="image-20230420223608053" /><figcaption aria-hidden="true">image-20230420223608053</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 首先定义一些常量</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">time_steps = <span class="number">3</span></span><br><span class="line">embedding_dim = <span class="number">4</span></span><br><span class="line">input_x = torch.randn(batch_size,time_steps,embedding_dim)</span><br></pre></td></tr></table></figure><h2 id="batch-normalization">1 batch normalization</h2><p>torch.nn.BatchNorm1d(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm1d</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202240276.png" alt="parameter and shape" /><figcaption aria-hidden="true">parameter and shape</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 官方API</span></span><br><span class="line"><span class="comment"># nlp: [N,L,C] -&gt; [C]</span></span><br><span class="line"><span class="comment"># cv: [N,C,H,W] -&gt; [C]</span></span><br><span class="line">batch_norm_op = nn.BatchNorm1d(embedding_dim,affine=<span class="literal">False</span>)</span><br><span class="line">bn_y = batch_norm_op(input_x.transpose(-<span class="number">1</span>,-<span class="number">2</span>)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(bn_y)</span><br><span class="line"><span class="comment"># 手写batch_norm</span></span><br><span class="line">bn_mean = input_x.mean(dim=(<span class="number">0</span>,<span class="number">1</span>)).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).repeat(batch_size,time_steps,<span class="number">1</span>)</span><br><span class="line">bn_std = input_x.std(dim=(<span class="number">0</span>,<span class="number">1</span>),unbiased=<span class="literal">False</span>).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).repeat(batch_size,time_steps,<span class="number">1</span>)</span><br><span class="line">verify_bn_y = (input_x-bn_mean)/(bn_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_bn_y)</span><br></pre></td></tr></table></figure><h2 id="layer-normalization-1">2 layer normalization</h2><p>torch.nn.LayerNorm(<em>normalized_shape</em>, <em>eps=1e-05</em>, <em>elementwise_affine=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html?highlight=layer+norm#torch.nn.LayerNorm</p><ul><li>需要保证batchsize是第一个维度，剩下的维度中有embedding_dim即可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现layer_norm并验证API</span></span><br><span class="line"><span class="comment"># nlp: [N,L,C] -&gt; [N,L]</span></span><br><span class="line"><span class="comment"># Cv: [N,C,H,W] -&gt; [N,H,W]</span></span><br><span class="line">layer_norm_op = nn.LayerNorm(embedding_dim,elementwise_affine=<span class="literal">False</span>)</span><br><span class="line">ln_y = layer_norm_op(input_x)</span><br><span class="line"><span class="built_in">print</span>(ln_y)</span><br><span class="line"><span class="comment"># shouxie  layer_norm</span></span><br><span class="line">ln_mean = input_x.mean(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">ln_std = input_x.std(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>,unbiased=<span class="literal">False</span>)</span><br><span class="line">verify_ln_y = (input_x-ln_mean)/(ln_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_ln_y)</span><br></pre></td></tr></table></figure><h2 id="instance-normalization">3 Instance Normalization</h2><ul><li>一般用于风格迁移中</li></ul><p>https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html?highlight=instance#torch.nn.InstanceNorm1d</p><p>torch.nn.InstanceNorm1d(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em>, <em>device=None</em>, <em>dtype=None</em>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用instance norm API</span></span><br><span class="line"><span class="comment"># nlp: [N,L,C] -&gt; [N,C]</span></span><br><span class="line"><span class="comment"># Cv: [N,C,H,W] -&gt; [N,C]</span></span><br><span class="line">ins_norm_op = nn.InstanceNorm1d(embedding_dim)</span><br><span class="line">in_y = ins_norm_op(input_x.transpose(-<span class="number">1</span>,-<span class="number">2</span>)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(in_y)</span><br><span class="line"><span class="comment"># 手写ins_norm</span></span><br><span class="line">in_mean = input_x.mean(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">in_std = input_x.std(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>,unbiased=<span class="literal">False</span>)</span><br><span class="line">verify_ins_y = (input_x-in_mean)/(in_std+<span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_ins_y)</span><br></pre></td></tr></table></figure><ul><li>因为在同一个mini-batch中，如果在序列上对其加权平均，最后得到的也就是序列中的风格信息。</li></ul><h2 id="group-normalization">4 group normalization</h2><p>与layer norm比较相同</p><p>torch.nn.GroupNorm(<em>num_groups</em>, <em>num_channels</em>, <em>eps=1e-05</em>, <em>affine=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html?highlight=group+norm#torch.nn.GroupNorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用group norm API</span></span><br><span class="line"><span class="comment"># nlp: [N,G,L,C/G] -&gt; [N,G]</span></span><br><span class="line"><span class="comment"># Cv: [N,G,C/G,H,W] -&gt; [N,G]</span></span><br><span class="line">group_norm_op = nn.GroupNorm(num_group,embedding_dim,affine=<span class="literal">False</span>)</span><br><span class="line">gn_y = group_norm_op(input_x.transpose(-<span class="number">1</span>,-<span class="number">2</span>)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(gn_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写 group norm</span></span><br><span class="line">group_input_x = torch.split(input_x,split_size_or_sections=embedding_dim//num_group,dim=-<span class="number">1</span>)</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> g_inputx <span class="keyword">in</span> group_input_x:</span><br><span class="line">    gn_mean = g_inputx.mean(dim=(<span class="number">1</span>,<span class="number">2</span>),keepdim=<span class="literal">True</span>)</span><br><span class="line">    gn_std = g_inputx.std(dim=(<span class="number">1</span>,<span class="number">2</span>),keepdim=<span class="literal">True</span>,unbiased=<span class="literal">False</span>)</span><br><span class="line">    gn_result = (g_inputx-gn_mean)/(gn_std+<span class="number">1e-5</span>)</span><br><span class="line">    results.append(gn_result)</span><br><span class="line">verify_gn_y = torch.cat(results,dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_gn_y)</span><br></pre></td></tr></table></figure><h2 id="weight-normalization">5 Weight Normalization</h2><p>torch.nn.utils.weight_norm(<em>module</em>, <em>name='weight'</em>, <em>dim=0</em>) https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#weight_norm <span class="math display">\[w = g\frac{v}{||v||}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现weight_norm并验证API</span></span><br><span class="line">linear = nn.Linear(embedding_dim,<span class="number">3</span>,bias=<span class="literal">False</span>) <span class="comment"># without weight norm</span></span><br><span class="line">wn_linear = torch.nn.utils.weight_norm(linear)</span><br><span class="line">wn_linear_output = wn_linear(input_x)</span><br><span class="line"><span class="built_in">print</span>(wn_linear_output)</span><br><span class="line"><span class="keyword">for</span> i,k <span class="keyword">in</span> <span class="built_in">enumerate</span>(wn_linear.named_parameters()):</span><br><span class="line">    <span class="built_in">print</span>(i,k)</span><br><span class="line"><span class="comment"># 手写实现 weight norm</span></span><br><span class="line">weight_direction = linear.weight/(linear.weight.norm(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>))</span><br><span class="line">weight_magnitude = wn_linear.weight_g</span><br><span class="line">verify_wn_linear_output = input_x @ (weight_direction.transpose(-<span class="number">1</span>,-<span class="number">2</span>) * weight_magnitude.transpose(-<span class="number">1</span>,-<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(verify_wn_linear_output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">feat_dim = <span class="number">3</span></span><br><span class="line">hid_dim = <span class="number">4</span></span><br><span class="line">inputx = torch.randn(batch_size, feat_dim)</span><br><span class="line">linear = nn.Linear(feat_dim, hid_dim, bias=<span class="literal">False</span>)</span><br><span class="line">wn_linear = torch.nn.utils.weight_norm(linear)</span><br><span class="line"></span><br><span class="line">weight_magnitude = torch.tensor([linear.weight[i, :].norm() <span class="keyword">for</span> i <span class="keyword">in</span> torch.arange(linear.weight.shape[<span class="number">0</span>])],</span><br><span class="line">                                dtype=torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">weight_direction = linear.weight / weight_magnitude</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear.weight:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(linear.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear.magnitude:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_magnitude)  <span class="comment"># 幅度向量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear.direction:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_direction)  <span class="comment"># 单位向量，表示方向</span></span><br><span class="line"><span class="built_in">print</span>((weight_direction ** <span class="number">2</span>).<span class="built_in">sum</span>(dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight_direction*weight_magnitude:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_direction * weight_magnitude)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;inputx @ (weight_direction * weight_magnitude).T:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(inputx @ (weight_direction * weight_magnitude).T)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear(inputx):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(linear(inputx))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;wn_linear(inputx:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(wn_linear(inputx))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters of wn_linear:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> n, p <span class="keyword">in</span> wn_linear.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(n, p)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;construct weight of linear:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(wn_linear.weight_g * (wn_linear.weight_v / torch.tensor(</span><br><span class="line">    [wn_linear.weight_v[i, :].norm() <span class="keyword">for</span> i <span class="keyword">in</span> torch.arange(wn_linear.weight_v.shape[<span class="number">0</span>])],dtype=torch.float32).unsqueeze(-<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;layer-normalization&quot;&gt;Layer Normalization&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.06450.pdf&quot;&gt;1607.06450.pdf (arxiv.org)&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络10-MAE</title>
    <link href="https://wangtongyouwen.github.io/post/f0f056f4.html"/>
    <id>https://wangtongyouwen.github.io/post/f0f056f4.html</id>
    <published>2023-04-19T13:21:04.000Z</published>
    <updated>2023-04-30T06:09:49.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="masked-autoencoders-are-scalable-vision-learners">Masked Autoencoders Are Scalable Vision Learners</h1><p><a href="https://arxiv.org/pdf/2111.06377.pdf">2111.06377.pdf (arxiv.org)</a></p><p>本文表明，掩蔽自编码器（MAE）是计算机视觉领域可扩展的自监督学习方法。我们的 MAE 方法很简单：我们随机遮盖输入图像的一部分区域，并重建丢失的像素。该方法基于两个核心设计。首先，我们开发了一种非对称的编码器-解码器架构，其中编码器仅对可见子图像区域（没有遮盖标记）进行操作，而轻量级的解码器则从潜在表示和遮盖标记中重建原始图像。其次，我们发现遮盖输入图像较大比例（例如 75%）会产生一个具有意义且难度适中的自监督任务。将这两个设计结合起来，使我们能够高效、有效地训练大型模型：我们加速训练（至少提高 3 倍）并提高准确性。我们的可扩展方法允许学习高容量模型以实现更好的泛化：例如，一个普通的 ViT-Huge 模型在仅使用 ImageNet-1K 数据的方法中达到了最高准确率（87.8%）。在下游任务中，迁移性能超过了有监督预训练，并展示出有希望的扩展行为。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304192127666.png" alt="image-20230419212658615" /><figcaption aria-hidden="true">image-20230419212658615</figcaption></figure><p>image-&gt;patch-&gt;random mask-&gt;shuffle the patch-&gt;encoder-&gt;combine the whole embedding-&gt;unshuffle to align all tokens with target-&gt;decoder</p><p>自编码器是一种可以重构原始信号的方法，它有一个编码器，将观察到的信号映射到潜在表示，以及一个解码器，从潜在表示和掩码标记中重构原始信号。与传统的自编码器不同，我们采用了一种不对称的设计，允许编码器仅对部分观察到的信号（不带掩码标记）进行操作，并使用轻量级解码器从潜在表示和掩码标记中重构完整信号。</p><h2 id="mask">mask</h2><p>其中对于掩码的部分，并不是空向量，而是一个可以学习的向量，可以用过对其学习而恢复完整信号。因为图片中存在大量的冗余信息，所以这个掩码比例通常很高，比如75%</p><p>作者将图像分成规则的非重叠块，然后对其进行采样并遮蔽剩余的块。他们的采样策略很简单：随机采样块，不重复，遵循均匀分布。高遮蔽比率的随机采样大大减少了冗余，从而创造了一个不能通过可见邻近块外推来轻松解决的任务。均匀分布防止了潜在的中心偏差（即图像中心附近有更多的遮蔽块）。最后，高度稀疏的输入为设计高效编码器提供了机会。</p><p>重要的是，不需要任何<strong>专门的稀疏操作</strong>。首先，我们为每个输入图像块生成一个标记（通过线性投影并添加<strong>位置嵌入</strong>）。接下来，我们随机打乱标记列表，并根据遮盖比例删除列表的最后一部分。这个过程为<strong>编码器产生了一个小的标记子集</strong>，相当于在不重复采样的情况下采样图像块。编码后，我们将一列遮盖标记添加到编码后的图像块列表中，并对整个列表进行反打乱操作（反转随机打乱操作）以使所有标记与其目标对齐。解码器应用于这个完整列表（添加位置嵌入）。如前所述，不需要稀疏操作。这种简单的实现引入的开销可忽略不计，因为洗牌和反洗牌操作很快。</p><h2 id="encoder">encoder</h2><p>这个编码器类似ViT的结构，但仅应用于可见的、未遮盖的图像块。与标准的 ViT 一样，我们的编码器通过线性投影和添加位置嵌入对图像块进行嵌入，然后通过一系列 Transformer 模块处理生成的集合。然而，我们的编码器仅在完整集合的一小部分（例如，25%）上进行操作。遮盖的图像块被移除；不使用遮盖标记。这使我们能够仅用一部分计算和内存资源训练非常大的编码器。</p><h2 id="decoder">decoder</h2><p>MAE解码器的输入是完整的标记集合，包括：（i）编码后的可见图像块，以及（ii）遮盖标记。每个遮盖标记是一个共享的、可学习的向量，表示需要预测的缺失图像块的存在。我们为完整集合中的所有标记添加位置嵌入；如果没有这个，遮盖标记将无法获取关于它们在图像中的位置的信息。</p><p>MAE 解码器仅在预训练阶段(回归任务)用于执行图像重建任务（只有编码器用于生成图像表示以进行识别）。因此，解码器架构可以灵活地设计，其设计方式独立于编码器设计。我们尝试使用非常小的解码器，比编码器更窄、更浅。例如，我们默认的解码器每个标记的计算量不到编码器的10%。借助这种非对称设计，完整的标记集合仅由轻量级解码器处理，从而大幅减少预训练时间。</p><ul><li>解码器的目标是完成这个自回归任务，是为了更好的获得存在掩码的编码器，通过编码器才能完成cv的常见任务。</li><li>解码器输出中的每个元素都是代表一个图像块的像素值向量。解码器的最后一层是线性投影，其输出通道数量等于图像块中的像素值数量</li><li>损失函数计算像素空间中重建图像与原始图像之间的均方误差(MSE),仅在遮盖的图像块上计算损失</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304192302724.png" alt="image-20230419230225444" /><figcaption aria-hidden="true">image-20230419230225444</figcaption></figure><h1 id="code">code</h1><p>https://github.com/facebookresearch/mae</p><h2 id="models_mae">1 models_mae</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201412692.png" alt="image-20230420141204420" /><figcaption aria-hidden="true">image-20230420141204420</figcaption></figure><h3 id="模型搭建">1.1 模型搭建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedAutoencoderViT</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Masked Autoencoder with VisionTransformer backbone</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">1024</span>, depth=<span class="number">24</span>, num_heads=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, norm_layer=nn.LayerNorm, norm_pix_loss=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># MAE encoder specifics</span></span><br><span class="line">        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)</span><br><span class="line">        num_patches = self.patch_embed.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line"></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># MAE decoder specifics</span></span><br><span class="line">        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.mask_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, decoder_embed_dim))</span><br><span class="line"></span><br><span class="line">        self.decoder_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, decoder_embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line"></span><br><span class="line">        self.decoder_blocks = nn.ModuleList([</span><br><span class="line">            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(decoder_depth)])</span><br><span class="line"></span><br><span class="line">        self.decoder_norm = norm_layer(decoder_embed_dim)</span><br><span class="line">        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**<span class="number">2</span> * in_chans, bias=<span class="literal">True</span>) <span class="comment"># decoder to patch</span></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">        self.norm_pix_loss = norm_pix_loss</span><br><span class="line"></span><br><span class="line">        self.initialize_weights()</span><br></pre></td></tr></table></figure><h3 id="embedding的构造">1.2 embedding的构造</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span>, flatten=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]</span><br><span class="line">        self.flatten = flatten</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="初始化参数">1.3 初始化参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># initialization</span></span><br><span class="line">    <span class="comment"># initialize (and freeze) pos_embed by sin-cos embedding</span></span><br><span class="line">    pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">    self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">    self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize patch_embed like nn.Linear (instead of nn.Conv2d)</span></span><br><span class="line">    w = self.patch_embed.proj.weight.data</span><br><span class="line">    torch.nn.init.xavier_uniform_(w.view([w.shape[<span class="number">0</span>], -<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># timm&#x27;s trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)</span></span><br><span class="line">    torch.nn.init.normal_(self.cls_token, std=<span class="number">.02</span>)</span><br><span class="line">    torch.nn.init.normal_(self.mask_token, std=<span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize nn.Linear and nn.LayerNorm</span></span><br><span class="line">    self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">        <span class="comment"># we use xavier_uniform following official JAX ViT:</span></span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">        nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><ul><li>copy?</li></ul><p><code>pos_embed</code> 是一个位置嵌入矩阵，用于捕捉序列中元素的相对或绝对位置信息。在 Transformer 网络中，位置嵌入用于将位置信息与输入嵌入相结合，从而帮助模型处理输入序列。</p><p><code>get_2d_sincos_pos_embed</code> 函数生成了一个基于正弦和余弦函数的二维位置嵌入矩阵。在这种情况下，<code>pos_embed</code> 是一个预先初始化的 PyTorch 张量，而 <code>get_2d_sincos_pos_embed</code> 返回的是一个 NumPy 数组。</p><p><code>copy_()</code> 函数的目的是将 NumPy 数组的值复制到预先分配的 PyTorch 张量中。直接将值赋给 <code>pos_embed</code> 变量会导致以下问题：</p><ol type="1"><li><p>数据类型不匹配：<code>get_2d_sincos_pos_embed</code> 返回的 NumPy 数组可能具有与 PyTorch 张量不同的数据类型。使用 <code>copy_()</code> 函数可以确保在复制过程中自动执行必要的类型转换。在这里，<code>.float().unsqueeze(0)</code> 用于将 NumPy 数组转换为 PyTorch 张量，并确保其具有正确的维度和数据类型。</p></li><li><p>张量的引用问题：直接将值赋给 <code>pos_embed</code> 变量可能会更改原始张量的引用，这可能会导致意外的行为。<code>copy_()</code> 函数确保只有张量的值被修改，而不更改其引用。这对于在模型中保持预期的参数更新行为很重要。</p></li></ol><p>综上所述，使用 <code>copy_()</code> 函数将位置嵌入矩阵的值复制到预先分配的 PyTorch 张量中，可以确保正确处理数据类型转换，并在保持张量引用不变的情况下更新张量的值。</p><h3 id="patch-and-unpatch">1.4 patch and unpatch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">patchify</span>(<span class="params">self, imgs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: (N, 3, H, W)</span></span><br><span class="line"><span class="string">    x: (N, L, patch_size**2 *3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p = self.patch_embed.patch_size[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> imgs.shape[<span class="number">2</span>] == imgs.shape[<span class="number">3</span>] <span class="keyword">and</span> imgs.shape[<span class="number">2</span>] % p == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    h = w = imgs.shape[<span class="number">2</span>] // p</span><br><span class="line">    x = imgs.reshape(shape=(imgs.shape[<span class="number">0</span>], <span class="number">3</span>, h, p, w, p))</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nchpwq-&gt;nhwpqc&#x27;</span>, x)</span><br><span class="line">    x = x.reshape(shape=(imgs.shape[<span class="number">0</span>], h * w, p**<span class="number">2</span> * <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unpatchify</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x: (N, L, patch_size**2 *3)</span></span><br><span class="line"><span class="string">    imgs: (N, 3, H, W)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p = self.patch_embed.patch_size[<span class="number">0</span>]</span><br><span class="line">    h = w = <span class="built_in">int</span>(x.shape[<span class="number">1</span>]**<span class="number">.5</span>)</span><br><span class="line">    <span class="keyword">assert</span> h * w == x.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    x = x.reshape(shape=(x.shape[<span class="number">0</span>], h, w, p, p, <span class="number">3</span>))</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)</span><br><span class="line">    imgs = x.reshape(shape=(x.shape[<span class="number">0</span>], <span class="number">3</span>, h * p, h * p))</span><br><span class="line">    <span class="keyword">return</span> imgs</span><br></pre></td></tr></table></figure><h3 id="掩码构建">1.5 掩码构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Perform per-sample random masking by per-sample shuffling.</span></span><br><span class="line"><span class="string">    Per-sample shuffling is done by argsort random noise.</span></span><br><span class="line"><span class="string">    x: [N, L, D], sequence</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, L, D = x.shape  <span class="comment"># batch, length, dim</span></span><br><span class="line">    len_keep = <span class="built_in">int</span>(L * (<span class="number">1</span> - mask_ratio))</span><br><span class="line">    </span><br><span class="line">    noise = torch.rand(N, L, device=x.device)  <span class="comment"># noise in [0, 1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sort noise for each sample</span></span><br><span class="line">    ids_shuffle = torch.argsort(noise, dim=<span class="number">1</span>)  <span class="comment"># ascend: small is keep, large is remove</span></span><br><span class="line">    ids_restore = torch.argsort(ids_shuffle, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep the first subset</span></span><br><span class="line">    ids_keep = ids_shuffle[:, :len_keep]</span><br><span class="line">    x_masked = torch.gather(x, dim=<span class="number">1</span>, index=ids_keep.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, D))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate the binary mask: 0 is keep, 1 is remove</span></span><br><span class="line">    mask = torch.ones([N, L], device=x.device)</span><br><span class="line">    mask[:, :len_keep] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># unshuffle to get the binary mask</span></span><br><span class="line">    mask = torch.gather(mask, dim=<span class="number">1</span>, index=ids_restore)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_masked, mask, ids_restore</span><br></pre></td></tr></table></figure><h3 id="encoder-forward">1.6 encoder forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_encoder</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="comment"># embed patches</span></span><br><span class="line">    x = self.patch_embed(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed w/o cls token</span></span><br><span class="line">    x = x + self.pos_embed[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># masking: length -&gt; length * mask_ratio</span></span><br><span class="line">    x, mask, ids_restore = self.random_masking(x, mask_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># append cls token</span></span><br><span class="line">    cls_token = self.cls_token + self.pos_embed[:, :<span class="number">1</span>, :]  <span class="comment"># 第0个位置</span></span><br><span class="line">    cls_tokens = cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, mask, ids_restore</span><br></pre></td></tr></table></figure><h3 id="decoder-forward">1.7 decoder forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_decoder</span>(<span class="params">self, x, ids_restore</span>):</span><br><span class="line">    <span class="comment"># embed tokens</span></span><br><span class="line">    x = self.decoder_embed(x) <span class="comment"># 降维</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># append mask tokens to sequence</span></span><br><span class="line">    mask_tokens = self.mask_token.repeat(x.shape[<span class="number">0</span>], ids_restore.shape[<span class="number">1</span>] + <span class="number">1</span> - x.shape[<span class="number">1</span>], <span class="number">1</span>) <span class="comment"># repeat in batchsize</span></span><br><span class="line">    x_ = torch.cat([x[:, <span class="number">1</span>:, :], mask_tokens], dim=<span class="number">1</span>)  <span class="comment"># no cls token</span></span><br><span class="line">    x_ = torch.gather(x_, dim=<span class="number">1</span>, index=ids_restore.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, x.shape[<span class="number">2</span>]))  <span class="comment"># unshuffle</span></span><br><span class="line">    x = torch.cat([x[:, :<span class="number">1</span>, :], x_], dim=<span class="number">1</span>)  <span class="comment"># append cls token</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed</span></span><br><span class="line">    x = x + self.decoder_pos_embed</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.decoder_blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.decoder_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predictor projection</span></span><br><span class="line">    x = self.decoder_pred(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove cls token</span></span><br><span class="line">    x = x[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="loss-forward">1.8 loss forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_loss</span>(<span class="params">self, imgs, pred, mask</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: [N, 3, H, W]</span></span><br><span class="line"><span class="string">    pred: [N, L, p*p*3]</span></span><br><span class="line"><span class="string">    mask: [N, L], 0 is keep, 1 is remove, </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target = self.patchify(imgs)  <span class="comment"># N, L, patch_size**2 *3</span></span><br><span class="line">    <span class="keyword">if</span> self.norm_pix_loss:</span><br><span class="line">        mean = target.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = target.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        target = (target - mean) / (var + <span class="number">1.e-6</span>)**<span class="number">.5</span> <span class="comment"># 防止方差为0</span></span><br><span class="line"></span><br><span class="line">    loss = (pred - target) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean(dim=-<span class="number">1</span>)  <span class="comment"># [N, L], mean loss per patch</span></span><br><span class="line"></span><br><span class="line">    loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()  <span class="comment"># mean loss on removed patches</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="different-models">1.9 different models</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_base_patch16_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">16</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>, num_heads=<span class="number">12</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_large_patch16_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">16</span>, embed_dim=<span class="number">1024</span>, depth=<span class="number">24</span>, num_heads=<span class="number">16</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_huge_patch14_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">14</span>, embed_dim=<span class="number">1280</span>, depth=<span class="number">32</span>, num_heads=<span class="number">16</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks</span><br><span class="line">mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks</span><br><span class="line">mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks</span><br></pre></td></tr></table></figure><h2 id="main_pretrain">2 main_pretrain</h2><figure><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230420154128379.png" alt="image-20230420154128379" /><figcaption aria-hidden="true">image-20230420154128379</figcaption></figure><h3 id="设置参数的入口">2.1 设置参数的入口</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    args = get_args_parser()</span><br><span class="line">    args = args.parse_args()</span><br><span class="line">    <span class="keyword">if</span> args.output_dir:</span><br><span class="line">        Path(args.output_dir).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="main">2.2 main</h3><p>略。这个函数能够实现单机单卡，多机多卡，单机多卡，cpu等模式，通用性很强</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--accum_iter&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Accumulate gradient iterations (for increasing the effective batch size under memory constraints)&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>此内容可以用在低GPU内存，但是想要训练大batchsize的网络上(时间换空间)</li></ul><h3 id="misc.init_distributed_mode">2.3 misc.init_distributed_mode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_distributed_mode</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> args.dist_on_itp:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_RANK&#x27;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_SIZE&#x27;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_LOCAL_RANK&#x27;</span>])</span><br><span class="line">        args.dist_url = <span class="string">&quot;tcp://%s:%s&quot;</span> % (os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>], os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>])</span><br><span class="line">        os.environ[<span class="string">&#x27;LOCAL_RANK&#x27;</span>] = <span class="built_in">str</span>(args.gpu)</span><br><span class="line">        os.environ[<span class="string">&#x27;RANK&#x27;</span>] = <span class="built_in">str</span>(args.rank)</span><br><span class="line">        os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>] = <span class="built_in">str</span>(args.world_size)</span><br><span class="line">        <span class="comment"># [&quot;RANK&quot;, &quot;WORLD_SIZE&quot;, &quot;MASTER_ADDR&quot;, &quot;MASTER_PORT&quot;, &quot;LOCAL_RANK&quot;]</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;RANK&#x27;</span> <span class="keyword">in</span> os.environ <span class="keyword">and</span> <span class="string">&#x27;WORLD_SIZE&#x27;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;LOCAL_RANK&#x27;</span>])</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;SLURM_PROCID&#x27;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;SLURM_PROCID&#x27;</span>])</span><br><span class="line">        args.gpu = args.rank % torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Not using distributed mode&#x27;</span>)</span><br><span class="line">        setup_for_distributed(is_master=<span class="literal">True</span>)  <span class="comment"># hack</span></span><br><span class="line">        args.distributed = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    args.distributed = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    args.dist_backend = <span class="string">&#x27;nccl&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;| distributed init (rank &#123;&#125;): &#123;&#125;, gpu &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        args.rank, args.dist_url, args.gpu), flush=<span class="literal">True</span>)</span><br><span class="line">    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,</span><br><span class="line">                                         world_size=args.world_size, rank=args.rank)</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    setup_for_distributed(args.rank == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="prepare">2.4 prepare</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">misc.init_distributed_mode(args)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;job dir: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(os.path.dirname(os.path.realpath(__file__))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(args).replace(<span class="string">&#x27;, &#x27;</span>, <span class="string">&#x27;,\n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(args.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fix the seed for reproducibility</span></span><br><span class="line">seed = args.seed + misc.get_rank()</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="simple-augmentation">2.5 simple augmentation</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201555531.png" alt="image-20230420155514559" /><figcaption aria-hidden="true">image-20230420155514559</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># simple augmentation</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(args.input_size, scale=(<span class="number">0.2</span>, <span class="number">1.0</span>), interpolation=<span class="number">3</span>),  <span class="comment"># 3 is bicubic</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(), <span class="comment"># unint8-&gt;float</span></span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line">dataset_train = datasets.ImageFolder(os.path.join(args.data_path, <span class="string">&#x27;train&#x27;</span>), transform=transform_train)</span><br><span class="line"><span class="built_in">print</span>(dataset_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_train, sampler=sampler_train,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="define-model">2.6 define model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">model_without_ddp = model</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model = %s&quot;</span> % <span class="built_in">str</span>(model_without_ddp))</span><br><span class="line"></span><br><span class="line">eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.lr <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># only base_lr is specified</span></span><br><span class="line">    args.lr = args.blr * eff_batch_size / <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;base lr: %.2e&quot;</span> % (args.lr * <span class="number">256</span> / eff_batch_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;actual lr: %.2e&quot;</span> % args.lr)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accumulate grad iterations: %d&quot;</span> % args.accum_iter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;effective batch size: %d&quot;</span> % eff_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=<span class="literal">True</span>)</span><br><span class="line">    model_without_ddp = model.module</span><br><span class="line">    </span><br><span class="line">misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)</span><br></pre></td></tr></table></figure><h3 id="optimizer">2.7 optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># following timm: set wd as 0 for bias and norm layers</span></span><br><span class="line">param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)</span><br><span class="line">optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(<span class="number">0.9</span>, <span class="number">0.95</span>))</span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br><span class="line">loss_scaler = NativeScaler()</span><br></pre></td></tr></table></figure><h3 id="train">2.8 train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Start training for <span class="subst">&#123;args.epochs&#125;</span> epochs&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        data_loader_train.sampler.set_epoch(epoch)</span><br><span class="line">    train_stats = train_one_epoch(</span><br><span class="line">        model, data_loader_train,</span><br><span class="line">        optimizer, device, epoch, loss_scaler,</span><br><span class="line">        log_writer=log_writer,</span><br><span class="line">        args=args</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> (epoch % <span class="number">20</span> == <span class="number">0</span> <span class="keyword">or</span> epoch + <span class="number">1</span> == args.epochs):</span><br><span class="line">        misc.save_model(</span><br><span class="line">            args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,</span><br><span class="line">            loss_scaler=loss_scaler, epoch=epoch)</span><br><span class="line"></span><br><span class="line">    log_stats = &#123;**&#123;<span class="string">f&#x27;train_<span class="subst">&#123;k&#125;</span>&#x27;</span>: v <span class="keyword">for</span> k, v <span class="keyword">in</span> train_stats.items()&#125;,</span><br><span class="line">                    <span class="string">&#x27;epoch&#x27;</span>: epoch,&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> misc.is_main_process():</span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            log_writer.flush()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(args.output_dir, <span class="string">&quot;log.txt&quot;</span>), mode=<span class="string">&quot;a&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(json.dumps(log_stats) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">total_time = time.time() - start_time</span><br><span class="line">total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training time &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_time_str))</span><br></pre></td></tr></table></figure><p>其中核心代码，单个epoch的训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model: torch.nn.Module,</span></span><br><span class="line"><span class="params">                    data_loader: Iterable, optimizer: torch.optim.Optimizer,</span></span><br><span class="line"><span class="params">                    device: torch.device, epoch: <span class="built_in">int</span>, loss_scaler,</span></span><br><span class="line"><span class="params">                    log_writer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    args=<span class="literal">None</span></span>):</span><br><span class="line">    model.train(<span class="literal">True</span>)</span><br><span class="line">    metric_logger = misc.MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    metric_logger.add_meter(<span class="string">&#x27;lr&#x27;</span>, misc.SmoothedValue(window_size=<span class="number">1</span>, fmt=<span class="string">&#x27;&#123;value:.6f&#125;&#x27;</span>))</span><br><span class="line">    header = <span class="string">&#x27;Epoch: [&#123;&#125;]&#x27;</span>.<span class="built_in">format</span>(epoch)</span><br><span class="line">    print_freq = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    accum_iter = args.accum_iter</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;log_dir: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(log_writer.log_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data_iter_step, (samples, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(metric_logger.log_every(data_loader, print_freq, header)):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># we use a per iteration (instead of per epoch) lr scheduler</span></span><br><span class="line">        <span class="keyword">if</span> data_iter_step % accum_iter == <span class="number">0</span>:</span><br><span class="line">            lr_sched.adjust_learning_rate(optimizer, data_iter_step / <span class="built_in">len</span>(data_loader) + epoch, args)</span><br><span class="line"></span><br><span class="line">        samples = samples.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.amp.autocast(): <span class="comment"># 自动混合精度</span></span><br><span class="line">            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)</span><br><span class="line"></span><br><span class="line">        loss_value = loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> math.isfinite(loss_value):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Loss is &#123;&#125;, stopping training&quot;</span>.<span class="built_in">format</span>(loss_value))</span><br><span class="line">            sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        loss /= accum_iter</span><br><span class="line">        loss_scaler(loss, optimizer, parameters=model.parameters(),</span><br><span class="line">                    update_grad=(data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        torch.cuda.synchronize()</span><br><span class="line"></span><br><span class="line">        metric_logger.update(loss=loss_value)</span><br><span class="line"></span><br><span class="line">        lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">        metric_logger.update(lr=lr)</span><br><span class="line"></span><br><span class="line">        loss_value_reduce = misc.all_reduce_mean(loss_value)</span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            <span class="string">&quot;&quot;&quot; We use epoch_1000x as the x-axis in tensorboard.</span></span><br><span class="line"><span class="string">            This calibrates different curves when batch size changes.</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            epoch_1000x = <span class="built_in">int</span>((data_iter_step / <span class="built_in">len</span>(data_loader) + epoch) * <span class="number">1000</span>)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, loss_value_reduce, epoch_1000x)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;lr&#x27;</span>, lr, epoch_1000x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Averaged stats:&quot;</span>, metric_logger)</span><br><span class="line">    <span class="keyword">return</span> &#123;k: meter.global_avg <span class="keyword">for</span> k, meter <span class="keyword">in</span> metric_logger.meters.items()&#125;</span><br></pre></td></tr></table></figure><ul><li><p>如果因为timm报错，需要把 qk_scale=None注释掉</p></li><li><p>如果需要多卡训练 python -m torch.distributed.launch --nproc_per_node=2 main_prerain.py --batchsize=32 --world_size=2 --data_path="..."</p></li><li><p>dataset的目录结构</p></li></ul><p>/path/to/imagenet-1k/： train/ class1/ img1.jpeg class2/ img2.jpeg val/ class1/ img3.jpeg class2/ img4.jpeg</p><h2 id="main_fintune">3 main_fintune</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201725532.png" alt="image-20230420172505860" /><figcaption aria-hidden="true">image-20230420172505860</figcaption></figure><p>大体和main_pretrain相同，这个是对编码器进行微调，微调的目的是为了更好的完成下游任务</p><h3 id="datasetnew">3.1 dataset(new)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">is_train, args</span>):</span><br><span class="line">    transform = build_transform(is_train, args)</span><br><span class="line"></span><br><span class="line">    root = os.path.join(args.data_path, <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">    dataset = datasets.ImageFolder(root, transform=transform)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_transform</span>(<span class="params">is_train, args</span>):</span><br><span class="line">    mean = IMAGENET_DEFAULT_MEAN</span><br><span class="line">    std = IMAGENET_DEFAULT_STD</span><br><span class="line">    <span class="comment"># train transform</span></span><br><span class="line">    <span class="comment"># 强增广</span></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        <span class="comment"># this should always dispatch to transforms_imagenet_train</span></span><br><span class="line">        transform = create_transform(</span><br><span class="line">            input_size=args.input_size,</span><br><span class="line">            is_training=<span class="literal">True</span>,</span><br><span class="line">            color_jitter=args.color_jitter,</span><br><span class="line">            auto_augment=args.aa,</span><br><span class="line">            interpolation=<span class="string">&#x27;bicubic&#x27;</span>,</span><br><span class="line">            re_prob=args.reprob,</span><br><span class="line">            re_mode=args.remode,</span><br><span class="line">            re_count=args.recount,</span><br><span class="line">            mean=mean,</span><br><span class="line">            std=std,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> transform</span><br><span class="line"></span><br><span class="line">    <span class="comment"># eval transform</span></span><br><span class="line">    <span class="comment"># 基本没有增广</span></span><br><span class="line">    t = []</span><br><span class="line">    <span class="keyword">if</span> args.input_size &lt;= <span class="number">224</span>:</span><br><span class="line">        crop_pct = <span class="number">224</span> / <span class="number">256</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        crop_pct = <span class="number">1.0</span></span><br><span class="line">    size = <span class="built_in">int</span>(args.input_size / crop_pct)</span><br><span class="line">    t.append(</span><br><span class="line">        transforms.Resize(size, interpolation=PIL.Image.BICUBIC),  <span class="comment"># to maintain same ratio w.r.t. 224 images</span></span><br><span class="line">    )</span><br><span class="line">    t.append(transforms.CenterCrop(args.input_size))</span><br><span class="line"></span><br><span class="line">    t.append(transforms.ToTensor())</span><br><span class="line">    t.append(transforms.Normalize(mean, std))</span><br><span class="line">    <span class="keyword">return</span> transforms.Compose(t)</span><br></pre></td></tr></table></figure><h3 id="dataloader">3.2 dataloader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_train, sampler=sampler_train,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_val, sampler=sampler_val,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="mixup增广">3.3 mixup增广</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mixup_fn = <span class="literal">None</span></span><br><span class="line">mixup_active = args.mixup &gt; <span class="number">0</span> <span class="keyword">or</span> args.cutmix &gt; <span class="number">0.</span> <span class="keyword">or</span> args.cutmix_minmax <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> mixup_active:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Mixup is activated!&quot;</span>)</span><br><span class="line">    mixup_fn = Mixup(</span><br><span class="line">        mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,</span><br><span class="line">        prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,</span><br><span class="line">        label_smoothing=args.smoothing, num_classes=args.nb_classes)</span><br></pre></td></tr></table></figure><h3 id="model定义">3.4 model定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = models_vit.__dict__[args.model](</span><br><span class="line">    num_classes=args.nb_classes,</span><br><span class="line">    drop_path_rate=args.drop_path,</span><br><span class="line">    global_pool=args.global_pool,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(timm.models.vision_transformer.VisionTransformer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer with support for global average pooling</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, global_pool=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self.global_pool = global_pool</span><br><span class="line">        <span class="keyword">if</span> self.global_pool:</span><br><span class="line">            norm_layer = kwargs[<span class="string">&#x27;norm_layer&#x27;</span>]</span><br><span class="line">            embed_dim = kwargs[<span class="string">&#x27;embed_dim&#x27;</span>]</span><br><span class="line">            self.fc_norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">del</span> self.norm  <span class="comment"># remove the original norm</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line"></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = blk(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.global_pool:</span><br><span class="line">            x = x[:, <span class="number">1</span>:, :].mean(dim=<span class="number">1</span>)  <span class="comment"># global pool without cls token</span></span><br><span class="line">            outcome = self.fc_norm(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">            outcome = x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outcome</span><br></pre></td></tr></table></figure><h3 id="预训练权重的导入">3.5 预训练权重的导入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.finetune <span class="keyword">and</span> <span class="keyword">not</span> args.<span class="built_in">eval</span>:</span><br><span class="line">    checkpoint = torch.load(args.finetune, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Load pre-trained checkpoint from: %s&quot;</span> % args.finetune)</span><br><span class="line">    checkpoint_model = checkpoint[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    state_dict = model.state_dict()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> checkpoint_model <span class="keyword">and</span> checkpoint_model[k].shape != state_dict[k].shape:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Removing key <span class="subst">&#123;k&#125;</span> from pretrained checkpoint&quot;</span>)</span><br><span class="line">            <span class="keyword">del</span> checkpoint_model[k]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># interpolate position embedding</span></span><br><span class="line">    interpolate_pos_embed(model, checkpoint_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load pre-trained model</span></span><br><span class="line">    msg = model.load_state_dict(checkpoint_model, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(msg)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.global_pool:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(msg.missing_keys) == &#123;<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>, <span class="string">&#x27;fc_norm.weight&#x27;</span>, <span class="string">&#x27;fc_norm.bias&#x27;</span>&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(msg.missing_keys) == &#123;<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># manually initialize fc layer</span></span><br><span class="line">    trunc_normal_(model.head.weight, std=<span class="number">2e-5</span>)</span><br></pre></td></tr></table></figure><ul><li>确保head部分的权重没有导入</li><li>线性插值：让微调阶段的position embedding仍然适用(如果模型变大)</li></ul><h2 id="linear-probing">4 linear probing</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201725555.png" alt="image-20230420172530538" /><figcaption aria-hidden="true">image-20230420172530538</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for linear prob only</span></span><br><span class="line"><span class="comment"># hack: revise model&#x27;s head with BN</span></span><br><span class="line">model.head = torch.nn.Sequential(torch.nn.BatchNorm1d(model.head.in_features, affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>), model.head)</span><br><span class="line"><span class="comment"># freeze all but the head</span></span><br><span class="line"><span class="keyword">for</span> _, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> _, p <span class="keyword">in</span> model.head.named_parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;masked-autoencoders-are-scalable-vision-learners&quot;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.o</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络9-ResNet</title>
    <link href="https://wangtongyouwen.github.io/post/1c753934.html"/>
    <id>https://wangtongyouwen.github.io/post/1c753934.html</id>
    <published>2023-04-19T11:24:03.000Z</published>
    <updated>2023-05-05T12:01:19.996Z</updated>
    
    <content type="html"><![CDATA[<p>https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</p><h1 id="使用timm复现resnet">使用timm复现ResNet</h1><p>论文内容略，本文采用的python+pytorch 环境为:</p><p>torch 1.13.1+cu117</p><p>python 3.7</p><p>timm 0.4.12</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timm</span><br><span class="line">timm.create_model(<span class="string">&quot;resnet50&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br></pre></td><td class="code"><pre><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (act1): ReLU(inplace=True)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (4): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (5): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)</span><br><span class="line">  (fc): Linear(in_features=2048, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>lib/site-packages/timm/models/resnet.py</p><h2 id="resnet">1 ResNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stem</span></span><br><span class="line">self.conv1 = nn.Conv2d(in_chans, inplanes, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = norm_layer(inplanes)</span><br><span class="line">self.act1 = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">self.feature_info = [<span class="built_in">dict</span>(num_chs=inplanes, reduction=<span class="number">2</span>, module=<span class="string">&#x27;act1&#x27;</span>)]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Stem Pooling</span></span><br><span class="line"><span class="keyword">if</span> aa_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">self.maxpool = nn.Sequential(*[</span><br><span class="line">nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">aa_layer(channels=inplanes, stride=<span class="number">2</span>)])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>核心部分：feature map</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feature Blocks</span></span><br><span class="line">channels = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">stage_modules, stage_feature_info = make_blocks(</span><br><span class="line">block, channels, layers, inplanes, cardinality=cardinality, base_width=base_width,</span><br><span class="line">output_stride=output_stride, reduce_first=block_reduce_first, avg_down=avg_down,</span><br><span class="line">down_kernel_size=down_kernel_size, act_layer=act_layer, norm_layer=norm_layer, aa_layer=aa_layer,</span><br><span class="line">drop_block_rate=drop_block_rate, drop_path_rate=drop_path_rate, **block_args)</span><br><span class="line"><span class="keyword">for</span> stage <span class="keyword">in</span> stage_modules:</span><br><span class="line">self.add_module(*stage)  <span class="comment"># layer1, layer2, etc</span></span><br><span class="line">self.feature_info.extend(stage_feature_info)</span><br></pre></td></tr></table></figure><p>其中make_blocks是构建每个stage的具体实现，这里具体内容在第二部分查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.num_features = <span class="number">512</span> * block.expansion</span><br><span class="line">self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)</span><br></pre></td></tr></table></figure><p>为什么这里有block.expansion?</p><p>在ResNet50中有些stage之间的通道数是不相同的，如果直接使用残差连接，无法完成直接相加，需要对某些深度通道进行压缩，expansion能够保证某些通道的输出维度是匹配的</p><h2 id="make_blocks">2 make_blocks</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_blocks</span>(<span class="params"></span></span><br><span class="line"><span class="params">        block_fn, channels, block_repeats, inplanes, reduce_first=<span class="number">1</span>, output_stride=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">        down_kernel_size=<span class="number">1</span>, avg_down=<span class="literal">False</span>, drop_block_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.</span>, **kwargs</span>):</span><br><span class="line">    stages = []</span><br><span class="line">    feature_info = []</span><br><span class="line">    net_num_blocks = <span class="built_in">sum</span>(block_repeats)</span><br><span class="line">    net_block_idx = <span class="number">0</span></span><br><span class="line">    net_stride = <span class="number">4</span></span><br><span class="line">    dilation = prev_dilation = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> stage_idx, (planes, num_blocks, db) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(channels, block_repeats, drop_blocks(drop_block_rate))):</span><br><span class="line">        stage_name = <span class="string">f&#x27;layer<span class="subst">&#123;stage_idx + <span class="number">1</span>&#125;</span>&#x27;</span>  <span class="comment"># never liked this name, but weight compat requires it</span></span><br><span class="line">        stride = <span class="number">1</span> <span class="keyword">if</span> stage_idx == <span class="number">0</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> net_stride &gt;= output_stride:</span><br><span class="line">            dilation *= stride</span><br><span class="line">            stride = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            net_stride *= stride</span><br><span class="line"></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inplanes != planes * block_fn.expansion:</span><br><span class="line">            down_kwargs = <span class="built_in">dict</span>(</span><br><span class="line">                in_channels=inplanes, out_channels=planes * block_fn.expansion, kernel_size=down_kernel_size,</span><br><span class="line">                stride=stride, dilation=dilation, first_dilation=prev_dilation, norm_layer=kwargs.get(<span class="string">&#x27;norm_layer&#x27;</span>))</span><br><span class="line">            downsample = downsample_avg(**down_kwargs) <span class="keyword">if</span> avg_down <span class="keyword">else</span> downsample_conv(**down_kwargs)</span><br><span class="line"></span><br><span class="line">        block_kwargs = <span class="built_in">dict</span>(reduce_first=reduce_first, dilation=dilation, drop_block=db, **kwargs)</span><br><span class="line">        blocks = []</span><br><span class="line">        <span class="keyword">for</span> block_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks):</span><br><span class="line">            downsample = downsample <span class="keyword">if</span> block_idx == <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            stride = stride <span class="keyword">if</span> block_idx == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">            block_dpr = drop_path_rate * net_block_idx / (net_num_blocks - <span class="number">1</span>)  <span class="comment"># stochastic depth linear decay rule</span></span><br><span class="line">            blocks.append(block_fn(</span><br><span class="line">                inplanes, planes, stride, downsample, first_dilation=prev_dilation,</span><br><span class="line">                drop_path=DropPath(block_dpr) <span class="keyword">if</span> block_dpr &gt; <span class="number">0.</span> <span class="keyword">else</span> <span class="literal">None</span>, **block_kwargs))</span><br><span class="line">            prev_dilation = dilation</span><br><span class="line">            inplanes = planes * block_fn.expansion</span><br><span class="line">            net_block_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        stages.append((stage_name, nn.Sequential(*blocks)))</span><br><span class="line">        feature_info.append(<span class="built_in">dict</span>(num_chs=inplanes, reduction=net_stride, module=stage_name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stages, feature_info</span><br></pre></td></tr></table></figure><p>这段代码定义了一个名为<code>make_blocks</code>的函数，用于构建一系列残差块（residual blocks）并形成卷积神经网络（CNN）的多个阶段。这种结构在 ResNet 及其变体中很常见。<code>make_blocks</code>函数接收多个参数，用于控制网络的结构和行为。函数的主要目的是根据给定的参数创建网络的各个阶段并返回这些阶段及其相关的特征信息。</p><p>以下是函数参数的简要说明：</p><ul><li><code>block_fn</code>：残差块的函数，如 ResNet 中的基本块或瓶颈块。</li><li><code>channels</code>：每个阶段的输出通道数量。</li><li><code>block_repeats</code>：每个阶段的残差块重复次数。</li><li><code>inplanes</code>：输入通道数量。</li><li>其他参数用于控制步长（<code>output_stride</code>）、下采样方式（<code>avg_down</code>）、DropBlock（<code>drop_block_rate</code>）和 Stochastic Depth（<code>drop_path_rate</code>）等高级功能。</li></ul><p>代码的主要部分是一个循环，针对每个阶段执行以下操作：</p><ol type="1"><li>为每个阶段创建一个名为<code>stage_name</code>的变量，如<code>layer1</code>、<code>layer2</code>等。</li><li>计算步长（<code>stride</code>）和扩张率（<code>dilation</code>），这些值控制每个阶段的空间分辨率。</li><li>如果需要，创建下采样层（<code>downsample</code>），以使输入与输出具有兼容的尺寸。</li><li>根据给定的参数，创建一系列残差块。</li><li>将这些残差块组合成一个名为<code>nn.Sequential</code>的模块，将其添加到<code>stages</code>列表中。</li><li>将每个阶段的特征信息添加到<code>feature_info</code>列表中。</li></ol><p>最后，函数返回<code>stages</code>和<code>feature_info</code>，它们分别表示网络的各个阶段及其相关的特征信息。这些信息通常在之后的代码中用于构建完整的神经网络，并在训练过程中用于损失计算和评估等任务。</p><ul><li>downsample和block.expansion之间的联系：</li></ul><p><code>downsample</code> 和 <code>block.expansion</code> 在这段代码中具有不同的目的，但它们之间存在联系。首先，让我们解释它们各自的作用：</p><ol type="1"><li><code>downsample</code>：<code>downsample</code> 是一个可选的下采样层，用于在输入和输出之间调整维度。在构建残差网络时，<code>downsample</code> 的目的是使跳过连接（skip connection）中的输入与残差块输出具有相同的尺寸，以便将它们相加。<code>downsample</code> 层通常包含一个卷积层和一个批量归一化层，用于调整输入的通道数量和/或分辨率。这样，即使输入和输出具有不同的尺寸，也可以将它们相加。</li><li><code>block.expansion</code>：<code>block.expansion</code> 是残差块中的扩张因子，用于控制输入和输出通道的增长。在 ResNet 架构中，基本块的扩张因子为 1（即输入和输出通道数量相同），而瓶颈块的扩张因子为 4（即输出通道数量为输入通道数量的 4 倍）。</li></ol><p>这两者之间的联系是：<code>downsample</code> 层用于调整输入的通道数量，使其与残差块输出的通道数量匹配。而<code>block.expansion</code> 则决定了残差块输出的通道数量。因此，在计算下采样层时，通常会将输出通道数量设置为 <code>planes * block_fn.expansion</code>，其中 <code>planes</code> 是当前阶段的输出通道数量，<code>block_fn.expansion</code> 是残差块的扩张因子。</p><p>简而言之，<code>downsample</code> 和 <code>block.expansion</code> 分别负责调整输入尺寸以匹配输出尺寸，以及控制残差块中输入和输出通道的增长。尽管它们具有不同的目的，但它们之间存在联系，共同影响网络的整体结构。</p><h2 id="basicblock">3 BasicBlock</h2><p>这个类就是对ResNet进行的一个实现，这个网络中的block大小都是相同的，所以没有用到上述的downsample和block.expansion</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, cardinality=<span class="number">1</span>, base_width=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                 reduce_first=<span class="number">1</span>, dilation=<span class="number">1</span>, first_dilation=<span class="literal">None</span>, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,</span></span><br><span class="line"><span class="params">                 attn_layer=<span class="literal">None</span>, aa_layer=<span class="literal">None</span>, drop_block=<span class="literal">None</span>, drop_path=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> cardinality == <span class="number">1</span>, <span class="string">&#x27;BasicBlock only supports cardinality of 1&#x27;</span></span><br><span class="line">        <span class="keyword">assert</span> base_width == <span class="number">64</span>, <span class="string">&#x27;BasicBlock does not support changing base width&#x27;</span></span><br><span class="line">        first_planes = planes // reduce_first</span><br><span class="line">        outplanes = planes * self.expansion</span><br><span class="line">        first_dilation = first_dilation <span class="keyword">or</span> dilation</span><br><span class="line">        use_aa = aa_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (stride == <span class="number">2</span> <span class="keyword">or</span> first_dilation != dilation)</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(</span><br><span class="line">            inplanes, first_planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span> <span class="keyword">if</span> use_aa <span class="keyword">else</span> stride, padding=first_dilation,</span><br><span class="line">            dilation=first_dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = norm_layer(first_planes)</span><br><span class="line">        self.act1 = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.aa = aa_layer(channels=first_planes, stride=stride) <span class="keyword">if</span> use_aa <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(</span><br><span class="line">            first_planes, outplanes, kernel_size=<span class="number">3</span>, padding=dilation, dilation=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = norm_layer(outplanes)</span><br><span class="line"></span><br><span class="line">        self.se = create_attn(attn_layer, outplanes)</span><br><span class="line"></span><br><span class="line">        self.act2 = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.dilation = dilation</span><br><span class="line">        self.drop_block = drop_block</span><br><span class="line">        self.drop_path = drop_path</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_init_last_bn</span>(<span class="params">self</span>):</span><br><span class="line">        nn.init.zeros_(self.bn2.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        shortcut = x</span><br><span class="line"></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_block <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.drop_block(x)</span><br><span class="line">        x = self.act1(x)</span><br><span class="line">        <span class="keyword">if</span> self.aa <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.aa(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_block <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.drop_block(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.se <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.se(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.drop_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.drop_path(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            shortcut = self.downsample(shortcut)</span><br><span class="line">        x += shortcut</span><br><span class="line">        x = self.act2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&lt;/p&gt;
&lt;h1 id=&quot;使用timm复现resnet&quot;&gt;使用timm复</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络8-ConvNext</title>
    <link href="https://wangtongyouwen.github.io/post/aa153511.html"/>
    <id>https://wangtongyouwen.github.io/post/aa153511.html</id>
    <published>2023-04-18T10:32:10.000Z</published>
    <updated>2023-04-30T06:08:29.723Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-convnet-for-the-2020s">A ConvNet for the 2020s</h1><p>https://arxiv.org/pdf/2201.03545.pdf</p><p>A vanilla <strong>ViT</strong>, on the other hand, faces <strong>difficulties</strong> when applied to general computer vision tasks such as object detection and semantic segmentation</p><p>It is the <strong>hierarchical</strong> Transformers (e.g., <strong>Swin Transformers</strong>) that reintroduced several <strong>ConvNet</strong> priors, making Transformers practically viable as a generic vision backbone and demonstrating <strong>remarkable performance on a wide variety of vision tasks</strong>.</p><h2 id="理解什么是resnet-50">1 理解什么是ResNet-50</h2><ul><li>由48层卷积+1层maxpool+1层avgpool构成，卷积每个block的配比为3:4:6:3</li><li>ResNet50 Architecture</li></ul><figure><img src="https://iq.opengenus.org/content/images/2020/03/Screenshot-from-2020-03-20-15-49-54.png" alt="Table 1" /><figcaption aria-hidden="true">Table 1</figcaption></figure><h2 id="convnext主要宗旨">2 ConVNeXt主要宗旨</h2><ul><li>本文主要是希望基于ReSNet-50结构，并参考Swin-T的思考来升级改造ResNet，最终得到ResNet结构，并实现了新的准确率，并进一步探索了它的可扩展性。</li></ul><h2 id="优化器参数">3 优化器参数</h2><ul><li>AdamW，300epochs</li><li>准确率直接从76.1%提升到了78.8%</li><li>预训练学习率为4e-3，weight_decay=0.05,batchsize=4096</li><li>微调学习率为5e-5,weight_decay=1e-8,batchsize=512,layer-wise Ir decay</li></ul><h2 id="宏观设计">4 宏观设计</h2><ul><li>将[3,4,6,3]的区块比例改成了[3,3,9,3]</li><li>将底层的卷积替换成了4*4 stride=4的卷积，类似于patch</li><li>引入depth-wise conv，并将channels从64提升到96</li><li>引入bottleneck结构{channels分别为96 384 96}，并增大 kernel size 到 7*7</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191428468.png" alt="image-20230419142812305" /><figcaption aria-hidden="true">image-20230419142812305</figcaption></figure><ul><li>至此，ImageNet-1k的准确率从78.8%提升到80.6%</li></ul><h2 id="微观设计">5 微观设计</h2><ul><li>将RELU替换成GELU，将BN替换为LN</li><li>引入更少激活函数和归一化层</li><li>采用2*2，stride=2的卷积进行下采样，并在底层、下采样之前和最后的平均池化之后加入LN层，使得训练更加稳定</li><li>至此，ImageNet-1k的准确率进一步提升到82.0%，击败Swin-T中的81.3%</li></ul><h2 id="可扩展性">6 可扩展性</h2><ul><li>ImageNet-1k训练<ul><li>随着参数数目和计算量的增大，准确率也在逐步提升至85.5%</li></ul></li><li>增加ImageNet-22k训练，在迁移至ImageNet-1k微调<ul><li>伴随预训练，同样的模型，效果涨幅约为2%</li><li>最终，ConvNeXt-XL效果达到87.8%</li></ul></li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191537241.png" alt="image-20230419144017393" /><figcaption aria-hidden="true">image-20230419144017393</figcaption></figure><h1 id="code">CODE</h1><p><a href="https://github.com/facebookresearch/ConvNeXt">facebookresearch/ConvNeXt: Code release for ConvNeXt model (github.com)</a></p><h2 id="block">1 block</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; ConvNeXt Block. There are two equivalent implementations:</span></span><br><span class="line"><span class="string">    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)</span></span><br><span class="line"><span class="string">    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back</span></span><br><span class="line"><span class="string">    We use (2) as we find it slightly faster in PyTorch</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        drop_path (float): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, drop_path=<span class="number">0.</span>, layer_scale_init_value=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dwconv = nn.Conv2d(dim, dim, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, groups=dim) <span class="comment"># depthwise conv</span></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.pwconv1 = nn.Linear(dim, <span class="number">4</span> * dim) <span class="comment"># pointwise/1x1 convs, implemented with linear layers</span></span><br><span class="line">        self.act = nn.GELU()</span><br><span class="line">        self.pwconv2 = nn.Linear(<span class="number">4</span> * dim, dim)</span><br><span class="line">        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), </span><br><span class="line">                                    requires_grad=<span class="literal">True</span>) <span class="keyword">if</span> layer_scale_init_value &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="built_in">input</span> = x</span><br><span class="line">        x = self.dwconv(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="comment"># (N, C, H, W) -&gt; (N, H, W, C)</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.pwconv1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.pwconv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.gamma <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.gamma * x</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (N, H, W, C) -&gt; (N, C, H, W)</span></span><br><span class="line"></span><br><span class="line">        x = <span class="built_in">input</span> + self.drop_path(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="connext">2 ConNeXt</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191505813.png" alt="image-20230419150530038" /><figcaption aria-hidden="true">image-20230419150530038</figcaption></figure><ul><li>stem<ul><li>conv2d: 3-&gt;96 kernel_size =4,stride=4</li><li>layernarm: 96</li></ul></li><li>downsampler_layer<ul><li>layernrom: 96, 192, 384</li><li>conv2d: 96, 192, 384 -&gt; 192, 384, 768 kernel_size = 2,stride = 2 (patch merging)</li></ul></li><li>stage(4个阶段)<ul><li>block数量:3,3,9,3</li><li>block input_channel: 96, 192, 384, 768</li><li>cur记录总的深度：每个深度的dropout是不同的，随着深度增大，dropout比例越大</li></ul></li><li>layernorm：768</li><li>head(Linear): 768 -&gt; num_classes</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, </span></span><br><span class="line"><span class="params">             depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>], dims=[<span class="number">96</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>], drop_path_rate=<span class="number">0.</span>, </span></span><br><span class="line"><span class="params">             layer_scale_init_value=<span class="number">1e-6</span>, head_init_scale=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">             </span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    self.downsample_layers = nn.ModuleList() <span class="comment"># stem and 3 intermediate downsampling conv layers</span></span><br><span class="line">    stem = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_chans, dims[<span class="number">0</span>], kernel_size=<span class="number">4</span>, stride=<span class="number">4</span>),</span><br><span class="line">        LayerNorm(dims[<span class="number">0</span>], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    self.downsample_layers.append(stem)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        downsample_layer = nn.Sequential(</span><br><span class="line">                LayerNorm(dims[i], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>),</span><br><span class="line">                nn.Conv2d(dims[i], dims[i+<span class="number">1</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.downsample_layers.append(downsample_layer)</span><br><span class="line"></span><br><span class="line">    self.stages = nn.ModuleList() <span class="comment"># 4 feature resolution stages, each consisting of multiple residual blocks</span></span><br><span class="line">    dp_rates=[x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))] </span><br><span class="line">    cur = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        stage = nn.Sequential(</span><br><span class="line">            *[Block(dim=dims[i], drop_path=dp_rates[cur + j], </span><br><span class="line">            layer_scale_init_value=layer_scale_init_value) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(depths[i])]</span><br><span class="line">        )</span><br><span class="line">        self.stages.append(stage)</span><br><span class="line">        cur += depths[i]</span><br><span class="line"></span><br><span class="line">    self.norm = nn.LayerNorm(dims[-<span class="number">1</span>], eps=<span class="number">1e-6</span>) <span class="comment"># final norm layer</span></span><br><span class="line">    self.head = nn.Linear(dims[-<span class="number">1</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    self.apply(self._init_weights)</span><br><span class="line">    self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">    self.head.bias.data.mul_(head_init_scale)</span><br></pre></td></tr></table></figure><h1 id="isotropic-convnext-各向同性">Isotropic ConvNeXt 各向同性</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191537246.png" alt="image-20230419153724437" /><figcaption aria-hidden="true">image-20230419153724437</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ef __init__(self, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, </span><br><span class="line">                 depth=<span class="number">18</span>, dim=<span class="number">384</span>, drop_path_rate=<span class="number">0.</span>, </span><br><span class="line">                 layer_scale_init_value=<span class="number">0</span>, head_init_scale=<span class="number">1.</span>,</span><br><span class="line">                 ):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.stem = nn.Conv2d(in_chans, dim, kernel_size=<span class="number">16</span>, stride=<span class="number">16</span>)</span><br><span class="line">        dp_rates=[x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)] </span><br><span class="line">        self.blocks = nn.Sequential(*[Block(dim=dim, drop_path=dp_rates[i], </span><br><span class="line">                                    layer_scale_init_value=layer_scale_init_value)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>) <span class="comment"># final norm layer</span></span><br><span class="line">        self.head = nn.Linear(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line">        self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">        self.head.bias.data.mul_(head_init_scale)</span><br></pre></td></tr></table></figure><ul><li>不需要step，各向同性，通道数目在每个阶段都是相同的</li></ul><h1 id="训练">训练</h1><ul><li>data_path</li><li>data_set {CIFAR,IMNET,image_foler}</li></ul><p>https://timm.fast.ai/这里有大量的cv网络的实现</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;a-convnet-for-the-2020s&quot;&gt;A ConvNet for the 2020s&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/pdf/2201.03545.pdf&lt;/p&gt;
&lt;p&gt;A vanilla &lt;strong&gt;ViT&lt;/strong</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
</feed>
