<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-04-16T15:21:42.596Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch基础入门15-SwimTransformer</title>
    <link href="http://example.com/post/daaec8c8.html"/>
    <id>http://example.com/post/daaec8c8.html</id>
    <published>2023-04-15T14:34:26.000Z</published>
    <updated>2023-04-16T15:21:42.596Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152245800.png" alt="image-20230415224534491"></p><p>将transformer直接使用到CV领域遇到的问题：</p><ul><li>尺度问题：同一张图中代表不用语义信息的block尺度差距很大</li><li>处理像素问题的计算成本大(形成的序列很长)</li></ul><p>本文提出一种hierarchical transformer,主要使用了shifted windows</p><ul><li>自注意力机制是在这个窗口中计算的，序列长度大大降低</li><li>通过shifting这个操作能够让相邻的两个窗口之间产生交互，上下层之间有了cross-window-connection</li></ul><p>image classification/object detection/semantic segmentation</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152255423.png" alt="image-20230415225520894"></p><h1 id="1-如何基于图片生成-patch-embedding"><a href="#1-如何基于图片生成-patch-embedding" class="headerlink" title="1.如何基于图片生成 patch embedding?"></a>1.如何基于图片生成 patch embedding?</h1><h2 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h2><ul><li>基于 pytorch unfold 的API将图片进行分块，也就是模仿卷积的思路，设置kernel_size=patch_size，得到分块后的图片</li><li>得到格式为[bs,num_patch,patch_depth]的张量</li><li>将张量与形状为[patch_depth,model_dim_C]的权重矩阵进行乘法操作，即可得到形状为[bs,num_patch,model_dim_C]的path_embedding <h2 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h2></li><li>patch_depth等于input_channel*patch_size*patch_size</li><li>model_dim_C相当于二维卷积的输出通道数目</li><li>将形状为[patch_depth,model_dim_C]的权重矩阵转换为[model_dim_C,input_channel,patch_size,path_size]的卷积核</li><li>调用pytorch中的conv2d API得到卷积的输出张量，形状为[bs,output_channel,height,width]</li><li>转换为[bs,num_patch,model_dim_C]的格式，即为patch embedding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_naive</span>(<span class="params">image,patch_size,weight</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;直观方法实现patch_embedding&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># image shape: bs*channel*h*w</span></span><br><span class="line">    patch = F.unfold(image,kernel_size=(patch_size,patch_size),</span><br><span class="line">                    stride=(patch_size,patch_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    patch_embedding = patch @ weight</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_conv</span>(<span class="params">image,kernel,stride</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于二维卷积来实现patch_embedding，embedding的维度就是卷积的输出通道数&quot;&quot;&quot;</span></span><br><span class="line">    conv_output = F.conv2d(image,kernel,stride=stride) <span class="comment">#[bs,oc,oh.ow]</span></span><br><span class="line">    bs, oc, oh, ow = conv_output.shape</span><br><span class="line">    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">    </span><br></pre></td></tr></table></figure><h1 id="2-如何构建MHSA并计算其复杂度？"><a href="#2-如何构建MHSA并计算其复杂度？" class="headerlink" title="2.如何构建MHSA并计算其复杂度？"></a>2.如何构建MHSA并计算其复杂度？</h1><ul><li>基于输入x进行三个映射分别得到q,k,v<ul><li>此步复杂度为$3LC^2$,其中$L$为序列长度，$C$为特征大小      </li><li>$q,k,v$维度:$[L,C]$</li></ul></li><li>将q,k,v拆分成多头的形式，注意这里的多头各自计算不受影响，所以可以与bs维度进行统一看待(c-&gt;c/n，把embedding看成一个个小的embedding)</li><li>计算$qk^T$,并考虑可能的掩码，即让无效的两两位置之间的能量为负无穷，掩码在shift window MHSA中会需要，而在window MHSA中暂不需要<ul><li>$attn_prob=\frac{q\times k^T}{\sqrt{d}}$</li><li>此步复杂度为$L^2C$</li></ul></li><li>计算概率值与$v$的乘积 <ul><li>概率维度:$[L,L]$;$v$维度:$[L,C]$</li><li>此步复杂度为$L^2C$</li></ul></li><li>对输出进行再次映射<ul><li>映射矩阵:$[C,C]$;输出矩阵:$[L,C]$</li><li>此步复杂度为$LC^2$</li></ul></li><li>总体复杂度为$4LC^2+2L^2C$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadSelfAttention</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model_dim,num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadSelfAttention,self).__init__()</span><br><span class="line">        self.num_head = num_head</span><br><span class="line">        </span><br><span class="line">        self.proj_linear_layer = nn.Linear(model_dim,<span class="number">3</span>*model_dim)</span><br><span class="line">        self.final_linear_layer = nn.Linear(model_dim,model_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span>,additive_mask = <span class="literal">None</span></span>):</span><br><span class="line">        bs, seqlen, model_dim = <span class="built_in">input</span>.shape</span><br><span class="line">        num_head = self.num_head</span><br><span class="line">        head_dim = model_dim // num_head</span><br><span class="line">        </span><br><span class="line">        proj_output = self.proj_linear_layer(<span class="built_in">input</span>)</span><br><span class="line">        q,k,v = proj_output.chunk(<span class="number">3</span>,dim=-<span class="number">1</span>) <span class="comment"># [bs,seqlen,seqlen,model_dim]</span></span><br><span class="line">        </span><br><span class="line">        q = q.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        q = q.reshape(bs*num_head,seqlen,head_dim)</span><br><span class="line">        </span><br><span class="line">        k = k.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        k = k.reshape(bs*num_head,seqlen,head_dim)</span><br><span class="line">        </span><br><span class="line">        v = v.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        v = v.reshape(bs*num_head,seqlen,head_dim) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> additive_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attn_prob = F.softmax(torch.bmm(q,k.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/math.sqrt(head_dim),dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            additive_mask = additive_mask.tile((num_head,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">            attn_prob = F.softmax(bs,num_head,seqlen,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs*num_head,seqlen,seqlen]</span></span><br><span class="line">            </span><br><span class="line">        output = torch.bmm(attn_prob,v) <span class="comment"># [bs*num_head,seqlen,head_dim]</span></span><br><span class="line">        output = output.reshape(bs,num_head,seqlen,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,seqlen,num_head,head_dim]</span></span><br><span class="line">        output = output.reshape(bs,seqlen,model_dim)</span><br><span class="line">        </span><br><span class="line">        output = self.final_linear_layer(output)</span><br><span class="line">        <span class="keyword">return</span> attn_prob,ouput</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="3-如何构建Window-MHSA并计算其复杂度？"><a href="#3-如何构建Window-MHSA并计算其复杂度？" class="headerlink" title="3.如何构建Window MHSA并计算其复杂度？"></a>3.如何构建Window MHSA并计算其复杂度？</h1><ul><li><p>将patch组成的图片进一步划分成一个个更大的window</p><ul><li>首先需要将三维的patch embedding转换成图片格式 [bs,channel,h,w] num_patch = h*w</li><li>使用unfold来将patch换分成window</li></ul></li><li><p>在每个window内部计算MHSA</p><ul><li><p>window数目其实可以跟batchsize进行统一对待，因为window与window之间没有交互计算</p></li><li><p>关于计算复杂度</p><ul><li><p>假设窗的边长为$W$，起那么计算每个窗的总体复杂度是$4W^2C^2+2W^4C$ </p></li><li><p>假设patch的总数目为$L$,那么窗的数目为$\frac{L}{W^2}$</p></li><li><p>因此，W-HMSA的总体复杂度为$(4W^2C^2+2W^4C)\times\frac{L}{W^2} = 4LC^2+2LW^2C$</p></li></ul></li><li><p>此处不需要mask</p></li><li><p>将计算结果转换成window的四维张量形式</p></li></ul></li><li><p>复杂度对比</p><ul><li>MHSA:$4LC^2+2L^2C$</li><li>W-MHSA:$4LC^2+2LW^2C$</li></ul><p>​    </p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_multi_head_self_attention</span>(<span class="params">patch_embedding,mhsa,window_size=<span class="number">4</span>,num_head=<span class="number">2</span></span>):</span><br><span class="line">    num_patch_in_window = widow_size * widow_size</span><br><span class="line">    bs, num_patch, patch_depth = patch_embedding.shape</span><br><span class="line">    image_height = image_width = <span class="built_in">int</span>(math.sqrt(num_patch))</span><br><span class="line">    </span><br><span class="line">    patch_embedding = patch_embedding,transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    patch = patch_embedding.reshape(bs,patch_depth,image_height,image_width)</span><br><span class="line">    window = F.unfold(patch,kernel_size=(widow_size,widow_size),</span><br><span class="line">                     stride=(window_size,window_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># [bs,num_window,window_depth]</span></span><br><span class="line">    bs,num_window,patch_depth_times_num_patch_in_window = window.shape</span><br><span class="line">    window = window.reshape(bs*num_window,patch_depth,num_patch_in_window).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    attn_prob, output = mhsa(window) <span class="comment"># [bs*num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    output = output.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h1 id="4-如何构建Shift-Window-MHSA及其Mask？"><a href="#4-如何构建Shift-Window-MHSA及其Mask？" class="headerlink" title="4.如何构建Shift Window MHSA及其Mask？"></a>4.如何构建Shift Window MHSA及其Mask？</h1><ul><li>将上一步的W-MHSA的结果转换成图片格式</li><li>假设已经做了新的window划分，这一步叫做shift-window</li><li>为了保持window数目不变从而有高效的计算，需要将图片的patch往左和往上各自滑动半个窗口大小的步长，保持patch所属window类型不变</li><li>将图片patch还原成window的数据格式</li><li>由于cycle shift后，每个window虽然形状规整，但部分window中存在原本不属于同一个窗口的patch，所以需要生成mask</li><li>如何生成mask？<ul><li>首先构建一个shift-window的patch所属的window类别矩阵</li><li>对该矩阵进行同样的往左往上各自滑动半个窗口大小的步长的操作</li><li>通过unfold操作得到[bs,num_window,num_patch_in_window]形状的类别矩阵</li><li>对该矩阵进行扩维成[bs,num_window,num_patch_in_window,1]</li><li>将该矩阵与其转置矩阵进行作差，得到同类关系矩阵(为0的位置上的patch属于同类，否则属于不同类)</li><li>对同类关系矩阵中非零的位置用负无穷数进行填充，对于零的位置用0去填充，这样就构建好了MHSA所需要的mask</li><li>此mask的形状为[bs,num_window,num_patch_in_window,num_patch_in_window]</li></ul></li><li>将window转换成三维的格式，[bs*num_window,num_patch_in_window,patch_depth]</li><li>将三维格式的特征连同mask一起送入MHSA中计算得到注意力输出</li><li>将注意力输出转换成图片patch格式，[bs,num_window,num_patch_in_window,patch_depth]</li><li>为了恢复位置，需要将图片的patch往右和往下各自滑动半个窗口大小的步长，至此，SW-MHSA计算完毕</li></ul><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/other/image-20230416152318837.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个辅助函数，window2image，也就是将transformer block的结构转换成图片的形式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window2image</span>(<span class="params">msa_output</span>):</span><br><span class="line">    bs,num_window,num_patch_in_window,patch_depth = msa_output.shape</span><br><span class="line">    window_size = <span class="built_in">int</span>(math.sqrt(num_patch_in_window))</span><br><span class="line">    image_height = <span class="built_in">int</span>(math.sqrt(num_window)) * window_size</span><br><span class="line">    image_width = image_height</span><br><span class="line">    </span><br><span class="line">    msa_output = msa_output.reshape(bs,<span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                       <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                       window_size,</span><br><span class="line">                                       window_size,</span><br><span class="line">                                       patch_depth)</span><br><span class="line">    msa_output = msa_output.transpose(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    image = msa_output.reshape(bs,image_height*image_width,patch_depth)</span><br><span class="line">    image = image.transpose(-<span class="number">1</span>,-<span class="number">2</span>).reshape(bs,patch_depth,image_height,image_width)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义辅助函数 shift_window，即高效计算sw-msa</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_window</span>(<span class="params">w_msa_output,window_size,shift_size,generate_mask=<span class="literal">False</span></span>): <span class="comment"># 只有在正向的时候才会 shift</span></span><br><span class="line">    bs,num_window,num_patch_in_window,patch_depth = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    w_msa_output = window2image(w_msa_output) <span class="comment"># [bs,depth,h,w]</span></span><br><span class="line">    bs,patch_depth,image_height,image_width = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    rolled_w_msa_output = torch.roll(w_msa_output,shifts=(shift_size,shift_size),dim=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = rolled_w_msa_output.reshape(bs,patch_depth,</span><br><span class="line">                                                     <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                                     window_size,</span><br><span class="line">                                                     <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                                     window_size)</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.transpose(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.reshape(bs,patch_depth,num_window*num_patch_in_window)</span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># [bs,num_window*num_patch_in_window,patch_depth]</span></span><br><span class="line">    shifted_window = shifted_w_msa_input.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> generate_mask:</span><br><span class="line">        additive_mask = build_mask_for_shifted_wmsa(bs,image_height,image_width,window_size) <span class="comment"># [bs,num_window,num_patch_in_windows,num_patch_in_windows]</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        additive_mask = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> shifted_window, additive_mask</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建shift window multi-head attention mask</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_mask_for_shifted_wmsa</span>(<span class="params">batch_size,image_height,image_width,window_size</span>):</span><br><span class="line">    index_metrix = torch.zeros(image_height,image_width)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(image_height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(image_width):</span><br><span class="line">            row_times = (i + window_size // <span class="number">2</span>) // window_size</span><br><span class="line">            col_times = (j + window_size // <span class="number">2</span>) // window_size</span><br><span class="line">            index_metrix[i,j] = row_times * (image_height/window_size) + col_times + <span class="number">1</span></span><br><span class="line">    rolled_index_matrix = torch.roll(index_metrix,shifts=(-window_size//<span class="number">2</span>,-window_size//<span class="number">2</span>),dims=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    rolled_index_matrix = rolled_index_matrix.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>) <span class="comment"># [bs,ch,h,w]</span></span><br><span class="line">    </span><br><span class="line">    c = F.unfold(rolled_index_matrix,kernel_size=(window_size,window_size),</span><br><span class="line">                stride=(window_size,window_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    c = c.tile(batch_size,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># [bs.num_window,num_patch_in_window]</span></span><br><span class="line">    </span><br><span class="line">    bs, num_window,num_patch_in_window = c.shape</span><br><span class="line">    </span><br><span class="line">    c1 = c.unsqueeze(-<span class="number">1</span>) <span class="comment"># [bs,num_window,num_patch_in_windows,1]</span></span><br><span class="line">    c2 = (c1-c1.transpose(-<span class="number">1</span>,-<span class="number">2</span>) == -<span class="number">0</span>) <span class="comment"># [bs,num_window,num_patch_in_windows,num_patch_in_windows]</span></span><br><span class="line">    valid_matrix = c2.to(torch.float32)</span><br><span class="line">    additive_mask = (<span class="number">1</span> - valid_matrix) * (-<span class="number">1e-9</span>) </span><br><span class="line">    </span><br><span class="line">    additive_mask = additive_mask.reshape(bs*num_window,num_patch_in_window,num_patch_in_window)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> additive_mask</span><br><span class="line">            </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_window_multi_head_self_attention</span>(<span class="params">w_msa_output,mhsa,window_size=<span class="number">4</span>,num_head=<span class="number">2</span></span>):</span><br><span class="line">    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input, additive_mask = shift_window(w_msa_output,window_size,</span><br><span class="line">                                                      shift_size=(-window_size//<span class="number">2</span>),</span><br><span class="line">                                                      generate_mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(shifted_w_msa_input.shape) <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    <span class="built_in">print</span>(additive_mask.shape) <span class="comment"># [bs*num_window,num_patch_in_window,num_patch_in_window]</span></span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.reshape(bs*num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    attn_prob,output = mhsa(shifted_w_msa_input,additive_mask=additive_mask)</span><br><span class="line">    </span><br><span class="line">    output = output.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    output, _ = shift_window(output,window_size,shift_size=window_size//<span class="number">2</span>,generate_mask=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape) <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h1 id="5-如何构建Patch-Merging？"><a href="#5-如何构建Patch-Merging？" class="headerlink" title="5.如何构建Patch Merging？"></a>5.如何构建Patch Merging？</h1><ul><li>将window格式的特征转换成图片patch格式</li><li>利用unfold操作，按照merge_size*merge_size大小得到新的patch,形状为[bs,num_patch_new,merge_size*merge_size*patch_depth_old]</li><li>使用一个全连接层对depth进行降维成0.5倍，也就是从merge_size*merge_size*patch_depth_old映射到0.5*merge_size*merge_size*patch_depth_old</li><li>输出的是patch embedding的形状格式,[bs,num_patch,patch_depth]</li><li>举例说明：以merge_size=2为例，经过PatchMerging后，patch数目减少为之前的$\frac{1}{4}$，但是depth增大为原来的2倍，而不是4倍</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_dim, merge_size,output_depth_scale=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PatchMerging, self).__init__()</span><br><span class="line">        self.merge_size = merge_size</span><br><span class="line">        self.model_dim = model_dim</span><br><span class="line">        self.proj_layer = nn.Linear(</span><br><span class="line">            model_dim * merge_size * merge_size,</span><br><span class="line">            <span class="built_in">int</span>(model_dim * merge_size * merge_size * output_depth_scale))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        bs, num_window, num_patch_in_window, patch_depth = <span class="built_in">input</span>.shape</span><br><span class="line">        window_size = <span class="built_in">int</span>(math.sqrt(num_patch_in_window))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">input</span> = window2image(<span class="built_in">input</span>)  <span class="comment"># [bs,patch_depth,image_h,image_w]</span></span><br><span class="line"></span><br><span class="line">        merged_window = F.unfold(<span class="built_in">input</span>, kernel_size=(self.merge_size, self.merge_size),</span><br><span class="line">                                 stride=(self.merge_size, self.merge_size)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line">        merged_window = self.proj_layer(merged_window)  <span class="comment"># [bs,num_patch,new_patch_depth]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> merged_window</span><br></pre></td></tr></table></figure><h1 id="6-如何构建Swin-TransformerBlock"><a href="#6-如何构建Swin-TransformerBlock" class="headerlink" title="6.如何构建Swin TransformerBlock?"></a>6.如何构建Swin TransformerBlock?</h1><ul><li>每个block包含LayerNorm，W-MHSA，MLP，SW-MHSA，残差连接等模块</li><li>输入是patch embedding格式</li><li>每个MLP包括两层，分别是4*mode_dim和mode_dim大小</li><li>输出的是window的数据格式，[bs,num_window,num_patch_in_window,patch_depth]</li><li>需要注意残差连接对数据形状的要求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model_dim,window_size,num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(SwinTransformerBlock,self).__init__()</span><br><span class="line">        self.layer_norm1 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm2 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm3 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm4 = nn.LayerNorm(model_dim)</span><br><span class="line">        </span><br><span class="line">        self.wsma_mlp1 = nn.Linear(model_dim,<span class="number">4</span>*model_dim)</span><br><span class="line">        self.wsma_mlp2 = nn.Linear(<span class="number">4</span>*model_dim,model_dim)</span><br><span class="line">        self.swsma_mlp1 = nn.Linear(model_dim,<span class="number">4</span>*model_dim)</span><br><span class="line">        self.swsma_mlp2 = nn.Linear(<span class="number">4</span>*model_dim,model_dim)</span><br><span class="line">        </span><br><span class="line">        self.mhsa1 = MultiHeadSelfAttention(model_dim,num_head)</span><br><span class="line">        self.mhsa2 = MultiHeadSelfAttention(model_dim,num_head)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        </span><br><span class="line">        bs,num_patch,patch_depth = <span class="built_in">input</span>.shape</span><br><span class="line">        </span><br><span class="line">        input1 = self.layer_norm1(<span class="built_in">input</span>)</span><br><span class="line">        w_msa_output = window_multi_head_self_attention(<span class="built_in">input</span>,self.mhsa1,window_size=<span class="number">4</span>,num_head=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#         bs, num_head, num_patch_in_window, patch_depth = w_msa_output.shape </span></span><br><span class="line">        w_msa_output = <span class="built_in">input</span> + w_msa_output.reshape(bs,num_patch,patch_depth)</span><br><span class="line">        output1 = self.wsma_mlp2(self.wsma_mlp1(self.layer_norm2(w_msa_output)))</span><br><span class="line">        output1 += w_msa_output</span><br><span class="line">        </span><br><span class="line">        input2 = self.layer_norm3(input1)</span><br><span class="line">        input2 = input2.reshape(bs,num_patch,num_patch_in_window,patch_depth)</span><br><span class="line">        sw_msa_output = shift_window_multi_head_self_attention(input2,self.mhsa2,window_size=<span class="number">4</span>,num_head=<span class="number">2</span>)</span><br><span class="line">        sw_msa_output = output1 + sw_msa_output.reshape(bs,num_patch,patch_depth)</span><br><span class="line">        output2 = self.swsma_mlp2(self.swsma_mlp1(self.layer_norm4(sw_msa_output)))</span><br><span class="line">        output2 += sw_msa_output</span><br><span class="line">        </span><br><span class="line">        output2 = output2.reshape(bs,num_patch,num_patch_in_window.patch_depth)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output2</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="7-如何构建SwinTransformerModel？"><a href="#7-如何构建SwinTransformerModel？" class="headerlink" title="7.如何构建SwinTransformerModel？"></a>7.如何构建SwinTransformerModel？</h1><ul><li>输入是图片</li><li>首先对图片进行分块并得到Patch embedding</li><li>经过第一个stage</li><li>进行patch merging,在进行第二个stage</li><li>以此类推…</li><li>对最后一个block的输出转换成patch embedding的格式[bs,num_patch_depth]</li><li>对patch embedding在时间维度进行平均池化，并映射到分类层得到分类的logits</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_image_channel=<span class="number">3</span>, patch_size=<span class="number">4</span>, model_dim_C=<span class="number">8</span>, num_classes=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">4</span>, num_head=<span class="number">2</span>, merge_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SwinTransformerModel, self).__init__()</span><br><span class="line">        patch_depth = patch_size * patch_size * input_image_channel</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.model_dim_C = model_dim_C</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.patch_embedding_weight = nn.Parameter(torch.randn(patch_depth, model_dim_C))  <span class="comment"># 模型可训练的一部分参数</span></span><br><span class="line"></span><br><span class="line">        self.block1 = SwinTransformerBlock(model_dim_C, window_size, num_head)</span><br><span class="line">        self.block2 = SwinTransformerBlock(model_dim_C * <span class="number">2</span>, window_size, num_head)</span><br><span class="line">        self.block3 = SwinTransformerBlock(model_dim_C * <span class="number">4</span>, window_size, num_head)</span><br><span class="line">        self.block4 = SwinTransformerBlock(model_dim_C * <span class="number">8</span>, window_size, num_head)</span><br><span class="line"></span><br><span class="line">        self.patch_merging1 = PatchMerging(model_dim_C , merge_size)</span><br><span class="line">        self.patch_merging2 = PatchMerging(model_dim_C * <span class="number">2</span>, merge_size)</span><br><span class="line">        self.patch_merging3 = PatchMerging(model_dim_C * <span class="number">4</span>, merge_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类</span></span><br><span class="line">        self.final_layer = nn.Linear(model_dim_C * <span class="number">8</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image</span>):</span><br><span class="line">        patch_embedding_naive = image2emb_naive(image, self.patch_size, self.patch_embedding_weight)</span><br><span class="line">        <span class="built_in">print</span>(patch_embedding_naive.shape)</span><br><span class="line"></span><br><span class="line">        kernel = self.patch_embedding_weight.transpose(<span class="number">0</span>, <span class="number">1</span>).reshape((-<span class="number">1</span>, ic, patch_size, patch_size))  <span class="comment"># oc*ic*kh*kw</span></span><br><span class="line">        patch_embedding_conv = image2emb_conv(image, kernel, self.patch_size)  <span class="comment"># 二维卷积的方法得到embedding</span></span><br><span class="line">        <span class="built_in">print</span>(patch_embedding_conv.shape)</span><br><span class="line">        <span class="comment"># block1</span></span><br><span class="line">        patch_embedding = patch_embedding_naive</span><br><span class="line">        <span class="built_in">print</span>(patch_embedding.shape)</span><br><span class="line">        sw_msa_output = self.block1(patch_embedding)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block1_output&quot;</span>, sw_msa_output.shape)  <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line"></span><br><span class="line">        merged_patch1 = self.patch_merging1(sw_msa_output)</span><br><span class="line">        sw_msa_output_1 = self.block2(merged_patch1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block2_output&quot;</span>, sw_msa_output_1.shape)</span><br><span class="line"></span><br><span class="line">        merged_patch2 = self.patch_merging2(sw_msa_output_1)</span><br><span class="line">        sw_msa_output_2 = self.block3(merged_patch2)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block3_output&quot;</span>, sw_msa_output_2.shape)</span><br><span class="line"></span><br><span class="line">        merged_patch3 = self.patch_merging3(sw_msa_output_2)</span><br><span class="line">        sw_msa_output_3 = self.block4(merged_patch3)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block4_output&quot;</span>, sw_msa_output_3.shape)</span><br><span class="line"></span><br><span class="line">        bs, num_window, num_patch_in_window, patch_depth = sw_msa_output_3.shape</span><br><span class="line">        sw_msa_output_3 = sw_msa_output_3.reshape(bs, -<span class="number">1</span>, patch_depth)</span><br><span class="line"></span><br><span class="line">        pool_output = torch.mean(sw_msa_output_3, dim=<span class="number">1</span>)</span><br><span class="line">        logits = self.final_layer(pool_output)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;logits&quot;</span>,logits.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="8-模型测试代码"><a href="#8-模型测试代码" class="headerlink" title="8.模型测试代码"></a>8.模型测试代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get parameters amount in the network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_n_params</span>(<span class="params">model</span>):</span><br><span class="line">    pp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">list</span>(model.parameters()):</span><br><span class="line">        nn = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">list</span>(p.size()):</span><br><span class="line">            nn = nn * s</span><br><span class="line">        pp += nn</span><br><span class="line">    <span class="keyword">return</span> pp</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    bs, ic, image_h, image_w = <span class="number">4</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">    patch_size = <span class="number">4</span></span><br><span class="line">    model_dim_C = <span class="number">8</span>  <span class="comment"># 一开始的patch embedding大小</span></span><br><span class="line">    max_num_token = <span class="number">16</span></span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    window_size = <span class="number">4</span></span><br><span class="line">    num_head = <span class="number">2</span></span><br><span class="line">    merge_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    patch_depth = patch_size * patch_size * ic</span><br><span class="line">    image = torch.randn(bs, ic, image_h, image_w)</span><br><span class="line"></span><br><span class="line">    model = SwinTransformerModel(ic, patch_size, model_dim_C, num_classes, window_size, num_head, merge_size)</span><br><span class="line"></span><br><span class="line">    model(image)</span><br><span class="line">    pp = get_n_params(model)</span><br><span class="line">    <span class="built_in">print</span>(pp)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152245800.png&quot; alt=&quot;image-20230415224534491&quot;&gt;&lt;/p&gt;
&lt;p&gt;将tran</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目2-基于GCN(DNN)的文本分类模型(GPU)</title>
    <link href="http://example.com/post/ceb60e40.html"/>
    <id>http://example.com/post/ceb60e40.html</id>
    <published>2023-04-15T11:19:44.000Z</published>
    <updated>2023-04-15T14:34:00.601Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://pytorch.org/tutorials/distributed/home.html">https://pytorch.org/tutorials/distributed/home.html</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152009298.png" alt="image-20230415200908110"></p><h2 id="1-单机单卡"><a href="#1-单机单卡" class="headerlink" title="1 单机单卡"></a>1 单机单卡</h2><p>在 main 函数前加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is available!&quot;</span>)</span><br><span class="line">    os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is not available! Exit!&quot;</span>)</span><br></pre></td></tr></table></figure><p>在train函数中，第一次调用模型的地方加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.cuda() <span class="comment"># 拷贝到GPU上，模型拷贝</span></span><br></pre></td></tr></table></figure><p>在train函数中，每次使用到数据的地方加入(train and eval)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">token_index = token_index.cuda()  <span class="comment"># tensor cuda拷贝方法,数据拷贝</span></span><br><span class="line">target = target.cuda() <span class="comment"># 数据拷贝</span></span><br><span class="line">eval_target = eval_target.cuda()  <span class="comment"># tensor cuda拷贝方法,数据拷贝</span></span><br><span class="line">eval_token_index = eval_token_index.cuda()  <span class="comment"># 数据拷贝</span></span><br></pre></td></tr></table></figure><h2 id="2-单机多卡"><a href="#2-单机多卡" class="headerlink" title="2 单机多卡"></a>2 单机多卡</h2><p>在 main 函数前加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is available!&quot;</span>)</span><br><span class="line">    os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">        logging.warning(<span class="string">f&quot;find <span class="subst">&#123;torch.cuda.device_count()&#125;</span> GPUs!&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logging.warning(<span class="string">&quot;Too few GPU!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is not available! Exit!&quot;</span>)</span><br></pre></td></tr></table></figure><p>在train函数中，第一次调用模型的地方加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model.cuda(),device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 拷贝到GPU上，模型拷贝,放入DataParallel</span></span><br></pre></td></tr></table></figure><p>在模型的保存中，修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % save_step_interval == <span class="number">0</span>:</span><br><span class="line">    os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    save_file = os.path.join(save_path, <span class="string">f&quot;step_<span class="subst">&#123;step&#125;</span>.pt&quot;</span>)</span><br><span class="line">    torch.save(&#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">&quot;model_state_dict&quot;</span>: model.module.state_dict(),</span><br><span class="line">...</span><br><span class="line">    &#125;, save_file)</span><br><span class="line">    logging.warning(<span class="string">f&quot;checkpoint has been saved in <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这种方式更加合理：</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230415211135334.png" alt="image-20230415211135334"></p><h3 id="2-1-init-process-group"><a href="#2-1-init-process-group" class="headerlink" title="2.1 init_process_group"></a>2.1 init_process_group</h3><p>This sets up the communication backend for distributed training (such as NCCL, Gloo, etc.) and initializes the process group.</p><h4 id="2-1-1-nccl"><a href="#2-1-1-nccl" class="headerlink" title="2.1.1 nccl"></a>2.1.1 nccl</h4><p><a href="https://developer.nvidia.com/nccl">https://developer.nvidia.com/nccl</a></p><h4 id="2-1-2-world-size"><a href="#2-1-2-world-size" class="headerlink" title="2.1.2 world_size"></a>2.1.2 world_size</h4><p>当前节点上有多少张GPU</p><h4 id="2-1-3-local-rank"><a href="#2-1-3-local-rank" class="headerlink" title="2.1.3 local_rank"></a>2.1.3 local_rank</h4><p>当前进程在某张确定的GPU卡上(因为这里采用了多线程，每个线程表示一张GPU)</p><h3 id="2-2-torch-cuda-set-device-args-local-rank"><a href="#2-2-torch-cuda-set-device-args-local-rank" class="headerlink" title="2.2 torch.cuda.set_device(args.local_rank)"></a>2.2 torch.cuda.set_device(args.local_rank)</h3><p>设定某张卡进行训练</p><h3 id="2-3-对模型进行包裹"><a href="#2-3-对模型进行包裹" class="headerlink" title="2.3 对模型进行包裹"></a>2.3 对模型进行包裹</h3><p>类似DataParallel中的操作</p><h3 id="2-4-train-sampler"><a href="#2-4-train-sampler" class="headerlink" title="2.4 train_sampler"></a>2.4 train_sampler</h3><p><a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler">https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler</a></p><p>把dataset中的样本分配当不同的GPU上面，这是随机分配的</p><p>It is especially useful in <strong>conjunction</strong> with class:torch.nn.parallel.<strong>DistributedDataParallel</strong></p><p>每张卡上面的参数的总数量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.num_samples = math.ceil(<span class="built_in">len</span>(self.dataset) / self.num_replicas)  <span class="comment"># type: ignore[arg-<span class="built_in">type</span>]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span><br><span class="line">        <span class="keyword">if</span> self.shuffle:</span><br><span class="line">            <span class="comment"># deterministically shuffle based on epoch and seed</span></span><br><span class="line">            g = torch.Generator()</span><br><span class="line">            g.manual_seed(self.seed + self.epoch)  <span class="comment"># 不改变种子和epoch，顺序是相同的(显然不合理)</span></span><br><span class="line">            indices = torch.randperm(<span class="built_in">len</span>(self.dataset), generator=g).tolist()  <span class="comment"># type: </span></span><br></pre></td></tr></table></figure><p>显然在需要在每个训练之前对此进行修改，调用sampler.set_epoch方法，把epoch传入进来。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152228839.png" alt="image-20230415222851173"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/tutorials/distributed/home.html&quot;&gt;https://pytorch.org/tutorials/distributed/home.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;ht</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目1-基于GCN+DNN的文本分类模型</title>
    <link href="http://example.com/post/27f58942.html"/>
    <id>http://example.com/post/27f58942.html</id>
    <published>2023-04-15T07:04:41.000Z</published>
    <updated>2023-04-15T11:19:17.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-项目介绍"><a href="#1-项目介绍" class="headerlink" title="1 项目介绍"></a>1 项目介绍</h2><p>IMDB dataset having 50K movie reviews for natural language processing or Text analytics.</p><p>This is a dataset for <strong>binary</strong> sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of <strong>25,000</strong> highly polar movie reviews for training and <strong>25,000</strong> for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.</p><p>代码构成：</p><ul><li>GCNN模型</li><li>简单版embeddingbag+DNN模型</li><li>yield_tokens: 对源 comment 进行分词处理</li><li>collate_fn： 对DataLoader所生成的mini-batch进行后处理</li><li>train</li><li>main函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> IMDB</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.imdb <span class="keyword">import</span> NUM_LINES</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.WARN, stream=sys.stdout,</span><br><span class="line">                    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s (%(module)s:%(lineno)d) %(levelname)s:%(message)s&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = <span class="number">15000</span> <span class="comment"># 这里的统计是根据数据集进行处理后得到的</span></span><br><span class="line">train_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;train&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&quot;basic_english&quot;</span>) </span><br><span class="line">vocab = build_vocab_from_iterator(yield_tokens(train_data_iter, tokenizer), min_freq=<span class="number">20</span>, specials=[<span class="string">&quot;&lt;unk&gt;&quot;</span>]) <span class="comment"># 只把出现频率高于20个的词语取出来，其他的单词都变成&lt;unk&gt; # 构建词表</span></span><br><span class="line">vocab.set_default_index(<span class="number">0</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;单词表大小：<span class="subst">&#123;<span class="built_in">len</span>(vocab)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果长时间下载失败，建议直接在以下链接进行下载：</p><p><a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p><p>之后把下载好的文件放在当前目录下的<code>.data</code>中</p><h2 id="2-GCNN模型"><a href="#2-GCNN模型" class="headerlink" title="2 GCNN模型"></a>2 GCNN模型</h2><h3 id="2-1-介绍"><a href="#2-1-介绍" class="headerlink" title="2.1 介绍"></a>2.1 介绍</h3><p><a href="https://arxiv.org/pdf/1612.08083v3.pdf">https://arxiv.org/pdf/1612.08083v3.pdf</a></p><p>门控卷积网络是一种将卷积网络与门控机制相结合的语言模型。使用零填充以确保未来的语境无法被观察。门控卷积层可以层次化地叠加在其他层之上。通过自适应softmax层来获取模型预测结果。</p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304151758805.png" alt="image-20230415175810671" style="zoom:67%;" /><h3 id="2-2-代码"><a href="#2-2-代码" class="headerlink" title="2.2 代码"></a>2.2 代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocal_size=VOCAB_SIZE, embedding_dim=<span class="number">64</span>, num_class=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GCNN, self).__init__()  <span class="comment"># 对父类进行初始化</span></span><br><span class="line"></span><br><span class="line">        self.embedding_table = nn.Embedding(vocal_size, embedding_dim)</span><br><span class="line">        nn.init.xavier_uniform_(self.embedding_table.weight)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;xavier_uniform的出现是为了训练过程中前后的方差稳定问题，正确的初始化有利于训练的稳定；</span></span><br><span class="line"><span class="string">        Xavier初始化表明，对于每⼀层，输出的⽅差不受输⼊数量的影响，任何梯度的⽅差不受输出数量的影响。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.conv_A_1 = nn.Conv1d(embedding_dim, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)  <span class="comment"># input_dim,output_dim,kernel_Size</span></span><br><span class="line">        self.conv_B_1 = nn.Conv1d(embedding_dim, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        self.conv_A_2 = nn.Conv1d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line">        self.conv_B_2 = nn.Conv1d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        self.output_linear1 = nn.Linear(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.output_linear2 = nn.Linear(<span class="number">128</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, word_index</span>):</span><br><span class="line">        <span class="comment"># 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1.通过word_index得到word_embedding</span></span><br><span class="line">        <span class="comment"># word_index shape: [bs,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        word_embedding = self.embedding_table(word_index)  <span class="comment"># [bs,max_seq_len,embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2.编写第一层ID门卷积模块</span></span><br><span class="line">        word_embedding = word_embedding.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [bs,embedding_dim,max_seq_len]</span></span><br><span class="line">        A = self.conv_A_1(word_embedding)</span><br><span class="line">        B = self.conv_B_1(word_embedding)</span><br><span class="line">        H = A * torch.sigmoid(B)  <span class="comment"># [bs,64,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        A = self.conv_A_2(H)</span><br><span class="line">        B = self.conv_B_2(H)</span><br><span class="line">        H = A * torch.sigmoid(B)  <span class="comment"># [bs,64,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.池化并进过全连接层</span></span><br><span class="line">        pool_output = torch.mean(H, dim=-<span class="number">1</span>)  <span class="comment"># 平均池化 得到 [bs,64]</span></span><br><span class="line">        linear1_output = self.output_linear1(pool_output)</span><br><span class="line">        logits = self.output_linear2(linear1_output)  <span class="comment"># [bs,2]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h2 id="3-简单版embeddingbag-DNN模型"><a href="#3-简单版embeddingbag-DNN模型" class="headerlink" title="3 简单版embeddingbag+DNN模型"></a>3 简单版embeddingbag+DNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextClassificationModel</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=VOCAB_SIZE, embed_dim=<span class="number">64</span>, num_class=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TextClassificationModel, self).__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token_index</span>):</span><br><span class="line">        embedded = self.embedding(token_index)  <span class="comment"># shape: [bs,embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure><p>通过embeddingbag可以省略平均池化层操作，变得更加简单</p><p><img src="https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=1024" alt="img"></p><h2 id="4-yield-tokens"><a href="#4-yield-tokens" class="headerlink" title="4 yield_tokens"></a>4 yield_tokens</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">train_data_iter, tokenizer</span>):</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_iter):</span><br><span class="line">        label, comment = sample</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(comment)  <span class="comment"># 把一句话转换成一个个token的列表</span></span><br></pre></td></tr></table></figure><h2 id="5-collate-fn"><a href="#5-collate-fn" class="headerlink" title="5 collate_fn"></a>5 collate_fn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>): </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;mini-batch 中最重要的就是对同一个批次内的数据进行统一处理，比如某些句子很短，需要padding，这样才能进行batch操作&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对dataloader所生成的mini-batch进行后处理&quot;&quot;&quot;</span></span><br><span class="line">    target = []</span><br><span class="line">    token_index = []</span><br><span class="line">    max_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (label, comment) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch):</span><br><span class="line">        tokens = tokenizer(comment)</span><br><span class="line">        token_index.append(vocab(tokens))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens) &gt; max_length:</span><br><span class="line">            max_length = <span class="built_in">len</span>(tokens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> label == <span class="string">&quot;pos&quot;</span>:</span><br><span class="line">            target.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    token_index = [index + [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(index)) <span class="keyword">for</span> index <span class="keyword">in</span> token_index]</span><br><span class="line">    <span class="keyword">return</span> (torch.tensor(target).to(torch.int64), torch.tensor((token_index)).to(torch.int32))  <span class="comment"># target：pos/neg token_index</span></span><br></pre></td></tr></table></figure><h2 id="6-train"><a href="#6-train" class="headerlink" title="6 train"></a>6 train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_data_loader, eval_data_loader, model, optimizer, num_epoch, log_step_interval, save_step_interval,</span></span><br><span class="line"><span class="params">          eval_step_interval, save_path, resume=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;此处的data_loader是map-style dataset&quot;&quot;&quot;</span></span><br><span class="line">    start_epoch = <span class="number">0</span></span><br><span class="line">    start_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> resume != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="comment"># 加载之前训练过的模型的参数文件</span></span><br><span class="line">        logging.warning(<span class="string">f&quot;loading from <span class="subst">&#123;resume&#125;</span>&quot;</span>)</span><br><span class="line">        checkpoint = torch.load(resume)</span><br><span class="line">        model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">        optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">        start_epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">        start_step = checkpoint[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch_index <span class="keyword">in</span> <span class="built_in">range</span>(start_epoch, num_epoch):</span><br><span class="line">        ema_loss = <span class="number">0.</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;https://www.investopedia.com/terms/e/ema.asp&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        num_bathes = <span class="built_in">len</span>(train_data_loader)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch_index, (target, token_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_loader):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            step = num_bathes * epoch_index + batch_index + <span class="number">1</span></span><br><span class="line">            logits = model(token_index)</span><br><span class="line">            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits),</span><br><span class="line">                                              F.one_hot(target, num_classes=<span class="number">2</span>).to(torch.float32))  <span class="comment"># 维度需要相同，把target转换</span></span><br><span class="line">            ema_loss = <span class="number">0.9</span> * ema_loss + <span class="number">0.1</span> * bce_loss  <span class="comment"># 指数平均loss</span></span><br><span class="line">            bce_loss.backward()  <span class="comment"># 梯度回传</span></span><br><span class="line">            nn.utils.clip_grad_norm(model.parameters(), <span class="number">0.1</span>)</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;https://blog.csdn.net/Mikeyboi/article/details/119522689&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % log_step_interval == <span class="number">0</span>:</span><br><span class="line">                logging.warning(</span><br><span class="line">                    <span class="string">f&quot;epoch_index:<span class="subst">&#123;epoch_index&#125;</span>,batch_index:<span class="subst">&#123;batch_index&#125;</span>,ema_loss:<span class="subst">&#123;ema_loss.item()&#125;</span>&quot;</span>)  <span class="comment"># 避免使用张量的形式打印，而是使用python格式，防止印象性能</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % save_step_interval == <span class="number">0</span>:</span><br><span class="line">                os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">                save_file = os.path.join(save_path, <span class="string">f&quot;step_<span class="subst">&#123;step&#125;</span>.pt&quot;</span>)</span><br><span class="line">                torch.save(&#123;</span><br><span class="line">                    <span class="string">&quot;epoch&quot;</span>: epoch_index,</span><br><span class="line">                    <span class="string">&quot;step&quot;</span>: step,</span><br><span class="line">                    <span class="string">&quot;model_state_dict&quot;</span>: model.state_dict(),</span><br><span class="line">                    <span class="string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: bce_loss</span><br><span class="line">                &#125;, save_file)</span><br><span class="line">                logging.warning(<span class="string">f&quot;checkpoint has been saved in <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % eval_step_interval == <span class="number">0</span>:</span><br><span class="line">                logging.warning(<span class="string">&quot;start to do evaluation...&quot;</span>)</span><br><span class="line">                model.<span class="built_in">eval</span>()  <span class="comment"># evaluation ,only forward calculation</span></span><br><span class="line">                eval_ema_loss = <span class="number">0</span></span><br><span class="line">                total_acc_account = <span class="number">0</span></span><br><span class="line">                total_account = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> eval_batch_index, (eval_target, eval_token_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_data_loader):</span><br><span class="line">                    total_account += eval_target.shape[<span class="number">0</span>]</span><br><span class="line">                    eval_logits = model(eval_token_index)</span><br><span class="line">                    total_acc_account += (torch.argmax(eval_logits, dim=-<span class="number">1</span>) == eval_target).<span class="built_in">sum</span>().item()</span><br><span class="line">                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(logits),</span><br><span class="line">                                                           F.one_hot(target, num_classes=<span class="number">2</span>).to(torch.float32))</span><br><span class="line">                    eval_ema_loss = <span class="number">0.9</span> * eval_ema_loss + <span class="number">0.1</span> * eval_bce_loss  <span class="comment"># 指数平均loss</span></span><br><span class="line">                    acc = total_acc_account / total_account  <span class="comment"># 精确度：一样的次数/总次数</span></span><br><span class="line">                logging.warning(</span><br><span class="line">                    <span class="string">f&quot;eval_ema_loss:<span class="subst">&#123;eval_ema_loss.item()&#125;</span>,eval_acc:<span class="subst">&#123;acc.item()&#125;</span>&quot;</span>)</span><br><span class="line">                model.train()</span><br></pre></td></tr></table></figure><h2 id="7-main"><a href="#7-main" class="headerlink" title="7 main"></a>7 main</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = GCNN()</span><br><span class="line">    <span class="comment"># model = TextClassificationModel()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型总参数&quot;</span>, <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()))</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    train_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;train&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">    train_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(train_data_iter), batch_size=BATCH_SIZE,</span><br><span class="line">                                                    collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">    eval_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;test&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">    eval_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(eval_data_iter), batch_size=<span class="number">8</span>,</span><br><span class="line">                                                   collate_fn=collate_fn) <span class="comment"># 变成map_style </span></span><br><span class="line">    resume = <span class="string">&#x27;F:\study\code\pytorch\logs_imdb_text_classification\step_500.pt&#x27;</span></span><br><span class="line">    train(train_data_loader, eval_data_loader, model, optimizer, num_epoch=<span class="number">10</span>, log_step_interval=<span class="number">20</span>,</span><br><span class="line">          save_step_interval=<span class="number">500</span>,</span><br><span class="line">          eval_step_interval=<span class="number">300</span>, save_path=<span class="string">&#x27;./logs_imdb_text_classification&#x27;</span>, resume=resume)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-项目介绍&quot;&gt;&lt;a href=&quot;#1-项目介绍&quot; class=&quot;headerlink&quot; title=&quot;1 项目介绍&quot;&gt;&lt;/a&gt;1 项目介绍&lt;/h2&gt;&lt;p&gt;IMDB dataset having 50K movie reviews for natural lang</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门14-GRU</title>
    <link href="http://example.com/post/f22b56ba.html"/>
    <id>http://example.com/post/f22b56ba.html</id>
    <published>2023-04-14T09:47:29.000Z</published>
    <updated>2023-04-15T07:03:45.911Z</updated>
    
    <content type="html"><![CDATA[<p>torch.nn.GRU(*<em>args</em>, **<em>kwargs</em>)</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU">https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU</a><br>$$<br>\begin{aligned}<br>r_{t} &amp; =\sigma\left(W_{i r} x_{t}+b_{i r}+W_{h r} h_{(t-1)}+b_{h r}\right) \<br>z_{t} &amp; =\sigma\left(W_{i z} x_{t}+b_{i z}+W_{h z} h_{(t-1)}+b_{h z}\right) \<br>n_{t} &amp; =\tanh \left(W_{i n} x_{t}+b_{i n}+r_{t} *\left(W_{h n} h_{(t-1)}+b_{h n}\right)\right) \<br>h_{t} &amp; =\left(1-z_{t}\right) * n_{t}+z_{t} * h_{(t-1)}<br>\end{aligned}<br>$$<br>同等<code>hidden size</code>的参数量，GRU是LSTM的$\frac{3}{4}$</p><p>何时使用GRU，何时使用LSTM？</p><p><a href="https://arxiv.org/pdf/1412.3555.pdf">https://arxiv.org/pdf/1412.3555.pdf</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304141848732.png" alt="image-20230414180541344"></p><h2 id="1-API"><a href="#1-API" class="headerlink" title="1 API"></a>1 API</h2><h3 id="1-1-Parameters"><a href="#1-1-Parameters" class="headerlink" title="1.1 Parameters"></a>1.1 Parameters</h3><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional GRU. Default: <code>False</code></li></ul><h2 id="2-实现"><a href="#2-实现" class="headerlink" title="2 实现"></a>2 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru_forward</span>(<span class="params"><span class="built_in">input</span>,initial_states,w_ih,w_hh,b_ih,b_hh</span>):</span><br><span class="line">    prev_h = initial_states</span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对权重扩维，复制成batch_size倍</span></span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    output = torch.zeros(bs,T,h_size) <span class="comment"># GRU网络的输出状态序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># t时刻的GRU cell的输入特征向量 [bs,i_size]</span></span><br><span class="line">        w_time_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment">#[bs,3*i_size,1]</span></span><br><span class="line">        w_time_x = w_time_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_time_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment">#[bs,3*i_size,1]</span></span><br><span class="line">        w_time_h_prev = w_time_h_prev.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        r_t = torch.sigmoid(w_time_x[:,:h_size] + w_time_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size]) <span class="comment"># 重置门</span></span><br><span class="line">        z_t = torch.sigmoid(w_time_x[:,h_size:<span class="number">2</span>*h_size] + w_time_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size]) <span class="comment"># 更新门</span></span><br><span class="line">        </span><br><span class="line">        n_t = torch.tanh(w_time_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size]+b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size]+</span><br><span class="line">                         r_t*(w_time_h_prev[:,<span class="number">2</span>*h_size: <span class="number">3</span>*h_size]+b_hh[<span class="number">2</span>*h_size: <span class="number">3</span>*h_size]))   <span class="comment"># 候选状态</span></span><br><span class="line">        </span><br><span class="line">        prev_h = (<span class="number">1</span>-z_t)*n_t + z_t*prev_h <span class="comment"># 增量更新得到当前时刻最新隐含状态</span></span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,prev_h</span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size)</span><br><span class="line">h0 = torch.randn(bs,h_size)</span><br><span class="line"></span><br><span class="line">gru_layer = nn.GRU(i_size,h_size,batch_first = <span class="literal">True</span>)</span><br><span class="line">output,h_final = gru_layer(<span class="built_in">input</span>,h0.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">output_custom,h_final_custom = gru_forward(<span class="built_in">input</span>,h0,gru_layer.weight_ih_l0,gru_layer.weight_hh_l0,gru_layer.bias_ih_l0,gru_layer.bias_hh_l0)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,h_final_custom))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(output,output_custom))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;torch.nn.GRU(*&lt;em&gt;args&lt;/em&gt;, **&lt;em&gt;kwargs&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门13-LSTM</title>
    <link href="http://example.com/post/23f44fab.html"/>
    <id>http://example.com/post/23f44fab.html</id>
    <published>2023-04-13T10:13:13.000Z</published>
    <updated>2023-04-13T12:26:22.728Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131816130.png" alt="image-20230413181558550"></p><h2 id="1-整体介绍"><a href="#1-整体介绍" class="headerlink" title="1 整体介绍"></a>1 整体介绍</h2><p>$$<br>\begin{array}{l}<br>i_{t}=\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \<br>f_{t}=\sigma\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \<br>g_{t}=\tanh \left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \<br>o_{t}=\sigma\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\right) \<br>c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot g_{t} \<br>h_{t}=o_{t} \odot \tanh \left(c_{t}\right)<br>\end{array}<br>$$</p><ul><li><p>$h_t$: hidden state at time $t$</p></li><li><p>$c_t$: cell state at time $t$</p></li><li><p>$x_t$: input at time $t$</p></li><li><p>$h_{t-1}$: hidden state of the layer at time $t-1$ or the initial hidden state at time o</p></li><li><p>$i_t$: input</p></li><li><p>$f_t$: forget</p></li><li><p>$g_t$: cell</p></li><li><p>$o_t$: output gates</p></li><li><p>$\sigma$: sigmoid function</p></li><li><p>$\odot$: Hadamard product</p></li><li><p>$N$ = batch size</p></li><li><p>$L$ = sequence length</p></li><li><p>$D$ = 2 if bidirectional = True otherwise 1</p></li><li><p>$H_{in}$ = input_size</p></li><li><p>$H_{cell}$ = hidden_size</p></li><li><p>$H_{out}$ = pro_size if pro_size &gt; 0 otherwise hidden_size</p></li></ul><h3 id="1-1-Parameters"><a href="#1-1-Parameters" class="headerlink" title="1.1 Parameters"></a>1.1 Parameters</h3><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional LSTM. Default: <code>False</code></li><li><strong>proj_size</strong> – If <code>&gt; 0</code>, will use LSTM with projections of corresponding size. Default: 0 减少LTSM的参数和计算量</li></ul><h3 id="1-2-Inputs-input-h-0-c-0"><a href="#1-2-Inputs-input-h-0-c-0" class="headerlink" title="1.2 Inputs: input, (h_0, c_0)"></a>1.2 Inputs: input, (h_0, c_0)</h3><ul><li><p>input：</p><ul><li>$(L,H_{in})$-&gt; unbatched input</li><li>$(L,N,H_{in})$-&gt;<code>batch_first=False</code></li><li>$(N,L,H_{in})$-&gt;<code>batch_first=True</code></li></ul></li><li><p>h_0: Defaults to zeros if (h_0, c_0) is not provided</p><ul><li>$(D*num_layers,H_{out})$ for unbatched input</li><li>$(D*num_layers,N,H_{out})$ containing the initial hidden state for each element in the input sequence.</li></ul></li><li><p>c_0: Defaults to zeros if (h_0, c_0) is not provided</p><ul><li>$(D*num_layers,H_{cell})$ for unbatched input</li><li>$(D*num_layers,N,H_{cell})$ containing the initial cell state for each element in the input sequence.</li></ul></li></ul><h3 id="1-3-Outputs-output-h-n-c-n"><a href="#1-3-Outputs-output-h-n-c-n" class="headerlink" title="1.3 Outputs: output, (h_n, c_n)"></a>1.3 Outputs: output, (h_n, c_n)</h3><ul><li><p>output:</p><ul><li>$(L,D*H_{out})$-&gt;unbatched input</li><li>$(L,N,D*H_{out})$-&gt;<code>batch_first=False</code> containing the output features (h_t) from the last layer of the LSTM,for each t</li><li>$(N,L,D*H_{out})$-&gt;<code>batch_first=True</code>  containing the output features (h_t) from the last layer of the LSTM,for each t</li><li>If a <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> has been given as the input, the output will also be a packed sequence</li><li>When <code>bidirectional=True</code>, output will contain a concatenation of the forward and reverse hidden states at each time step in the sequence</li></ul></li><li><p>h_n:</p><ul><li>$(D*num_layers,H_{out})$-&gt; unbatched input</li><li>$(D*num_layers,N,H_{out})$-&gt;containing the final hidden state for each element in the sequence. </li><li>When <code>bidirectional=True</code>, h_n will contain a concatenation of the final forward and reverse hidden states, respectively.</li></ul></li><li><p>c_n:</p><ul><li>$(D*num_layers,H_{cell})$-&gt; unbatched input</li><li>$(D*num_layers,N,H_{cell})$-&gt;containing the final hidden state for each element in the sequence. </li><li>When <code>bidirectional=True</code>, h_n will contain a concatenation of the final forward and reverse hidden states, respectively.</li></ul></li></ul><h3 id="1-4-Variables"><a href="#1-4-Variables" class="headerlink" title="1.4 Variables"></a>1.4 Variables</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131856525.png" alt="image-20230413185629132"></p><h2 id="2-调用API"><a href="#2-调用API" class="headerlink" title="2 调用API"></a>2 调用API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line"><span class="comment"># proj_size = </span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size) <span class="comment"># 输入序列</span></span><br><span class="line">c0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line">h0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用官方LSTM API</span></span><br><span class="line">lstm_layer = nn.LSTM(i_size,h_size,batch_first=<span class="literal">True</span>)</span><br><span class="line">output,(h_final,c_final) = lstm_layer(<span class="built_in">input</span>,(h0.unsqueeze(<span class="number">0</span>),c0.unsqueeze(<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k,v.shape)</span><br></pre></td></tr></table></figure><h2 id="3-LSTM-without-proj-size"><a href="#3-LSTM-without-proj-size" class="headerlink" title="3 LSTM without proj_size"></a>3 LSTM without proj_size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自己写一个LSTM模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_forward</span>(<span class="params"><span class="built_in">input</span>,inittal_states,w_ih,w_hh,b_ih,b_hh</span>):</span><br><span class="line">    h0,c0 = inittal_states <span class="comment"># 初始状态</span></span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = c0</span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,i_size)</span></span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,h_size)</span></span><br><span class="line">    </span><br><span class="line">    output_size = h_size</span><br><span class="line">    output = torch.zeros(bs,T,output_size) <span class="comment"># 输出序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># 当前时刻的输入向量 (bs,i_size)</span></span><br><span class="line">        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_x = w_times_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_h_prev = w_times_h_prev.squeeze(-<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分別计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)</span></span><br><span class="line">        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])</span><br><span class="line">        f_t = torch.sigmoid(w_times_x[:,h_size:<span class="number">2</span>*h_size] + w_times_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size])</span><br><span class="line">        g_t = torch.tanh(w_times_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + w_times_h_prev[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_hh[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size])</span><br><span class="line">        o_t = torch.sigmoid(w_times_x[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + w_times_h_prev[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_ih[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_hh[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size])</span><br><span class="line"></span><br><span class="line">        prev_c = f_t * prev_c + i_t * g_t</span><br><span class="line">        prev_h = o_t * torch.tanh(prev_c)</span><br><span class="line">        </span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,(prev_h,prev_c)</span><br><span class="line">    </span><br><span class="line">custom_output,(custom_h_final,custom_c_final) = lstm_forward(<span class="built_in">input</span>,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)    </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_output,output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,custom_h_final))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(c_final,custom_c_final))</span><br></pre></td></tr></table></figure><h2 id="4-LSTM-with-proj-size"><a href="#4-LSTM-with-proj-size" class="headerlink" title="4 LSTM with proj_size"></a>4 LSTM with proj_size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">proj_size = <span class="number">3</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size) <span class="comment"># 输入序列</span></span><br><span class="line">c0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line">h0 = torch.randn(bs,proj_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用官方LSTM API</span></span><br><span class="line">lstm_layer = nn.LSTM(i_size,h_size,batch_first=<span class="literal">True</span>,proj_size=proj_size)</span><br><span class="line">output,(h_final,c_final) = lstm_layer(<span class="built_in">input</span>,(h0.unsqueeze(<span class="number">0</span>),c0.unsqueeze(<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k,v)</span><br></pre></td></tr></table></figure><p>eight_ih_l0 torch.Size([20, 4])    h_size<em>4,i_size<br>weight_hh_l0 torch.Size([20, 3])    h_size</em>4,proj_size<br>bias_ih_l0 torch.Size([20])         h_size<em>4<br>bias_hh_l0 torch.Size([20])         h_size</em>4<br>weight_hr_l0 torch.Size([3, 5])     proj_size,h_size<br>只会对 h_state 进行改变，不会对 c_state 进行改变 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_forward</span>(<span class="params"><span class="built_in">input</span>,inittal_states,w_ih,w_hh,b_ih,b_hh,w_hr=<span class="literal">None</span></span>):</span><br><span class="line">    h0,c0 = inittal_states <span class="comment"># 初始状态</span></span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = c0</span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,i_size)</span></span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,h_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> w_hr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_size = w_hr.shape[<span class="number">0</span>]</span><br><span class="line">        output_size = p_size</span><br><span class="line">        batch_w_hr = w_hr.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,proj_size,h_size)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output_size = h_size</span><br><span class="line">    output = torch.zeros(bs,T,output_size) <span class="comment"># 输出序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># 当前时刻的输入向量 (bs,i_size)</span></span><br><span class="line">        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_x = w_times_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_h_prev = w_times_h_prev.squeeze(-<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分別计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)</span></span><br><span class="line">        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])</span><br><span class="line">        f_t = torch.sigmoid(w_times_x[:,h_size:<span class="number">2</span>*h_size] + w_times_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size])</span><br><span class="line">        g_t = torch.tanh(w_times_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + w_times_h_prev[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_hh[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size])</span><br><span class="line">        o_t = torch.sigmoid(w_times_x[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + w_times_h_prev[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_ih[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_hh[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size])</span><br><span class="line"></span><br><span class="line">        prev_c = f_t * prev_c + i_t * g_t</span><br><span class="line">        prev_h = o_t * torch.tanh(prev_c)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> w_hr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 做projection</span></span><br><span class="line">            prev_h = torch.bmm(batch_w_hr,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># bs,proj_size,1</span></span><br><span class="line">            prev_h = prev_h.squeeze(-<span class="number">1</span>) <span class="comment"># bs,proj_size</span></span><br><span class="line">            </span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,(prev_h,prev_c)</span><br><span class="line">    </span><br><span class="line">custom_output,(custom_h_final,custom_c_final) = lstm_forward(<span class="built_in">input</span>,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,</span><br><span class="line">                                                             lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0,lstm_layer.weight_hr_l0)    </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_output,output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,custom_h_final))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(c_final,custom_c_final))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/a&gt;&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门12-RNN</title>
    <link href="http://example.com/post/b945630.html"/>
    <id>http://example.com/post/b945630.html</id>
    <published>2023-04-13T06:29:13.000Z</published>
    <updated>2023-04-13T09:31:09.288Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cs.toronto.edu/~graves/preprint.pdf">https://www.cs.toronto.edu/~graves/preprint.pdf</a></p><p>Supervised Sequence Labelling with Recurrent Neural Networks</p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131437010.png" alt="image-20230413143700100" style="zoom:67%;" /><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131437818.png" alt="image-20230413143710020" style="zoom:67%;" /><p>delay: 能看到前几帧的信息，牺牲一定的时间，预测的更加准确。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131448069.png" alt="image-20230413144847329"></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131502377.png" alt="image-20230413150234647"></p><h2 id="1-API"><a href="#1-API" class="headerlink" title="1 API"></a>1 API</h2><p>torch.nn.RNN(*<em>args</em>, **<em>kwargs</em>)</p><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN">https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN</a></p><p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.</p><p>For each element in the input sequence, each layer computes the following function:<br>$$<br>h_t = tanh(x_tW_{ih}^T+b_{ih}+h_{t-1}W_{hh}^T+b_{hh})<br>$$<br>where $h_t$ is the hidden state at time t, $x_t$ is the input at time t, and $h_{t-1}$ is the hidden state of the previous layer at time t-1 or the inital hidden state at time 0. If nonlinearity is “relu”, then ReLU is used instead of tanh.</p><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</li><li><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code>&#39;tanh&#39;</code> or <code>&#39;relu&#39;</code>. Default: <code>&#39;tanh&#39;</code></li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional RNN. Default: <code>False</code></li></ul><p>Inputs: input, h_0</p><ul><li><p>inputs: tensor of shape $(L,H_{in})$ for unbatched input, $(L,N,H_{in})$ when <code>batch_first=False</code> or $(N,L,H_{in})$ when <code>batch_first=True</code></p></li><li><p>h_0: tensor of shape $(D<em>num_layer,H_{out})$ for unbatched input or$ (D</em>num_layers,N,H_{out})$containing the initial hidden state for the input sequence batch. Defaults to zeros if not provided.</p></li></ul><p>$$<br>N=batch\ size \<br>L = sequence\ length \<br>D = 2\ if\ bidirectional = True\ otherwise\ 1 \<br>H_{in} = input _size \<br>H_{out} = hidden_size<br>$$</p><p>Outputs: output, h_n</p><h2 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 单向，单层RNN</span></span><br><span class="line">single_rnn = nn.RNN(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,batch_first=<span class="literal">True</span>) <span class="comment"># feature_size * hidden_size * layer_num</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>) <span class="comment"># batch_size*sequence_length*feature_size</span></span><br><span class="line">output,h_n = single_rnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output) <span class="comment"># batch_size*sequence_length*hidden_size</span></span><br><span class="line"><span class="built_in">print</span>(h_n) <span class="comment"># layer_num*batch_size*hidden_size</span></span><br><span class="line"><span class="comment"># 双向，单层RNN</span></span><br><span class="line">bidirectional_rnn = nn.RNN(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,batch_first=<span class="literal">True</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line">bi_output,bi_h_n = bidirectional_rnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(bi_output) <span class="comment"># batch_size*sequence_length*(2*hidden_size)</span></span><br><span class="line"><span class="built_in">print</span>(bi_h_n) <span class="comment"># (2*layer_num)*batch_size*hidden_size</span></span><br></pre></td></tr></table></figure><h2 id="3-实现单向单层RNN"><a href="#3-实现单向单层RNN" class="headerlink" title="3 实现单向单层RNN"></a>3 实现单向单层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step2: 手写一个rnn_forward函数,实现单向rnn的计算原理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_forward</span>(<span class="params"><span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev</span>):</span><br><span class="line">    bs,T,input_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_dim = weight_ih.shape[<span class="number">0</span>]</span><br><span class="line">    h_out = torch.zeros(bs,T,h_dim) <span class="comment"># 初始化一个输出(状态)矩阵</span></span><br><span class="line">    <span class="comment"># h_prev: bs*hidden_size</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:].unsqueeze(<span class="number">2</span>) <span class="comment">#获取当前时刻输入 # bs*input_size*1</span></span><br><span class="line">        w_ih_batch = weight_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># bs*h_dim*input_size</span></span><br><span class="line">        w_hh_batch = weight_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># bs*h_dim*h_dim</span></span><br><span class="line">        </span><br><span class="line">        w_time_x = torch.bmm(w_ih_batch,x).squeeze(-<span class="number">1</span>) <span class="comment"># bs*h_dim</span></span><br><span class="line">        w_time_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(<span class="number">2</span>)).squeeze(-<span class="number">1</span>) <span class="comment">#bs*h_dim</span></span><br><span class="line">        h_prev = torch.tanh(w_time_x + bias_ih + w_time_h + bias_hh) <span class="comment"># t时刻的输出</span></span><br><span class="line">        </span><br><span class="line">        h_out[:,t,:] = h_prev</span><br><span class="line">    <span class="keyword">return</span> h_out,h_prev.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 验证一下rnn_forward的正确性</span></span><br><span class="line"><span class="comment"># for k,v in rnn.named_parameters():</span></span><br><span class="line"><span class="comment">#     print(k,v)</span></span><br><span class="line">custom_rnn_output,custom_state_final = run_forward(<span class="built_in">input</span>,rnn.weight_ih_l0,rnn.weight_hh_l0,rnn.bias_ih_l0,rnn.bias_hh_l0,h_prev)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_rnn_output,rnn_output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_state_final,state_final))</span><br></pre></td></tr></table></figure><h2 id="4-实现双向单层RNN"><a href="#4-实现双向单层RNN" class="headerlink" title="4 实现双向单层RNN"></a>4 实现双向单层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3: 手写一个bidirectional_rnn_forward函数，实现双向RNN的计算原理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bidirectional_run_forward</span>(<span class="params"><span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev,</span></span><br><span class="line"><span class="params">                              weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse</span>):</span><br><span class="line">    bs,T,input_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_dim = weight_ih.shape[<span class="number">0</span>]</span><br><span class="line">    h_out = torch.zeros(bs,T,h_dim*<span class="number">2</span>) <span class="comment"># 初始化一个输出(状态)矩阵</span></span><br><span class="line">    </span><br><span class="line">    forward_output = run_forward(<span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev)[<span class="number">0</span>] <span class="comment"># forward layer</span></span><br><span class="line">    backward_output = run_forward(torch.flip(<span class="built_in">input</span>,[<span class="number">1</span>]),</span><br><span class="line">                      weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse)[<span class="number">0</span>]  <span class="comment"># flip 对dim进行翻转, backward layer</span></span><br><span class="line">    </span><br><span class="line">    h_out[:,:,:h_dim] = forward_output</span><br><span class="line">    h_out[:,:,h_dim:] = torch.flip(backward_output,[<span class="number">1</span>]) <span class="comment"># 反向的结果需要在T维度上再次反向，才能与api结果相同</span></span><br><span class="line">    </span><br><span class="line">    h_n = torch.zeros(<span class="number">2</span>,bs,h_dim)</span><br><span class="line">    h_n[<span class="number">0</span>,:,:] = forward_output[:,-<span class="number">1</span>,:]</span><br><span class="line">    h_n[<span class="number">1</span>,:,:] = backward_output[:,-<span class="number">1</span>,:]</span><br><span class="line">        <span class="comment"># h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)</span></span><br><span class="line">    <span class="keyword">return</span> h_out, h_n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证一下 bidirectional_rnn_forward正确性</span></span><br><span class="line">bi_rnn = nn.RNN(input_size,hidden_size,batch_first=<span class="literal">True</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line">h_prev =torch.zeros(<span class="number">2</span>,bs,hidden_size)</span><br><span class="line">bi_rnn_output,bi_state_final = bi_rnn(<span class="built_in">input</span>,h_prev)</span><br><span class="line"></span><br><span class="line">custom_bi_rnn_output,custom_bi_state_final = bidirectional_run_forward(<span class="built_in">input</span>,</span><br><span class="line">                                                                       bi_rnn.weight_ih_l0,</span><br><span class="line">                                                                       bi_rnn.weight_hh_l0,</span><br><span class="line">                                                                       bi_rnn.bias_ih_l0,</span><br><span class="line">                                                                       bi_rnn.bias_hh_l0,</span><br><span class="line">                                                                       h_prev[<span class="number">0</span>],</span><br><span class="line">                                                                       bi_rnn.weight_ih_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.weight_hh_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.bias_ih_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.bias_hh_l0_reverse,</span><br><span class="line">                                                                       h_prev[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(bi_rnn_output,custom_bi_rnn_output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(bi_state_final,custom_bi_state_final))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~graves/preprint.pdf&quot;&gt;https://www.cs.toronto.edu/~graves/preprint.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Supervised Sequence </summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识11-ViT</title>
    <link href="http://example.com/post/1229d3b9.html"/>
    <id>http://example.com/post/1229d3b9.html</id>
    <published>2023-04-12T15:59:34.000Z</published>
    <updated>2023-04-13T06:33:31.369Z</updated>
    
    <content type="html"><![CDATA[<p>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p><p><a href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131331748.png" alt="image-20230413132929498"  /><p>We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017).</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131433347.png" alt="image-20230413143320971"></p><h2 id="1-image2emb"><a href="#1-image2emb" class="headerlink" title="1 image2emb"></a>1 image2emb</h2><h3 id="1-1-分块"><a href="#1-1-分块" class="headerlink" title="1.1 分块"></a>1.1 分块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_naive</span>(<span class="params">image,patch_size,weight</span>):</span><br><span class="line">    <span class="comment"># image size: bs*channel*h*w</span></span><br><span class="line">    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># bs*patch_num*patch_pixel_num</span></span><br><span class="line">    patch_embedding = patch @ weight</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">        </span><br><span class="line"><span class="comment"># test code for image2emb</span></span><br><span class="line">bs,ic,image_h,image_w = <span class="number">1</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">8</span></span><br><span class="line">patch_size = <span class="number">4</span></span><br><span class="line">model_dim = <span class="number">8</span> </span><br><span class="line">patch_depth = patch_size*patch_size*ic <span class="comment"># 每个patch中的元素个数</span></span><br><span class="line">image = torch.randn(bs,ic,image_h,image_w)</span><br><span class="line">weight = torch.randn(patch_depth,model_dim) <span class="comment"># model_dim 是输出通道数目，patch_depth是卷积核的面积乘以输入通道数</span></span><br><span class="line">patch_embedding_naive = image2emb_naive(image,patch_size,weight) <span class="comment"># 分块得到embedding</span></span><br><span class="line"><span class="built_in">print</span>(patch_embedding_naive.shape)</span><br></pre></td></tr></table></figure><h3 id="1-2-conv2d"><a href="#1-2-conv2d" class="headerlink" title="1.2 conv2d"></a>1.2 conv2d</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_conv</span>(<span class="params">image,kernel,stride</span>):</span><br><span class="line">    conv_output = F.conv2d(image,kernel,stride=stride) <span class="comment"># bs*oc*oh*ow</span></span><br><span class="line">    bs,oc,oh,ow = conv_output.shape</span><br><span class="line">    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line"></span><br><span class="line">kernel = weight.transpose(<span class="number">0</span>,<span class="number">1</span>).reshape((-<span class="number">1</span>,ic,patch_size,patch_size)) <span class="comment"># oc*ic*kh*kw</span></span><br><span class="line">patch_embedding_conv = image2emb_conv(image,kernel,stride=patch_size) <span class="comment"># 二维卷积的方法得到embedding</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(patch_embedding_conv,patch_embedding_naive))</span><br></pre></td></tr></table></figure><h2 id="2-classification-token"><a href="#2-classification-token" class="headerlink" title="2 classification token"></a>2 classification token</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cls_token_embedding = torch.randn(bs,<span class="number">1</span>,model_dim,requires_grad=<span class="literal">True</span>)</span><br><span class="line">token_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="3-add-position-embedding"><a href="#3-add-position-embedding" class="headerlink" title="3 add position embedding"></a>3 add position embedding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">position_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=<span class="literal">True</span>)</span><br><span class="line">seq_len = token_embedding.shape[<span class="number">1</span>]</span><br><span class="line">position_embedding = torch.tile(position_embedding_table[:seq_len],[token_embedding.shape[<span class="number">0</span>],<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">token_embedding += position_embedding</span><br></pre></td></tr></table></figure><h2 id="4-pass-embedding-to-Transformer-Encoder"><a href="#4-pass-embedding-to-Transformer-Encoder" class="headerlink" title="4 pass embedding to Transformer Encoder"></a>4 pass embedding to Transformer Encoder</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">encoder_output = transformer_encoder(token_embedding)</span><br></pre></td></tr></table></figure><h2 id="5-do-classification"><a href="#5-do-classification" class="headerlink" title="5 do classification"></a>5 do classification</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cls_token_output = encoder_output[:,<span class="number">0</span>,:]</span><br><span class="line">liner_layer = nn.Linear(model_dim,num_classes)</span><br><span class="line">logits = liner_layer(cls_token_output)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss = loss_fn(logits,label)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;https://</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识10-卷积网络</title>
    <link href="http://example.com/post/b0d26e2a.html"/>
    <id>http://example.com/post/b0d26e2a.html</id>
    <published>2023-04-10T09:16:59.000Z</published>
    <updated>2023-04-12T15:56:33.111Z</updated>
    
    <content type="html"><![CDATA[<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101930110.png" alt="image-20230410193006020" style="zoom:80%;" /><h2 id="1-卷积的API"><a href="#1-卷积的API" class="headerlink" title="1 卷积的API"></a>1 卷积的API</h2><h3 id="1-1-CONV2D"><a href="#1-1-CONV2D" class="headerlink" title="1.1 CONV2D"></a>1.1 CONV2D</h3><p>torch.nn.Conv2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode=’zeros’</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101932268.png" alt="image-20230410193201892" style="zoom:67%;" /><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels in the input image</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – Size of the convolving kernel</li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – Padding added to all four sides of the input. Default: 0</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – <code>&#39;zeros&#39;</code>, <code>&#39;reflect&#39;</code>, <code>&#39;replicate&#39;</code> or <code>&#39;circular&#39;</code>. Default: <code>&#39;zeros&#39;</code></li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></li></ul><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304111953529.png" alt="image-20230411195325050"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">in_channels = <span class="number">1</span></span><br><span class="line">out_channels = <span class="number">1</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">bias = <span class="literal">False</span></span><br><span class="line">input_size = [in_channels,<span class="number">4</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">conv_layer = nn.Conv2d(in_channels,out_channels,kernel_size,bias=bias)</span><br><span class="line">input_feature_map = torch.randn(input_size)</span><br><span class="line">output_feature_map = conv_layer(input_feature_map) <span class="comment"># 直接调用卷积这个方法</span></span><br><span class="line"><span class="built_in">print</span>(input_feature_map,<span class="string">&#x27;\n&#x27;</span>,output_feature_map)</span><br><span class="line"><span class="built_in">print</span>(conv_layer.weight) <span class="comment"># 1*1*3*3 out_channels*in_channels*height*width</span></span><br></pre></td></tr></table></figure><h3 id="1-2-FUNCTIONAL-CONV2D"><a href="#1-2-FUNCTIONAL-CONV2D" class="headerlink" title="1.2 FUNCTIONAL.CONV2D"></a>1.2 FUNCTIONAL.CONV2D</h3><p>torch.nn.functional.conv2d(<em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112015956.png" alt="image-20230411201534888" style="zoom:67%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_feature_map1 = F.conv2d(input_feature_map,conv_layer.weight) <span class="comment"># functional,需要传入卷积的weight</span></span><br><span class="line"><span class="built_in">print</span>(output_feature_map)</span><br><span class="line"><span class="built_in">print</span>(output_feature_map1)</span><br></pre></td></tr></table></figure><h2 id="2-padding-and-stride"><a href="#2-padding-and-stride" class="headerlink" title="2 padding and stride"></a>2 padding and stride</h2><p><a href="https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html">https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html</a></p><h3 id="2-1-padding"><a href="#2-1-padding" class="headerlink" title="2.1 padding"></a>2.1 padding</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112028509.png" alt="image-20230411202806742"></p><p>In general, if we add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and a total of $p_w$ columns of padding (roughly half on the left and half on the right), the output shape will be<br>$$<br>(n_k - k_h + p_h + 1)\times (n_w - k_w + p_w + 1)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We define a helper function to calculate convolutions. It initializes the</span></span><br><span class="line"><span class="comment"># convolutional layer weights and performs corresponding dimensionality</span></span><br><span class="line"><span class="comment"># elevations and reductions on the input and output</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># (1, 1) indicates that batch size and the number of channels are both 1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># Strip the first two dimensions: examples and channels</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 row and column is padded on either side, so a total of 2 rows or columns</span></span><br><span class="line"><span class="comment"># are added</span></span><br><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We use a convolution kernel with height 5 and width 3. The padding on either</span></span><br><span class="line"><span class="comment"># side of the height and width are 2 and 1, respectively</span></span><br><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><h3 id="2-2-stride"><a href="#2-2-stride" class="headerlink" title="2.2 stride"></a>2.2 stride</h3><p>In general, when the stride for the height is $s_h$ and the stride for the width is $s_w$, the output shape is<br>$$<br>[(n_h-k_h+p_h+s_h)/s_h] \times [((n_w-k_w+p_w+s_w)/s_w)]<br>$$<br>f we set$p_h =k_h -1$ and $p_w = k_w -1$, then the output shape can be simplified to $[(n_h+s_h-1)/s_h\times (n_w+s_w-1)/s_w]$. Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be $(n_h/s_h)\times (n_w/s_w)$</p><p>note: the [] means floor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><h3 id="2-3-demo"><a href="#2-3-demo" class="headerlink" title="2.3 demo"></a>2.3 demo</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># With square kernels and equal stride</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding and dilation</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>), dilation=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><h2 id="3-Multiple-Input-and-Multiple-Output-Channels"><a href="#3-Multiple-Input-and-Multiple-Output-Channels" class="headerlink" title="3 Multiple Input and Multiple Output Channels"></a>3 Multiple Input and Multiple Output Channels</h2><h3 id="3-1-Multiple-Input-Channels"><a href="#3-1-Multiple-Input-Channels" class="headerlink" title="3.1 Multiple Input Channels"></a>3.1 Multiple Input Channels</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112057860.png" alt="image-20230411205730182"></p><h3 id="3-2-Multiple-Output-Channels"><a href="#3-2-Multiple-Output-Channels" class="headerlink" title="3.2  Multiple Output Channels"></a>3.2  Multiple Output Channels</h3><p>we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater <em>channel depth</em>. Intuitively, you could think of each channel as responding to a different set of features. </p><p>把每个输出通道都看做一个单独的操作，最后stack起来得到结果</p><h2 id="4-矩阵运算实现卷积操作"><a href="#4-矩阵运算实现卷积操作" class="headerlink" title="4 矩阵运算实现卷积操作"></a>4 矩阵运算实现卷积操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 卷积的输入特征图</span></span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># 卷积核</span></span><br><span class="line">bias = torch.randn(<span class="number">1</span>) <span class="comment"># 卷积偏置，默认输出通道数目等于1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 用原始的矩阵运算来实现二维卷积,先不考虑 batchsize 维度和 channel 维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d</span>(<span class="params"><span class="built_in">input</span>,kernel,bias = <span class="number">0</span>,stride = <span class="number">1</span>,padding = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding))</span><br><span class="line">        </span><br><span class="line">    input_h,input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros((output_h,output_w))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">            region = <span class="built_in">input</span>[i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">            output[<span class="built_in">int</span>(i/stride)][<span class="built_in">int</span>(j/stride)] = torch.<span class="built_in">sum</span>(region * kernel) + bias <span class="comment"># 点乘，并赋值给输出位置的元素</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">mat_mul_conv_output = matrix_multiplication_for_conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(mat_mul_conv_output)</span><br><span class="line">pytorch_api_conv_output = F.conv2d(<span class="built_in">input</span>.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="built_in">input</span>.shape[<span class="number">0</span>],<span class="built_in">input</span>.shape[<span class="number">1</span>]),kernel.reshape(<span class="number">1</span>,<span class="number">1</span>,kernel.shape[<span class="number">0</span>],kernel.shape[<span class="number">1</span>]),bias=bias,padding=<span class="number">1</span>).squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(pytorch_api_conv_output)</span><br></pre></td></tr></table></figure><h2 id="5-向量内积实现卷积操作"><a href="#5-向量内积实现卷积操作" class="headerlink" title="5 向量内积实现卷积操作"></a>5 向量内积实现卷积操作</h2><h3 id="5-1-flatten"><a href="#5-1-flatten" class="headerlink" title="5.1 flatten"></a>5.1 flatten</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step2 用原始的矩阵运算来实现二维卷积,先不考虑 batchsize 维度和 channel 维度, flatten 版本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_flatten</span>(<span class="params"><span class="built_in">input</span>,kernel,bias = <span class="number">0</span>,stride = <span class="number">1</span>,padding = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding))</span><br><span class="line">        </span><br><span class="line">    input_h,input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros((output_h,output_w))</span><br><span class="line">    region_matrix = torch.zeros(output.numel(),kernel.numel()) <span class="comment"># 存储所有的拉平后的特征区域</span></span><br><span class="line">    kernel_matrix = kernel.reshape((kernel.numel(),<span class="number">1</span>)) <span class="comment"># 变为列向量,kernel的列向量形式</span></span><br><span class="line">    row_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">            region = <span class="built_in">input</span>[i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">            region_vetor = torch.flatten(region) </span><br><span class="line">            region_matrix[row_index] = region_vetor</span><br><span class="line">            row_index += <span class="number">1</span></span><br><span class="line">    output_matrix = region_matrix @ kernel_matrix</span><br><span class="line">    output = output_matrix.reshape((output_h,output_w)) + bias</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># flatten input</span></span><br><span class="line">pytorch_api_conv_output = F.conv2d(<span class="built_in">input</span>.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="built_in">input</span>.shape[<span class="number">0</span>],<span class="built_in">input</span>.shape[<span class="number">1</span>]),</span><br><span class="line">                                   kernel.reshape(<span class="number">1</span>,<span class="number">1</span>,kernel.shape[<span class="number">0</span>],kernel.shape[<span class="number">1</span>]),</span><br><span class="line">                                   bias=bias,padding=<span class="number">1</span>).squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line">mat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_api_conv_output,mat_mul_conv_output_flatten))</span><br></pre></td></tr></table></figure><p>这里可以使用torch.unfold实现flatten操作</p><p>torch.nn.Unfold(<em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em>)</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html">https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html</a></p><h3 id="5-2-考虑batchsize维度和channel维度"><a href="#5-2-考虑batchsize维度和channel维度" class="headerlink" title="5.2 考虑batchsize维度和channel维度"></a>5.2 考虑batchsize维度和channel维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 用原始的矩阵运算来实现二维卷积，考虑batchsize维度和channel维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_full</span>(<span class="params"><span class="built_in">input</span>,kernel,bias=<span class="number">0</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span></span>):</span><br><span class="line">    <span class="comment"># input和kernel 都是4维张量 </span></span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)) <span class="comment"># w,h,input_channel,batchsize</span></span><br><span class="line">        </span><br><span class="line">    bs,in_channel,input_h,input_w = <span class="built_in">input</span>.shape <span class="comment"># batchsize,in_channel,input_h,input_w</span></span><br><span class="line">    out_channel,in_channel,kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        bias = torch.zeros(out_channel)</span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros(bs,out_channel,output_h,output_w)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(bs):</span><br><span class="line">        <span class="keyword">for</span> oc <span class="keyword">in</span> <span class="built_in">range</span>(out_channel):</span><br><span class="line">            <span class="keyword">for</span> ic <span class="keyword">in</span> <span class="built_in">range</span>(in_channel):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">                        region = <span class="built_in">input</span>[ind,ic,i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">                        output[ind,oc,<span class="built_in">int</span>(i/stride),<span class="built_in">int</span>(j/stride)] += torch.<span class="built_in">sum</span>(region * kernel[oc,ic]) <span class="comment"># 点乘，并赋值给输出位置的元素</span></span><br><span class="line">            output[ind,oc] += bias[oc]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 卷积的输入特征图(batchsize,in_channel,in_h,in_w)</span></span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># 卷积核(out_channel,in_channel,kernel_h,kernel_w)</span></span><br><span class="line">bias = torch.randn(<span class="number">3</span>) <span class="comment"># 卷积偏置，默认输出通道数目等于1</span></span><br><span class="line">pytorch_conv2d_api_output = F.conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line">mm_conv2d_full_output = matrix_multiplication_for_conv2d_full(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_conv2d_api_output,mm_conv2d_full_output))</span><br></pre></td></tr></table></figure><h2 id="6-转置卷积"><a href="#6-转置卷积" class="headerlink" title="6 转置卷积"></a>6 转置卷积</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积</span></span><br><span class="line"><span class="comment"># 把input 和 kernel 都resize 列向量(把kernel空缺的位置用0填充) 不考虑padding,假设stride=1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_kernel_matrix</span>(<span class="params">kernel,input_size,stride=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠后的矩阵&quot;&quot;&quot;</span></span><br><span class="line">    kernel_h, kernel_w = kernel.shape</span><br><span class="line">    input_h,input_w = input_size</span><br><span class="line">    num_out_feature_map = (math.floor((input_h - kernel_h)/stride) + <span class="number">1</span>) * (math.floor((input_w - kernel_w)/stride) + <span class="number">1</span>)</span><br><span class="line">    result = torch.zeros((num_out_feature_map,input_h*input_w)) <span class="comment"># 初始化结果矩阵，输出特征图元素个数*输入特征图元素个数</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h-kernel_h+<span class="number">1</span>,stride):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w-kernel_w+<span class="number">1</span>,stride):</span><br><span class="line">            padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j),) <span class="comment"># 上下左右</span></span><br><span class="line">            result[count] = padded_kernel.flatten()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">kernel_matrix = get_kernel_matrix(kernel,<span class="built_in">input</span>.shape) <span class="comment"># 4*16</span></span><br><span class="line"><span class="built_in">print</span>(kernel)</span><br><span class="line"><span class="built_in">print</span>(kernel_matrix)</span><br></pre></td></tr></table></figure><h3 id="6-1-验证二维卷积"><a href="#6-1-验证二维卷积" class="headerlink" title="6.1 验证二维卷积"></a>6.1 验证二维卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试1：验证二维卷积</span></span><br><span class="line">pytorch_conv2d_output = F.conv2d(<span class="built_in">input</span>.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),kernel.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)) <span class="comment"># output 2*2</span></span><br><span class="line"><span class="comment"># 因为计算得到的 mm_conv2d_output 是列向量，所以需要reshape为大小一致的矩阵，再进行比较</span></span><br><span class="line">mm_conv2d_output = (kernel_matrix @ <span class="built_in">input</span>.reshape((-<span class="number">1</span>,<span class="number">1</span>))).reshape(pytorch_conv2d_output.shape).transpose(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 通过矩阵乘积来计算卷积</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(mm_conv2d_output,pytorch_conv2d_output))</span><br></pre></td></tr></table></figure><h3 id="6-2-验证二维转置卷积"><a href="#6-2-验证二维转置卷积" class="headerlink" title="6.2 验证二维转置卷积"></a>6.2 验证二维转置卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试2：验证二维转置卷积</span></span><br><span class="line"><span class="comment"># 2*2 -&gt; 4*4 一般用于上采样过程 output 的 feature map 恢复到输入的 feature map 的 size</span></span><br><span class="line"><span class="comment"># 卷积的梯度,后项传播实现 √</span></span><br><span class="line"><span class="comment"># 使用填充的方式实现 ×</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>torch.nn.ConvTranspose2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em>, <em>padding_mode=’zeros’</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d">https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d</a></p><p>torch.nn.functional.conv_transpose2d(<em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em>)</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html">https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html</a></p><h2 id="7-group"><a href="#7-group" class="headerlink" title="7 group"></a>7 group</h2><ul><li><strong>groups</strong> (int,optional) – Number of blocked connections from input channels to output channels. Default: 1</li></ul><h2 id="8-dilation"><a href="#8-dilation" class="headerlink" title="8 dilation"></a>8 dilation</h2><ul><li><strong>dilation</strong> (int or tuple,optional) – Spacing between kernel elements. Default: 1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">7</span>,<span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">3</span>])     <span class="comment"># dilation=1</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">5</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># dilation=2</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">7</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">7</span>:<span class="number">3</span>]) <span class="comment"># dilation=3</span></span><br></pre></td></tr></table></figure><h2 id="9-最终版本"><a href="#9-最终版本" class="headerlink" title="9 最终版本"></a>9 最终版本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_final</span>(<span class="params"><span class="built_in">input</span>,kernel,bias=<span class="literal">None</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>,dilation=<span class="number">1</span>,groups=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    bs, in_channel, input_h, input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    out_channel,_, kernel_h, kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> out_channel % groups == <span class="number">0</span> <span class="keyword">and</span> in_channel % groups == <span class="number">0</span>, <span class="string">&quot;group必须要同时被输入通道数和输出通道数整除！&quot;</span></span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.reshape((bs, groups, in_channel//groups, input_h, input_w))</span><br><span class="line">    kernel = kernel.reshape((groups, out_channel//groups, in_channel//groups, kernel_h, kernel_w))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># dilation (往原来的kernel中加入空洞)</span></span><br><span class="line">    kernel_h = (kernel_h - <span class="number">1</span>) * (dilation - <span class="number">1</span>) + kernel_h</span><br><span class="line">    kernel_w = (kernel_w - <span class="number">1</span>) * (dilation - <span class="number">1</span>) + kernel_w    </span><br><span class="line">    <span class="comment"># 输出结果的 feature map</span></span><br><span class="line">    output_h = math.floor((input_h - kernel_h) / stride) + <span class="number">1</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w) / stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    output_shape = (bs,groups,out_channel//groups,output_h,output_w)</span><br><span class="line">    output = torch.zeros(output_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        bias = torch.zeros(out_channel)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(bs):                                                                           <span class="comment"># 对 batchsize进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> <span class="built_in">range</span>(groups):                                                                     <span class="comment"># 对群组进行遍历</span></span><br><span class="line">            <span class="keyword">for</span> oc <span class="keyword">in</span> <span class="built_in">range</span>(out_channel // groups):                                                 <span class="comment"># 对分组后的输出通道进行遍历</span></span><br><span class="line">                <span class="keyword">for</span> ic <span class="keyword">in</span> <span class="built_in">range</span>(in_channel // groups):                                              <span class="comment"># 对分组厚的输入通道进行遍历</span></span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h-kernel_h+<span class="number">1</span>,stride):                                    <span class="comment"># 对kernel高度遍历</span></span><br><span class="line">                        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w-kernel_w+<span class="number">1</span>,stride):                                <span class="comment"># 对kernel宽度遍历</span></span><br><span class="line">                            region = <span class="built_in">input</span>[ind, g, ic, i:i+kernel_h:dilation,j:j+kernel_w:dilation]   <span class="comment"># 特征区域</span></span><br><span class="line">                            output[ind,g,oc,<span class="built_in">int</span>(i/stride),<span class="built_in">int</span>(j/stride)] += torch.<span class="built_in">sum</span>(region * kernel[g,oc,ic])</span><br><span class="line">                output[ind,g,oc] += bias[g*(out_channel//groups)+oc]                                <span class="comment"># 考虑偏置</span></span><br><span class="line">    output = output.reshape((bs,out_channel,output_h,output_w))                                     <span class="comment"># 还原成4维</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">bs, in_channel, input_h, input_w = <span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span></span><br><span class="line">out_channel = <span class="number">4</span></span><br><span class="line">groups, dilation, stride, padding = <span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs, in_channel, input_h, input_w)</span><br><span class="line">kernel = torch.randn(out_channel,in_channel//groups,kernel_size,kernel_size)</span><br><span class="line">bias = torch.randn(out_channel)</span><br><span class="line"></span><br><span class="line">pytorch_conv2d_api_output = F.conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=padding,stride=stride,dilation=dilation,groups=groups)</span><br><span class="line">mm_conv2d_final_output = matrix_multiplication_for_conv2d_final(<span class="built_in">input</span>,kernel,bias=bias,stride=stride,padding=padding,dilation=dilation,groups=groups)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_conv2d_api_output,mm_conv2d_final_output))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101930110.png&quot; alt=&quot;image-20230410193006020&quot; style=&quot;zoom:80%;</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识9-transformer</title>
    <link href="http://example.com/post/c9ccea0.html"/>
    <id>http://example.com/post/c9ccea0.html</id>
    <published>2023-04-08T12:33:48.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p><p><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p><h3 id="attention-is-all-you-need-—-gt-transformer"><a href="#attention-is-all-you-need-—-gt-transformer" class="headerlink" title="attention is all you need —&gt; transformer"></a>attention is all you need —&gt; transformer</h3><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725197.png" alt="image-20230408205122663" style="zoom: 80%;" /><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725367.png" alt="image-20230408205258220" style="zoom:50%;" /><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082110329.jpg" alt="1"></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082209301.png" alt="image-20230408220929314" style="zoom:80%;" /><h3 id="1-pytorch-源码"><a href="#1-pytorch-源码" class="headerlink" title="1 pytorch 源码"></a>1 pytorch 源码</h3><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer">https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span><br><span class="line">        &gt;&gt;&gt; src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">             num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">             activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">             custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">             device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">    <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.encoder = custom_encoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.decoder = custom_decoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">    self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.nhead = nhead</span><br><span class="line"></span><br><span class="line">    self.batch_first = batch_first</span><br></pre></td></tr></table></figure><p>其中初始化部分最为重要的四部分：TransformerEncoderLayer（通过encoder_layer连接），TransformerDecoderLayer（通过decoder_layer连接）</p><h4 id="1-1-TransformerEncoderLayer"><a href="#1-1-TransformerEncoderLayer" class="headerlink" title="1.1 TransformerEncoderLayer"></a>1.1 TransformerEncoderLayer</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Args:</span><br><span class="line">    d_model: the number of expected features <span class="keyword">in</span> the input (required).</span><br><span class="line">    nhead: the number of heads <span class="keyword">in</span> the multiheadattention models (required).</span><br><span class="line">    dim_feedforward: the dimension of the feedforward network model (default=2048).</span><br><span class="line">    dropout: the dropout value (default=0.1).</span><br><span class="line">    activation: the activation <span class="keyword">function</span> of the intermediate layer, can be a string</span><br><span class="line">        (<span class="string">&quot;relu&quot;</span> or <span class="string">&quot;gelu&quot;</span>) or a unary callable. Default: relu</span><br><span class="line">    layer_norm_eps: the eps value <span class="keyword">in</span> layer normalization components (default=1e-5).</span><br><span class="line">    batch_first: If ``True``, <span class="keyword">then</span> the input and output tensors are provided</span><br><span class="line">        as (batch, <span class="built_in">seq</span>, feature). Default: ``False`` (<span class="built_in">seq</span>, batch, feature).</span><br><span class="line">    norm_first: <span class="keyword">if</span> ``True``, layer norm is <span class="keyword">done</span> prior to attention and feedforward</span><br><span class="line">        operations, respectively. Otherwise it<span class="string">&#x27;s done after. Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Examples::</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">              activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">              layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">              device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">     factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">     <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">     self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                         **factory_kwargs)</span><br><span class="line">     <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">     self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">     self.dropout = Dropout(dropout)</span><br><span class="line">     self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">     self.norm_first = norm_first</span><br><span class="line">     self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.dropout1 = Dropout(dropout)</span><br><span class="line">     self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">         activation = _get_activation_fn(activation)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># We can&#x27;t test self.activation in forward() in TorchScript,</span></span><br><span class="line">     <span class="comment"># so stash some information about it instead.</span></span><br><span class="line">     <span class="keyword">if</span> activation <span class="keyword">is</span> F.relu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.ReLU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">1</span></span><br><span class="line">     <span class="keyword">elif</span> activation <span class="keyword">is</span> F.gelu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.GELU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">2</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">0</span></span><br><span class="line">     self.activation = activation</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">x = src</span><br><span class="line">     <span class="keyword">if</span> self.norm_first:</span><br><span class="line">         x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">         x = x + self._ff_block(self.norm2(x))</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">         x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> x</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h4 id="1-2-TransformerEncoder"><a href="#1-2-TransformerEncoder" class="headerlink" title="1.2 TransformerEncoder"></a>1.2 TransformerEncoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers. Users can build the</span></span><br><span class="line"><span class="string">    BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        encoder_layer: an instance of the TransformerEncoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-encoder-layers in the encoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string">        enable_nested_tensor: if True, input will automatically convert to nested tensor</span></span><br><span class="line"><span class="string">            (and convert back on output). This will improve the overall performance of</span></span><br><span class="line"><span class="string">            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span>, enable_nested_tensor=<span class="literal">True</span>, mask_check=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">        self.enable_nested_tensor = enable_nested_tensor</span><br><span class="line">        self.mask_check = mask_check</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="1-3-TransformerDecoderLayer"><a href="#1-3-TransformerDecoderLayer" class="headerlink" title="1.3 TransformerDecoderLayer"></a>1.3 TransformerDecoderLayer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of the intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, layer norm is done prior to self attention, multihead</span></span><br><span class="line"><span class="string">            attention and feedforward operations, respectively. Otherwise it&#x27;s done after.</span></span><br><span class="line"><span class="string">            Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">        memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">        tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">        memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">        tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">        memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        see the docs in Transformer class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">    x = tgt</span><br><span class="line">    <span class="keyword">if</span> self.norm_first:</span><br><span class="line">        x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">        x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">        x = x + self._ff_block(self.norm3(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">        x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">        x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="1-4-TransformerDecoder"><a href="#1-4-TransformerDecoder" class="headerlink" title="1.4 TransformerDecoder"></a>1.4 TransformerDecoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        decoder_layer: an instance of the TransformerDecoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-decoder-layers in the decoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><em>Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.</em></p><p><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png" alt="png"></p><p>在进行解码过程中，第一个词的预测只与第一个词有关，因此最后的的attention机制是个上三角的形式，如上图所示。</p><h4 id="1-5-Attention"><a href="#1-5-Attention" class="headerlink" title="1.5 Attention"></a>1.5 Attention</h4><p><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png" alt="png"></p><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.</p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as:<br>$$<br>\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304100940797.png" alt="img"></p><h3 id="2-Encoder-细节"><a href="#2-Encoder-细节" class="headerlink" title="2 Encoder 细节"></a>2 Encoder 细节</h3><h4 id="2-1-word-embedding"><a href="#2-1-word-embedding" class="headerlink" title="2.1 word embedding"></a>2.1 word embedding</h4><p>考虑 source sentence 和 target sentence 构建序列，序列的字符以其词表中的索引的形式表示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source sentence 和 target sentence 的初始长度</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 单词表大小</span></span><br><span class="line">max_num_src_words = <span class="number">8</span></span><br><span class="line">max_num_tgt_words = <span class="number">8</span></span><br><span class="line">model_dim = <span class="number">8</span> <span class="comment"># 特征大小，原文是512</span></span><br><span class="line"><span class="comment"># 序列最大长度</span></span><br><span class="line">max_src_seq_len = <span class="number">5</span></span><br><span class="line">max_tgt_seq_len = <span class="number">5</span></span><br><span class="line">max_position_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># src_len = torch.randint(2,5,(batch_size,))</span></span><br><span class="line"><span class="comment"># tgt_len = torch.randint(2,5,(batch_size,)) </span></span><br><span class="line">src_len = torch.Tensor([<span class="number">2</span>,<span class="number">4</span>]).to(torch.int32)  <span class="comment"># 句子长度（2个句子）</span></span><br><span class="line">tgt_len = torch.Tensor([<span class="number">4</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line"><span class="built_in">print</span>(src_len,tgt_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 单词索引构成的源句子和目标句子,pad为最大序列长度,unsqueeze 变为2维张量，然后使用cat拼接起来,padding 默认值为0,构建batch</span></span><br><span class="line">src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_src_words,(L,)),(<span class="number">0</span>,max_src_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]) </span><br><span class="line">tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_tgt_words,(L,)),(<span class="number">0</span>,max_tgt_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_seq,<span class="string">&quot;\n&quot;</span>,tgt_seq,end=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2 构造 word embedding</span></span><br><span class="line"><span class="comment"># 第0行是默认padding的0，第1-9行是每个单词的embedding结果</span></span><br><span class="line">src_embedding_table = nn.Embedding(max_num_src_words + <span class="number">1</span>, model_dim)</span><br><span class="line">tgt_embedding_table = nn.Embedding(max_num_tgt_words + <span class="number">1</span>, model_dim)</span><br><span class="line">src_embedding = src_embedding_table(src_seq) <span class="comment"># embedding 的 forward 方法</span></span><br><span class="line">stgt_embedding = tgt_embedding_table(tgt_seq) </span><br><span class="line"><span class="built_in">print</span>(src_embedding_table.weight)</span><br><span class="line"><span class="built_in">print</span>(src_seq)</span><br><span class="line"><span class="built_in">print</span>(src_embedding)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-2-position-embedding"><a href="#2-2-position-embedding" class="headerlink" title="2.2 position embedding"></a>2.2 position embedding</h4><p>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp; =\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \<br>P E_{(p o s, 2 i+1)} &amp; =\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)<br>\end{aligned}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 构建 position embedding</span></span><br><span class="line">pos_mat = torch.arange(max_position_len).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">i_mat = torch.<span class="built_in">pow</span>(<span class="number">10000</span>,torch.arange(<span class="number">0</span>,model_dim,<span class="number">2</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)/model_dim)</span><br><span class="line">pe_embedding_table = torch.zeros(max_position_len,model_dim)</span><br><span class="line"><span class="comment"># element point</span></span><br><span class="line">pe_embedding_table[:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos_mat/i_mat)  <span class="comment"># 偶数行</span></span><br><span class="line">pe_embedding_table[:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos_mat/i_mat)  <span class="comment"># 奇数行</span></span><br><span class="line"><span class="built_in">print</span>(pos_mat,<span class="string">&#x27;\n&#x27;</span>,i_mat,<span class="string">&#x27;\n&#x27;</span>,pe_embedding_table)</span><br><span class="line"></span><br><span class="line">src_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(src_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> src_len]).to(torch.int32)</span><br><span class="line">tgt_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(tgt_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> tgt_len]).to(torch.int32)</span><br><span class="line"></span><br><span class="line">src_pe_embedding = pe_embedding(src_pos)</span><br><span class="line">tgt_pe_embedding = pe_embedding(tgt_pos)</span><br><span class="line"><span class="built_in">print</span>(src_pe_embedding)</span><br><span class="line"><span class="built_in">print</span>(tgt_pe_embedding)</span><br></pre></td></tr></table></figure><p>$10000^{2 i / d_{\mathrm{model}}}$ 表示为$\omega_k$，pos表示为$t$</p><p>解决 out of demain: 如果是超出序列长度，可以通过之前序列长度的线性组合来表示</p><p>For every sine-cosine pair corresponding to frequency $\omega_k$, there is a linear transformation $ M \in \R^{2 \times2} $(independent of t) where the following equation holds:</p><p>$$<br>M \cdot \begin{bmatrix}<br>  sin(\omega _k \cdot t)  \<br>  cos(\omega _k \cdot t)  \  </p><p>\end{bmatrix} = \begin{bmatrix} sin(\omega _k \cdot (t+\phi))\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}<br>$$<br>proof:</p><p>Let $M$ be a $2\times2$ matrix, we want to find $u_1,v_1,u_2$ and $v_2$ so that:<br>$$<br>\begin{bmatrix} u_1 &amp;v_1 \ u_2 &amp; v_2\end{bmatrix} \cdot \begin{bmatrix}<br>  sin(\omega _k \cdot t)  \<br>  cos(\omega _k \cdot t)  \  </p><p>\end{bmatrix} = \begin{bmatrix} sin(\omega _k \cdot (t+\phi))\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}<br>$$<br>By applying the addition theorem, we can expand the right hand side as follows:<br>$$<br>\begin{bmatrix} u_1 &amp;v_1 \ u_2 &amp; v_2\end{bmatrix} \cdot \begin{bmatrix}<br>  sin(\omega _k \cdot t)  \<br>  cos(\omega _k \cdot t)  \  </p><p>\end{bmatrix} = \begin{bmatrix} sin(\omega_k \cdot t)cos(\omega_k \cdot \phi) + cos(\omega_k \cdot t)sin(\omega_k \cdot \phi)         \cos(\omega_k \cdot t)cos(\omega_k \cdot \phi)  - sin(\omega_k \cdot t)sin(\omega_k \cdot \phi)\end{bmatrix}<br>$$<br>Which result in the following two equations:<br>$$<br>\begin{array}{l}<br>u_{1} \sin \left(\omega_{k} \cdot t\right)+v_{1} \cos \left(\omega_{k} \cdot t\right)=\cos \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\sin \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right) \<br>u_{2} \sin \left(\omega_{k} \cdot t\right)+v_{2} \cos \left(\omega_{k} \cdot t\right)=-\sin \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\cos \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right)<br>\end{array}<br>$$<br>By solving above equations, we get:<br>$$<br>\begin{aligned}<br>u_{1}=\cos \left(\omega_{k} \cdot \phi\right) ， v_{1}=\sin \left(\omega_{k} \cdot \phi\right) \<br>u_{2}=-\sin \left(\omega_{k} \cdot \phi\right) ， v_{2}=\cos \left(\omega_{k} \cdot \phi\right)<br>\end{aligned}<br>$$<br>So the final transformation matrix M is:<br>$$<br>M_{\phi,k}= \begin{bmatrix} cos(\omega_k,\phi) &amp; sin(\omega_k,\phi) \- sin(\omega_k,\phi) &amp; cos(\omega_k,\phi)\end{bmatrix}<br>$$</p><h4 id="2-3-构建encoder的self-attention-mask"><a href="#2-3-构建encoder的self-attention-mask" class="headerlink" title="2.3 构建encoder的self-attention mask"></a>2.3 构建encoder的self-attention mask</h4><p>mask的shape：[batch_size,max_src_len,max_tgt_len] 值为1或-inf</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 构建encoder的self-attention mask</span></span><br><span class="line">valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(src_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]),<span class="number">2</span>) <span class="comment"># 有效长度</span></span><br><span class="line">valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效矩阵</span></span><br><span class="line">invalid_encoder_pos_matrix = <span class="number">1</span> - valid_encoder_pos_matrix</span><br><span class="line">mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.<span class="built_in">bool</span>) <span class="comment"># 变为bool</span></span><br><span class="line"><span class="built_in">print</span>(valid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,invalid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,mask_encoder_self_attention) <span class="comment">#(batchsize,maxlen after padding,_)</span></span><br><span class="line"><span class="comment"># true 需要 mask</span></span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(src_len),<span class="built_in">max</span>(src_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_score = score.masked_fill(mask_encoder_self_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_len)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="built_in">print</span>(masked_score)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br><span class="line"><span class="comment"># 无需因果的遮掩</span></span><br></pre></td></tr></table></figure><h4 id="2-4-scaled-的重要性"><a href="#2-4-scaled-的重要性" class="headerlink" title="2.4 scaled 的重要性"></a>2.4 scaled 的重要性</h4><p>$$<br>\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$</p><p>这里的softmax中为什么要除以$\sqrt{d_k}$?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># when the varience of prob is too big</span></span><br><span class="line">alpha1 = <span class="number">0.1</span></span><br><span class="line">alpha2 = <span class="number">10</span></span><br><span class="line">score = torch.randn(<span class="number">5</span>)</span><br><span class="line">prob1 = F.softmax(score*alpha1,-<span class="number">1</span>)</span><br><span class="line">prob2 = F.softmax(score*alpha2,-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_func</span>(<span class="params">score</span>):</span><br><span class="line">    <span class="keyword">return</span> F.softmax(score)</span><br><span class="line">jaco_mat1 = torch.autograd.functional.jacobian(softmax_func,score*alpha1)</span><br><span class="line">jaco_mat2 = torch.autograd.functional.jacobian(softmax_func,score*alpha2)</span><br><span class="line"><span class="comment"># jaco matrix is close to zero when the varience is too big</span></span><br><span class="line"><span class="comment"># print(score,prob1,prob2)</span></span><br><span class="line"><span class="built_in">print</span>(jaco_mat1,<span class="string">&#x27;\n&#x27;</span>,jaco_mat2)</span><br></pre></td></tr></table></figure><h3 id="3-decoder-细节"><a href="#3-decoder-细节" class="headerlink" title="3 decoder 细节"></a>3 decoder 细节</h3><h4 id="3-1-intra-attention-的-mask"><a href="#3-1-intra-attention-的-mask" class="headerlink" title="3.1 intra-attention 的 mask"></a>3.1 intra-attention 的 mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step5 构造intra-attention的mask</span></span><br><span class="line"><span class="comment"># Q @ k^T shape:(batch_size,tgt_seq_len,src_seq_len)</span></span><br><span class="line">valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len]),<span class="number">2</span>)</span><br><span class="line">valid_cross_pos_matrix = torch.bmm(valid_decoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效位置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;源序列有效位置张量：&quot;</span>,valid_encoder_pos,<span class="string">&quot;\n 目标序列有效位置张量：&quot;</span>,valid_decoder_pos,<span class="string">&quot;\n 目标序列对源头序列有效位置张量：&quot;</span>,valid_cross_pos)</span><br><span class="line"></span><br><span class="line">invalid_cross_pos_matrix = <span class="number">1</span> - valid_cross_pos_matrix</span><br><span class="line">mask_cross_attention = invalid_cross_pos_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(mask_cross_attention)</span><br><span class="line"><span class="comment"># print(valid_cross_pos)</span></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_cross_score = score.masked_fill(mask_cross_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob_cross = F.softmax(masked_cross_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(prob_cross)</span><br></pre></td></tr></table></figure><h4 id="3-2-decoder-self-attention"><a href="#3-2-decoder-self-attention" class="headerlink" title="3.2 decoder self-attention"></a>3.2 decoder self-attention</h4><p>下三角形的mask：防止因果</p><p>要把答案遮住，如果预测第四个位置，就要把第四个位置以后的所有内容都遮住。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step6 构造 decoder self-attention 的 mask</span></span><br><span class="line">valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L,<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">invalid_decoder_tri_matrix = <span class="number">1</span> - valid_decoder_tri_matrix</span><br><span class="line">invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(valid_decoder_tri_matrix,invalid_decoder_tri_matrix)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line">masked_score = score.masked_fill(invalid_decoder_tri_matrix,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(tgt_len)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br></pre></td></tr></table></figure><p>流式预测的时候，特别需要这个掩码。</p><h4 id="3-3-scaled-self-attention"><a href="#3-3-scaled-self-attention" class="headerlink" title="3.3 scaled self-attention"></a>3.3 scaled self-attention</h4><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101603265.png" alt="image-20230410160332412"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q,K,V,attn_mask</span>):</span><br><span class="line">    <span class="comment"># shape pf Q,k,V: (batch_size * num_head,seq_len,model_dim/num_head)</span></span><br><span class="line">    score = torch.bmn(Q,K.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/torch.sqrt(model_dim)</span><br><span class="line">    masked_score = score.masked_fill(attn_mask,-<span class="number">1e9</span>)</span><br><span class="line">    prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line">    context = torch.bmn(prov,V)</span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>源码：D:\0_python\anaconda\envs\pytorch\Lib\site-packages\torch\nn\functional.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multi_head_attention_forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    query: Tensor,</span></span><br><span class="line"><span class="params">    key: Tensor,</span></span><br><span class="line"><span class="params">    value: Tensor,</span></span><br><span class="line"><span class="params">    embed_dim_to_check: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    in_proj_weight: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    in_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_k: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_v: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    add_zero_attn: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">    dropout_p: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    out_proj_weight: Tensor,</span></span><br><span class="line"><span class="params">    out_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    training: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    use_separate_proj_weight: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    q_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    k_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    v_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_k: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_v: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    average_attn_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101653337.png" alt="image-20230410165300158"></p><h3 id="4-Loss-function"><a href="#4-Loss-function" class="headerlink" title="4 Loss function"></a>4 Loss function</h3><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101703643.png" alt="image-20230410170322549"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bath_size = <span class="number">2</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">vocab_size = <span class="number">4</span></span><br><span class="line">logits = torch.randn(bath_size,seq_len,vocab_size)      <span class="comment"># bath_size = 2, seq_len = 3, vocab_size = 4</span></span><br><span class="line">label = torch.randint(<span class="number">0</span>,vocab_size,(bath_size,seq_len))</span><br><span class="line">logits = logits.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">F.cross_entropy(logits,label) <span class="comment"># 六个单词的平均交叉熵</span></span><br><span class="line">F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) <span class="comment"># 返回所有单词的交叉熵</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mask</span></span><br><span class="line">tgt_len =torch.Tensor([<span class="number">2</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line">mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len)-L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">cross_entropy = F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) * mask </span><br><span class="line"><span class="built_in">print</span>(cross_entropy)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nlp.seas.harvard.edu/2018</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识8-CONV层</title>
    <link href="http://example.com/post/2d27b0da.html"/>
    <id>http://example.com/post/2d27b0da.html</id>
    <published>2023-04-08T07:26:00.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CONV2D"><a href="#CONV2D" class="headerlink" title="CONV2D"></a>CONV2D</h1><p>torch.nn.Conv2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode=’zeros’</em>, <em>device=None</em>, <em>dtype=None</em>)</p><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels in the input image</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – Size of the convolving kernel</li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – Padding added to all four sides of the input. Default: 0</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – <code>&#39;zeros&#39;</code>, <code>&#39;reflect&#39;</code>, <code>&#39;replicate&#39;</code> or <code>&#39;circular&#39;</code>. Default: <code>&#39;zeros&#39;</code></li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></li></ul><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081528114.png" alt="image-20230408152839156" style="zoom:80%;" /><h1 id="conv-residual-block-fusion"><a href="#conv-residual-block-fusion" class="headerlink" title="conv_residual_block_fusion"></a>conv_residual_block_fusion</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">in_channels = <span class="number">2</span></span><br><span class="line">out_channels = <span class="number">2</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">w = <span class="number">9</span></span><br><span class="line">h = <span class="number">9</span></span><br><span class="line"><span class="comment"># res_block = 3*3 conv + 1*1 conv + input</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>,in_channels,w,h) <span class="comment"># 输入图片大小</span></span><br><span class="line"><span class="comment"># 方法1：原生写法</span></span><br><span class="line">conv_2d = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_pointwise = nn.Conv2d(in_channels,out_channels,<span class="number">1</span>)</span><br><span class="line">result1 = conv_2d(x) + conv_2d_pointwise(x) + x</span><br><span class="line"><span class="comment"># print(result1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：算子融合</span></span><br><span class="line"><span class="comment"># 把point-wise卷积核x本身都写成3*3卷积</span></span><br><span class="line"><span class="comment"># 最终把三个卷积写成一个卷积</span></span><br><span class="line"><span class="comment"># 1*1 -&gt; 3*3</span></span><br><span class="line"><span class="comment"># 1.改造</span></span><br><span class="line">pointwise_to_conv_weight = F.pad(conv_2d_pointwise.weight,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]) <span class="comment"># 2*2*1*1 -&gt; 2*2*3*3</span></span><br><span class="line">conv_2d_for_pointwise = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_for_pointwise.weight = nn.Parameter(pointwise_to_conv_weight)</span><br><span class="line">conv_2d_for_pointwise.bias = nn.Parameter(conv_2d_pointwise.bias)</span><br><span class="line"><span class="comment"># x -&gt; 3*3</span></span><br><span class="line"><span class="comment"># 2*2*3*3</span></span><br><span class="line">zeros = torch.unsqueeze(torch.zeros(kernel_size,kernel_size),<span class="number">0</span>) <span class="comment"># 不考虑相邻通道的影响</span></span><br><span class="line">stars = torch.unsqueeze(F.pad(torch.ones(<span class="number">1</span>,<span class="number">1</span>),[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]),<span class="number">0</span>) <span class="comment"># 不考虑周围点的影响(1*1)</span></span><br><span class="line">stars_zeros = torch.unsqueeze(torch.cat([stars,zeros],<span class="number">0</span>),<span class="number">0</span>) <span class="comment"># 第一个输出通道</span></span><br><span class="line">zeros_stars = torch.unsqueeze(torch.cat([stars,zeros],<span class="number">0</span>),<span class="number">0</span>) <span class="comment"># 第二个输出通道</span></span><br><span class="line">identity_to_conv_weight = torch.cat([stars_zeros,zeros_stars],<span class="number">0</span>)  </span><br><span class="line">identity_to_conv_bias = torch.zeros([out_channels])</span><br><span class="line">conv_2d_for_identity = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_for_identity.weight = nn.Parameter(identity_to_conv_weight)</span><br><span class="line">conv_2d_for_identity.bias = nn.Parameter(identity_to_conv_bias)</span><br><span class="line"></span><br><span class="line">result2 = conv_2d(x) + conv_2d_for_pointwise(x) + conv_2d_for_identity(x)</span><br><span class="line"><span class="comment"># print(result2)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result1,result2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.融合</span></span><br><span class="line">conv_2d_for_fusion = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_for_fusion.weight = nn.Parameter(conv_2d.weight.data + conv_2d_for_pointwise.weight.data + conv_2d_for_identity.weight.data)</span><br><span class="line">conv_2d_for_fusion.bias = nn.Parameter(conv_2d.bias.data + conv_2d_for_pointwise.bias.data + conv_2d_for_identity.bias.data)</span><br><span class="line">result3 = conv_2d_for_fusion(x)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result2,result3)))</span><br></pre></td></tr></table></figure><p>算子融合能够提升速度。</p><h1 id="ConvMixer-Layer"><a href="#ConvMixer-Layer" class="headerlink" title="ConvMixer Layer"></a>ConvMixer Layer</h1><p><a href="https://arxiv.org/pdf/2201.09792.pdf">https://arxiv.org/pdf/2201.09792.pdf</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081948885.png" alt="image-20230408194824730"></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081955360.png" alt="image-20230408195505692"></p><p>空间混合(depthwise)+通道混合(pointwise)</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082032487.png" alt="image-20230408203214916"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CONV2D&quot;&gt;&lt;a href=&quot;#CONV2D&quot; class=&quot;headerlink&quot; title=&quot;CONV2D&quot;&gt;&lt;/a&gt;CONV2D&lt;/h1&gt;&lt;p&gt;torch.nn.Conv2d(&lt;em&gt;in_channels&lt;/em&gt;, &lt;em&gt;out_channels</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门7-dropout</title>
    <link href="http://example.com/post/6ae9c33a.html"/>
    <id>http://example.com/post/6ae9c33a.html</id>
    <published>2023-04-07T14:34:02.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-dropout"><a href="#1-dropout" class="headerlink" title="1 dropout"></a>1 dropout</h2><ul><li>dropout class实现</li></ul><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072236280.png" alt="image-20230407223601601" style="zoom:80%;" /><ul><li>dropout函数实现</li></ul><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072237668.png" alt="image-20230407223740635" style="zoom:80%;" /><p>training = self.training</p><p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p><p><a href="https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer</a>,</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081429465.png" alt="image-20230408142934611"></p><p><a href="https://zhuanlan.zhihu.com/p/38200980">https://zhuanlan.zhihu.com/p/38200980</a></p><h3 id="在-numpy-中实现-dropout："><a href="#在-numpy-中实现-dropout：" class="headerlink" title="在 numpy 中实现 dropout："></a>在 numpy 中实现 dropout：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># implement dropout in numpy codes</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">rate,x,w1,w2,b1,b2</span>):</span><br><span class="line">    <span class="comment"># suppose two layers</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer1.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer1 = mask1 * layer1</span><br><span class="line">    </span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer2.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer2 = mask2 * layer2</span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">rate,x,w1,b1,w2,b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    layer1 = layer1 * (<span class="number">1</span> - rate)</span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    layer2 = layer2 * (<span class="number">1</span> - rate)</span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test scale in the train</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_train</span>(<span class="params">rate,x,w1,w2,b1,b2</span>):</span><br><span class="line">    <span class="comment"># suppose two layers</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer1.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer1 = mask1 * layer1</span><br><span class="line">    layer1 = layer1 / (<span class="number">1</span> - rate)</span><br><span class="line">    </span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer2.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer2 = mask2 * layer2</span><br><span class="line">    layer2 = layer2 / (<span class="number">1</span> - rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="comment"># without the scale</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_test</span>(<span class="params">x,w1,b1,w2,b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure><h2 id="2-r-dropout"><a href="#2-r-dropout" class="headerlink" title="2 r dropout"></a>2 r dropout</h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/5a66b9200f29ac3fa0ae244cc2a51b39-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/5a66b9200f29ac3fa0ae244cc2a51b39-Paper.pdf</a></p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230408151201702.png" alt="image-20230408151201702" style="zoom:67%;" /><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081515075.png" alt="image-20230408151505168" style="zoom:80%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-dropout&quot;&gt;&lt;a href=&quot;#1-dropout&quot; class=&quot;headerlink&quot; title=&quot;1 dropout&quot;&gt;&lt;/a&gt;1 dropout&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;dropout class实现&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识1-自动微分</title>
    <link href="http://example.com/post/42120d73.html"/>
    <id>http://example.com/post/42120d73.html</id>
    <published>2023-04-07T06:57:37.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1502.05767.pdf">https://arxiv.org/pdf/1502.05767.pdf</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071556707.png" alt="image-20230407155605114"></p><p>几种的常见微分方式：</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071601783.png" alt="image-20230407160104716"></p><ul><li>符号微分：求导</li><li>数值微分：不稳定，并且不准确</li><li>自动微分：这个例子是一个前向过程，通过预设dv = 1，然后根据每一步的dv表达式求出当前，v和dv的值</li></ul><h3 id="1-forward-mode-AD"><a href="#1-forward-mode-AD" class="headerlink" title="1 forward mode AD"></a>1 forward mode AD</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071629866.png" alt="image-20230407162927134"></p><p>分为三个部分：输入节点，隐藏节点，输出节点。其中隐藏节点也可以为称为元操作</p><p>首先设置初值为2,5，初始倒数x1为1，x2为0.</p><p>使用前向微分的特点：</p><ul><li>能够在前向运算的同时，计算前向微分的值，能够计算出每个元操作的输入节点的偏导数值</li><li>但是一次只能计算一个输入节点的偏导数</li></ul><p>或者能够采用对偶数的计算方法：</p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072114545.png" alt="image-20230407211449434" style="zoom:80%;" />$$\left.\frac{d f(x)}{d x}\right|_{x=v}=\operatorname{epsilon-coefficient}(\text { dual-version }(f)(v+1 \epsilon)) \text {. }$$<h3 id="2-reverse-mode-AD"><a href="#2-reverse-mode-AD" class="headerlink" title="2 reverse mode AD"></a>2 reverse mode AD</h3><p>链式法则：</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072123618.png" alt="image-20230407212309766"></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072113372.png" alt="image-20230407211303243"></p><p>v0的倒数在这个例子中进行了梯度累加。</p><p>假设 a=f(x),b=g(a).y=h(b)<br>$$<br>\frac{d_y}{d_x} = \frac{d_h}{d_b} <em>\frac{d_b}{d_a}</em> \frac{d_a}{d_x}<br>$$<br>雅克比矩阵的维度为：|y|*|b|,|b|*|a|,|a|*|x|</p><p>分别统计计算量：</p><ul><li><p>forward mode AD：|y|*|b|*(|b|*|a|,|a|*|x|) = bax + ybx</p></li><li><p>reverse mode AD: |y|*|b|*|b|*|a|*|a|*|x| = yba + yax</p></li></ul><p>假设 a = b 比较 x 和 y 的大小：</p><ul><li>当x&gt;y，输入特征大于输出特征，reverse mode 计算量小</li><li>当x&lt;y，输入特征大小于输出特征，forward mode 计算量小</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1502.05767.pdf&quot;&gt;https://arxiv.org/pdf/1502.05767.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picgo-1259245122.cos.ap-</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门6-autograd</title>
    <link href="http://example.com/post/40d10b1c.html"/>
    <id>http://example.com/post/40d10b1c.html</id>
    <published>2023-04-06T12:25:44.000Z</published>
    <updated>2023-04-12T12:30:11.323Z</updated>
    
    <content type="html"><![CDATA[<p>训练神经网络如何使用pytorch中的自动微分</p><p><a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062036309.png" alt="image-20230406203624607"></p><h4 id="1-得到计算图"><a href="#1-得到计算图" class="headerlink" title="1 得到计算图"></a>1 得到计算图</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062042317.png" alt="image-20230406204239318" style="zoom:67%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure><p>通过对参数进行单个源操作，得到计算图，然后进一步进行反向梯度回传计算</p><h4 id="2-计算"><a href="#2-计算" class="headerlink" title="2 计算"></a>2 计算</h4><p>我们对其中设置grad为true 的变量进行梯度回传计算。使用backward函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><ul><li>我们只能对计算图中requires_grad为true的进行梯度回传计算，比如dropout，batchnorm等都不行</li><li>由于我们只能在回传计算时生成一个图，如果我们需要对一个静态图进行多次梯度回传，我们需要把 retain_graph=True </li></ul><h4 id="3-将某些计算节点取消梯度回传"><a href="#3-将某些计算节点取消梯度回传" class="headerlink" title="3 将某些计算节点取消梯度回传"></a>3 将某些计算节点取消梯度回传</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line"><span class="built_in">print</span>(z_det.requires_grad)</span><br></pre></td></tr></table></figure><ul><li>某些 frozen parameters</li><li>加速运算</li></ul><h4 id="4-grad-zero"><a href="#4-grad-zero" class="headerlink" title="4 grad.zero_()"></a>4 grad.zero_()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inp = torch.eye(<span class="number">4</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">out = (inp+<span class="number">1</span>).<span class="built_in">pow</span>(<span class="number">2</span>).t()</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First call\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSecond call\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br><span class="line">inp.grad.zero_()</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nCall after zeroing gradients\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>由于梯度在每次调用的时候都会累加(<strong>Jacobian Product</strong>),我们需要使用grad.zero_()这样的梯度才是正确的。</p><h4 id="5-jacobian在pytorch中的实现"><a href="#5-jacobian在pytorch中的实现" class="headerlink" title="5 jacobian在pytorch中的实现"></a>5 jacobian在pytorch中的实现</h4><p>torch.autograd.functional.jacobian(<em>func</em>, <em>inputs</em>, <em>create_graph=False</em>, <em>strict=False</em>, <em>vectorize=False</em>, <em>strategy=’reverse-mode’</em>)</p><p><a href="https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian">https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062118841.png" alt="image-20230406211810400"></p><p>这个求偏导表示：生成的sum第一个数与原张量的第二行没有关系，所以求偏导也是0</p><h4 id="6-向量对向量的微分"><a href="#6-向量对向量的微分" class="headerlink" title="6 向量对向量的微分"></a>6 向量对向量的微分</h4><p>首先当其中一个向量是列向量时候：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> a + x</span><br><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">torch.autograd.functional.jacobian(func,x)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br></pre></td></tr></table></figure><p>显然这里的结果表示，f = a + x 中 ，f1,2,3 只与 x1,2,3 有关，所以是个对角阵</p><p>向量对向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> a + x</span><br><span class="line">x = torch.randn(<span class="number">3</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">torch.autograd.functional.jacobian(func,x)</span><br><span class="line">y = func(x)</span><br><span class="line">y.backward(torch.ones_like(y))</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones_like(y) @ torch.autograd.functional.jacobian(func,x)</span><br></pre></td></tr></table></figure><h4 id="7-矩阵对矩阵的偏导"><a href="#7-矩阵对矩阵的偏导" class="headerlink" title="7 矩阵对矩阵的偏导"></a>7 矩阵对矩阵的偏导</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>,<span class="number">2</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = a @ b</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;训练神经网络如何使用pytorch中的自动微分&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html&quot;&gt;https://pytorch.org/tutor</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门5-container详解</title>
    <link href="http://example.com/post/f208623b.html"/>
    <id>http://example.com/post/f208623b.html</id>
    <published>2023-04-06T11:55:57.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-torch-nn-Sequential-args-Module"><a href="#1-torch-nn-Sequential-args-Module" class="headerlink" title="1 torch.nn.Sequential(args: Module)"></a>1 torch.nn.Sequential(args: <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a>)</h5><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential</a></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062008133.png" alt="image-20230406200820589" style="zoom:80%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self:</span><br><span class="line">        <span class="built_in">input</span> = module(<span class="built_in">input</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">input</span></span><br></pre></td></tr></table></figure><h5 id="2-torch-nn-ModuleList-modules-None"><a href="#2-torch-nn-ModuleList-modules-None" class="headerlink" title="2 torch.nn.ModuleList(modules=None)"></a>2 torch.nn.ModuleList(modules=None)</h5><p>(<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList</a>)</p><p>Holds submodules in a list.</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062017290.png" alt="image-20230406201713167"></p><h5 id="3-torch-nn-ModuleDict-modules-None"><a href="#3-torch-nn-ModuleDict-modules-None" class="headerlink" title="3 torch.nn.ModuleDict(modules=None)"></a>3 torch.nn.ModuleDict(modules=None)</h5><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict</a></p><p>Holds submodules in a dictionary.</p><h5 id="4-torch-nn-ParameterList-values-None"><a href="#4-torch-nn-ParameterList-values-None" class="headerlink" title="4 torch.nn.ParameterList(values=None)"></a>4 torch.nn.ParameterList(values=None)</h5><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList</a></p><p>Holds submodules in a list.</p><h5 id="5-torch-nn-ParameterDict-parameters-None"><a href="#5-torch-nn-ParameterDict-parameters-None" class="headerlink" title="5 torch.nn.ParameterDict(parameters=None)"></a>5 torch.nn.ParameterDict(<em>parameters=None</em>)</h5><p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict</a></p><p>Holds parameters in a dictionary.</p><p>其中sequential能使用forward，其他容器不能，所以一般都使用sequential这个容器</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h5 id=&quot;1-torch-nn-Sequential-args-Module&quot;&gt;&lt;a href=&quot;#1-torch-nn-Sequential-args-Module&quot; class=&quot;headerlink&quot; title=&quot;1 torch.nn.Sequential(args</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门4-module详解</title>
    <link href="http://example.com/post/565d0c01.html"/>
    <id>http://example.com/post/565d0c01.html</id>
    <published>2023-04-06T08:27:23.000Z</published>
    <updated>2023-04-12T12:30:11.323Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了module类中的一些方法</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module">https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module</a></p><p>路径位置如下：</p><p>D:\0_python\anaconda\envs\pytorch\Lib\site-packages\torch\nn\modules\module.py</p><h4 id="1-register-buffer"><a href="#1-register-buffer" class="headerlink" title="1 register_buffer"></a>1 register_buffer</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061705956.png" alt="image-20230406170548915" style="zoom:80%;" /><p>让当前模块中添加buffer参数，通过persistent让这个buffer是否一直存在</p><h4 id="2-register-parameter"><a href="#2-register-parameter" class="headerlink" title="2 register_parameter"></a>2 register_parameter</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061708295.png" alt="image-20230406170826936" style="zoom:80%;" /><h4 id="3-get-parameter"><a href="#3-get-parameter" class="headerlink" title="3 get_parameter"></a>3 get_parameter</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061720617.png" alt="image-20230406172007173" style="zoom:80%;" /><p>需要把嵌套层都写好，不能只写一层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_parameter</span>(<span class="params">self, target: <span class="built_in">str</span></span>) -&gt; <span class="string">&quot;Parameter&quot;</span>:</span><br><span class="line">    module_path, _, param_name = target.rpartition(<span class="string">&quot;.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    mod: torch.nn.Module = self.get_submodule(module_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(mod, param_name):</span><br><span class="line">        <span class="keyword">raise</span> AttributeError(mod._get_name() + <span class="string">&quot; has no attribute `&quot;</span></span><br><span class="line">                             + param_name + <span class="string">&quot;`&quot;</span>)</span><br><span class="line"></span><br><span class="line">    param: torch.nn.Parameter = <span class="built_in">getattr</span>(mod, param_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(param, torch.nn.Parameter):</span><br><span class="line">        <span class="keyword">raise</span> AttributeError(<span class="string">&quot;`&quot;</span> + param_name + <span class="string">&quot;` is not an &quot;</span></span><br><span class="line">                             <span class="string">&quot;nn.Parameter&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> param</span><br></pre></td></tr></table></figure><p>其中<code>target.rpartition</code>的作用如下：</p><p>Search for the last occurrence of the word “x”, and return a tuple with three elements:</p><p>1 - everything before the “match”<br>2 - the “match”<br>3 - everything after the “match”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">txt = <span class="string">&quot;I could eat bananas all day, bananas are my favorite fruit&quot;</span></span><br><span class="line"></span><br><span class="line">x = txt.rpartition(<span class="string">&quot;bananas&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>首先判断是否在module中，如果没有这个parameter_name就抛出异常</p><p>然后校验，是否这个parameter在字典中，不存在则抛出异常</p><h4 id="4-get-buffer"><a href="#4-get-buffer" class="headerlink" title="4 get_buffer"></a>4 get_buffer</h4><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230406172527868.png" alt="image-20230406172527868" style="zoom:80%;" /><p>同理与get_parameter相似</p><h4 id="5-如何进行断点训练"><a href="#5-如何进行断点训练" class="headerlink" title="5 如何进行断点训练"></a>5 如何进行断点训练</h4><p><a href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html</a></p><p><a href="https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html">https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html</a></p><h4 id="6-to"><a href="#6-to" class="headerlink" title="6 to"></a>6 to</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061818735.png" alt="image-20230406181821316" style="zoom:80%;" /><p>example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Test(torch.nn.Module):</span><br><span class="line">def __init__(self):</span><br><span class="line">super(Test, self).__init__()</span><br><span class="line">self.linear1 = torch.nn.Linear(2,3)</span><br><span class="line">self.linear2 = torch.nn.Linear(3,4)</span><br><span class="line">self.batch_norm = torch.nn.BatchNorm2d(4)</span><br><span class="line">test_module = Test()</span><br></pre></td></tr></table></figure><p>接下来测试to函数的用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_module.to(torch.double)</span><br><span class="line">test_module._modules[<span class="string">&#x27;linear1&#x27;</span>].weight.dtype</span><br></pre></td></tr></table></figure><p>为什么能够使用_modules（表示当前模块中的所有子模块）</p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061831991.png" alt="image-20230406183151791" style="zoom:80%;" /><p>由此发现当调用_parameters,_buffers时候，只能返回当前模块中的参数和buffers，并不能嵌套查询。</p><h4 id="7-save-to-state-dict"><a href="#7-save-to-state-dict" class="headerlink" title="7 _save_to_state_dict"></a>7 _save_to_state_dict</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061843934.png" alt="image-20230406184301390" style="zoom:80%;" /><p>将当前module中的name，param，buff都循环保存至destination中</p><h4 id="8-state-dict"><a href="#8-state-dict" class="headerlink" title="8 state_dict"></a>8 state_dict</h4><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061845557.png" alt="image-20230406184514473"></p><p>这个时候能够实现递归，保存当前模块，并且能够保存所有子模块。</p><h4 id="9-named-members"><a href="#9-named-members" class="headerlink" title="9 _named_members"></a>9 _named_members</h4><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061923535.png" alt="image-20230406192319875"></p><p>比较通用的一个查询函数：返回迭代器</p><h4 id="10-train"><a href="#10-train" class="headerlink" title="10 train"></a>10 train</h4><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061942823.png" alt="image-20230406194218024" style="zoom:80%;" /><p>dropout batchnorm中会受到影响。</p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061944676.png" alt="image-20230406194427357" style="zoom:80%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;介绍了module类中的一些方法&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module&quot;&gt;https:/</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实用工具1-网络可视化</title>
    <link href="http://example.com/post/b187ba6d.html"/>
    <id>http://example.com/post/b187ba6d.html</id>
    <published>2023-04-06T03:46:15.000Z</published>
    <updated>2023-04-16T15:20:10.503Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-torchsummary"><a href="#1-torchsummary" class="headerlink" title="1 torchsummary"></a>1 torchsummary</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16  <span class="comment"># 以 vgg16 为例</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">myNet = vgg16()  <span class="comment"># 实例化网络，可以换成自己的网络</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">myNet.to(device)</span><br><span class="line">summary(myNet, (<span class="number">1</span>,<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>))  <span class="comment"># 输出网络结构</span></span><br></pre></td></tr></table></figure><p>可以看出，torchsummary 不仅可以查看网络的顺序结构，还有网络参数量，网络模型大小等信息，非常实用。</p><h2 id="2-graphviz-torchviz"><a href="#2-graphviz-torchviz" class="headerlink" title="2 graphviz + torchviz"></a>2 graphviz + torchviz</h2><p>首先下载graphviz，并将其加入到环境变量中</p><p><a href="https://www2.graphviz.org/Packages/stable/windows/10/cmake/Release/x64/">https://www2.graphviz.org/Packages/stable/windows/10/cmake/Release/x64/</a></p><p><img src="https://img-blog.csdnimg.cn/20210706174457451.png#pic_center" alt="img"></p><p>接下来安装工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install alubbock pygraphviz</span><br><span class="line">pip install torchviz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchviz <span class="keyword">import</span> make_dot</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16  <span class="comment"># 以 vgg16 为例</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)  <span class="comment"># 随机生成一个张量</span></span><br><span class="line">model = vgg16()  <span class="comment"># 实例化 vgg16，网络可以改成自己的网络</span></span><br><span class="line">out = model(x)   <span class="comment"># 将 x 输入网络</span></span><br><span class="line">g = make_dot(out)  <span class="comment"># 实例化 make_dot</span></span><br><span class="line">g.view()  <span class="comment"># 直接在当前路径下保存 pdf 并打开</span></span><br><span class="line"><span class="comment"># g.render(filename=&#x27;netStructure/myNetModel&#x27;, view=False, format=&#x27;pdf&#x27;)  # 保存 pdf 到指定路径不打开</span></span><br></pre></td></tr></table></figure><h2 id="3-netron"><a href="#3-netron" class="headerlink" title="3 netron"></a>3 netron</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install netron</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 针对有网络模型，但还没有训练保存 .pth 文件的情况</span></span><br><span class="line"><span class="keyword">import</span> netron</span><br><span class="line"><span class="keyword">import</span> torch.onnx</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18  <span class="comment"># 以 resnet18 为例</span></span><br><span class="line"></span><br><span class="line">myNet = resnet18()  <span class="comment"># 实例化 resnet18</span></span><br><span class="line">x = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">40</span>, <span class="number">40</span>)  <span class="comment"># 随机生成一个输入</span></span><br><span class="line">modelData = <span class="string">&quot;./demo.pth&quot;</span>  <span class="comment"># 定义模型数据保存的路径</span></span><br><span class="line"><span class="comment"># modelData = &quot;./demo.onnx&quot;  # 有人说应该是 onnx 文件，但我尝试 pth 是可以的 </span></span><br><span class="line">torch.onnx.export(myNet, x, modelData)  <span class="comment"># 将 pytorch 模型以 onnx 格式导出并保存</span></span><br><span class="line">netron.start(modelData)  <span class="comment"># 输出网络结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  针对已经存在网络模型 .pth 文件的情况</span></span><br><span class="line"><span class="keyword">import</span> netron</span><br><span class="line"></span><br><span class="line">modelData = <span class="string">&quot;./demo.pth&quot;</span>  <span class="comment"># 定义模型数据保存的路径</span></span><br><span class="line">netron.start(modelData)  <span class="comment"># 输出网络结构</span></span><br></pre></td></tr></table></figure><h2 id="4-python-代码"><a href="#4-python-代码" class="headerlink" title="4 python 代码"></a>4 python 代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_n_params</span>(<span class="params">model</span>):</span><br><span class="line">    pp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">list</span>(model.parameters()):</span><br><span class="line">        nn = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">list</span>(p.size()):</span><br><span class="line">            nn = nn * s</span><br><span class="line">        pp += nn</span><br><span class="line">    <span class="keyword">return</span> pp</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-torchsummary&quot;&gt;&lt;a href=&quot;#1-torchsummary&quot; class=&quot;headerlink&quot; title=&quot;1 torchsummary&quot;&gt;&lt;/a&gt;1 torchsummary&lt;/h2&gt;&lt;figure class=&quot;highlight </summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
    <category term="tools" scheme="http://example.com/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门3-搭建分类网络</title>
    <link href="http://example.com/post/55849e59.html"/>
    <id>http://example.com/post/55849e59.html</id>
    <published>2023-04-06T02:45:31.000Z</published>
    <updated>2023-04-12T12:30:11.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-get-device-for-training"><a href="#1-get-device-for-training" class="headerlink" title="1 get device for training"></a>1 get device for training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">device = (</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;mps&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.backends.mps.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="2-define-the-class"><a href="#2-define-the-class" class="headerlink" title="2 define the class"></a>2 define the class</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>在这个网络的定义中最后一层的输出层是10，表示这是一个10分类的问题</p><h2 id="3-use-the-model"><a href="#3-use-the-model" class="headerlink" title="3 use the model"></a>3 use the model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>首先先将模型参数移动到设备上，这个时候初始化参数，最后得到logits。这个logits需要通过softmax各个标签的概率大小，最后得到最大概率的那个。</p><h2 id="4-Model-Layers"><a href="#4-Model-Layers" class="headerlink" title="4 Model Layers"></a>4 Model Layers</h2><p><a href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</a></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061134897.png" alt="image-20230406113408760" style="zoom: 80%;" /><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061136988.png" alt="image-20230406113633421" style="zoom:80%;" /><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304061140214.png" alt="image-20230406114025490"></p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304121931189.png" alt="image-20230406115016019" style="zoom:80%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-get-device-for-training&quot;&gt;&lt;a href=&quot;#1-get-device-for-training&quot; class=&quot;headerlink&quot; title=&quot;1 get device for training&quot;&gt;&lt;/a&gt;1 get devic</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门2-Data loader</title>
    <link href="http://example.com/post/2d21482d.html"/>
    <id>http://example.com/post/2d21482d.html</id>
    <published>2023-04-05T10:49:14.000Z</published>
    <updated>2023-04-12T12:30:11.323Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">https://pytorch.org/tutorials/beginner/basics/data_tutorial.html</a></p><h2 id="1-torch-utils-data-Dataset"><a href="#1-torch-utils-data-Dataset" class="headerlink" title="1 torch.utils.data.Dataset"></a>1 torch.utils.data.Dataset</h2><p>单个样本</p><p>A custom Dataset class must implement three functions: <strong>init</strong>, <strong>len</strong>, and <strong>getitem</strong>. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file.</p><p>In the next sections, we’ll break down what’s happening in each of these functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051911371.png" alt="image-20230405191154677"></p><h2 id="2-torch-utils-data-DataLoader"><a href="#2-torch-utils-data-DataLoader" class="headerlink" title="2 torch.utils.data.DataLoader"></a>2 torch.utils.data.DataLoader</h2><p>将样本组合起来，形成 mini-batch</p><p>The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.</p><p>DataLoader is an iterable that abstracts this complexity for us in an easy API.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051913098.png" alt="image-20230405191357204"></p><p>collate_fn:小批次，在一个batch中进一步操作</p><h2 id="3-Dataset-Types"><a href="#3-Dataset-Types" class="headerlink" title="3 Dataset Types"></a>3 Dataset Types</h2><p>The most important argument of <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a> constructor is <code>dataset</code>, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:</p><ul><li><a href="https://pytorch.org/docs/stable/data.html#map-style-datasets">map-style datasets</a>,</li><li><a href="https://pytorch.org/docs/stable/data.html#iterable-style-datasets">iterable-style datasets</a>.</li></ul><h2 id="4-Data-loader类详解"><a href="#4-Data-loader类详解" class="headerlink" title="4 Data loader类详解"></a>4 Data loader类详解</h2><p>path: D:\0_python\anaconda\envs\pytorch\Lib\site-packages\torch\utils\data</p><h3 id="4-1-class类介绍"><a href="#4-1-class类介绍" class="headerlink" title="4.1 class类介绍"></a>4.1 class类介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">dataset: Dataset[T_co]</span><br><span class="line">    batch_size: <span class="type">Optional</span>[<span class="built_in">int</span>]</span><br><span class="line">    num_workers: <span class="built_in">int</span></span><br><span class="line">    pin_memory: <span class="built_in">bool</span></span><br><span class="line">    drop_last: <span class="built_in">bool</span></span><br><span class="line">    timeout: <span class="built_in">float</span></span><br><span class="line">    sampler: <span class="type">Union</span>[Sampler, Iterable]</span><br><span class="line">    pin_memory_device: <span class="built_in">str</span></span><br><span class="line">    prefetch_factor: <span class="built_in">int</span></span><br><span class="line">    _iterator : <span class="type">Optional</span>[<span class="string">&#x27;_BaseDataLoaderIter&#x27;</span>]</span><br><span class="line">    __initialized = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset: Dataset[T_co], batch_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 shuffle: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>, sampler: <span class="type">Union</span>[Sampler, Iterable, <span class="literal">None</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 batch_sampler: <span class="type">Union</span>[Sampler[<span class="type">Sequence</span>], Iterable[<span class="type">Sequence</span>], <span class="literal">None</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 num_workers: <span class="built_in">int</span> = <span class="number">0</span>, collate_fn: <span class="type">Optional</span>[_collate_fn_t] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 pin_memory: <span class="built_in">bool</span> = <span class="literal">False</span>, drop_last: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 timeout: <span class="built_in">float</span> = <span class="number">0</span>, worker_init_fn: <span class="type">Optional</span>[_worker_init_fn_t] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 multiprocessing_context=<span class="literal">None</span>, generator=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 *, prefetch_factor: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">                 persistent_workers: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 pin_memory_device: <span class="built_in">str</span> = <span class="string">&quot;&quot;</span></span>):</span><br><span class="line">        torch._C._log_api_usage_once(<span class="string">&quot;python.data_loader&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>shuffle: 训练结束是否打乱数据集，一般设置为true</li><li>sampler：然后样本有序的进行组合成一系列的mini-batch，如果有些样本的长度很长，如果使用shuffle导致mini-batch 大小不均衡，显然式不合理的</li><li>num_workers：多进程</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> num_workers &lt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&#x27;num_workers option should be non-negative; &#x27;</span></span><br><span class="line">                     <span class="string">&#x27;use num_workers=0 to disable multiprocessing.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> num_workers == <span class="number">0</span> <span class="keyword">and</span> prefetch_factor != <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&#x27;prefetch_factor option could only be specified in multiprocessing.&#x27;</span></span><br><span class="line">                     <span class="string">&#x27;let num_workers &gt; 0 to enable multiprocessing.&#x27;</span>)</span><br><span class="line"><span class="keyword">assert</span> prefetch_factor &gt; <span class="number">0</span></span><br></pre></td></tr></table></figure><ul><li>collate_fn：对一个batch进行后处理，对batch进行pading处理</li></ul><h3 id="4-2-流程分析"><a href="#4-2-流程分析" class="headerlink" title="4.2 流程分析"></a>4.2 流程分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(dataset, IterableDataset):</span><br><span class="line">    self._dataset_kind = _DatasetKind.Iterable</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dataset, IterDataPipe):</span><br><span class="line">        <span class="keyword">if</span> shuffle <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            dataset = torch.utils.data.graph_settings.apply_shuffle_settings(dataset, shuffle=shuffle)</span><br><span class="line">    <span class="comment"># We cannot check `shuffle is not None` here, since previously `shuffle=False` was the default.</span></span><br><span class="line">    <span class="keyword">elif</span> shuffle <span class="keyword">not</span> <span class="keyword">in</span> &#123;<span class="literal">False</span>, <span class="literal">None</span>&#125;:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;DataLoader with IterableDataset: expected unspecified &quot;</span></span><br><span class="line">            <span class="string">&quot;shuffle option, but got shuffle=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(shuffle))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> sampler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># See NOTE [ Custom Samplers and IterableDataset ]</span></span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;DataLoader with IterableDataset: expected unspecified &quot;</span></span><br><span class="line">            <span class="string">&quot;sampler option, but got sampler=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(sampler))</span><br><span class="line">    <span class="keyword">elif</span> batch_sampler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># See NOTE [ Custom Samplers and IterableDataset ]</span></span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;DataLoader with IterableDataset: expected unspecified &quot;</span></span><br><span class="line">            <span class="string">&quot;batch_sampler option, but got batch_sampler=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch_sampler))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    shuffle = <span class="built_in">bool</span>(shuffle)</span><br><span class="line">    self._dataset_kind = _DatasetKind.Map</span><br></pre></td></tr></table></figure><p>首先判断数据类型是否为Map，一般如果在磁盘上进行读取，都是Map，流式读取才是 iterable-style dataset</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051943611.png" alt="image-20230405194305702"></p><p>sampler 和 shuffle 是互斥的。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051952013.png" alt="image-20230405194752124"></p><p>接下来查看 sampler 中的两个实现</p><ul><li>SequentialSampler</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SequentialSampler</span>(Sampler[<span class="built_in">int</span>]):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements sequentially, always in the same order.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_source: Sized</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source: Sized</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.data_source = data_source</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.data_source)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br></pre></td></tr></table></figure><ul><li>RandomSampler</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RandomSampler</span>(Sampler[<span class="built_in">int</span>]):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements randomly. If without replacement, then sample from a shuffled dataset.</span></span><br><span class="line"><span class="string">    If with replacement, then user can specify :attr:`num_samples` to draw.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``</span></span><br><span class="line"><span class="string">        num_samples (int): number of samples to draw, default=`len(dataset)`.</span></span><br><span class="line"><span class="string">        generator (Generator): Generator used in sampling.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_source: Sized</span><br><span class="line">    replacement: <span class="built_in">bool</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source: Sized, replacement: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 num_samples: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>, generator=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.data_source = data_source</span><br><span class="line">        self.replacement = replacement</span><br><span class="line">        self._num_samples = num_samples</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.replacement, <span class="built_in">bool</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;replacement should be a boolean value, but got &quot;</span></span><br><span class="line">                            <span class="string">&quot;replacement=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.replacement))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.num_samples, <span class="built_in">int</span>) <span class="keyword">or</span> self.num_samples &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;num_samples should be a positive integer &quot;</span></span><br><span class="line">                             <span class="string">&quot;value, but got num_samples=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.num_samples))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_samples</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># dataset size might change at runtime</span></span><br><span class="line">        <span class="keyword">if</span> self._num_samples <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br><span class="line">        <span class="keyword">return</span> self._num_samples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[<span class="built_in">int</span>]:</span><br><span class="line">        n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">        <span class="keyword">if</span> self.generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            seed = <span class="built_in">int</span>(torch.empty((), dtype=torch.int64).random_().item())</span><br><span class="line">            generator = torch.Generator()</span><br><span class="line">            generator.manual_seed(seed)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            generator = self.generator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.replacement:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_samples // <span class="number">32</span>):</span><br><span class="line">                <span class="keyword">yield</span> <span class="keyword">from</span> torch.randint(high=n, size=(<span class="number">32</span>,), dtype=torch.int64, generator=generator).tolist()</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> torch.randint(high=n, size=(self.num_samples % <span class="number">32</span>,), dtype=torch.int64, generator=generator).tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_samples // n):</span><br><span class="line">                <span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()[:self.num_samples % n]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()[:self.num_samples % n]</span><br></pre></td></tr></table></figure><p>返回一个随机的索引</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051951281.png" alt="image-20230405195157477"></p><p>接下来分析batchsampler，一般不会设置batchsampler</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchSampler</span>(Sampler[<span class="type">List</span>[<span class="built_in">int</span>]]):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Wraps another sampler to yield a mini-batch of indices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sampler (Sampler or Iterable): Base sampler. Can be any iterable object</span></span><br><span class="line"><span class="string">        batch_size (int): Size of mini-batch.</span></span><br><span class="line"><span class="string">        drop_last (bool): If ``True``, the sampler will drop the last batch if</span></span><br><span class="line"><span class="string">            its size would be less than ``batch_size``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sampler: <span class="type">Union</span>[Sampler[<span class="built_in">int</span>], Iterable[<span class="built_in">int</span>]], batch_size: <span class="built_in">int</span>, drop_last: <span class="built_in">bool</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Since collections.abc.Iterable does not check for `__getitem__`, which</span></span><br><span class="line">        <span class="comment"># is one way for an object to be an iterable, we don&#x27;t do an `isinstance`</span></span><br><span class="line">        <span class="comment"># check here.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(batch_size, <span class="built_in">int</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(batch_size, <span class="built_in">bool</span>) <span class="keyword">or</span> \</span><br><span class="line">                batch_size &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;batch_size should be a positive integer value, &quot;</span></span><br><span class="line">                             <span class="string">&quot;but got batch_size=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch_size))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(drop_last, <span class="built_in">bool</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;drop_last should be a boolean value, but got &quot;</span></span><br><span class="line">                             <span class="string">&quot;drop_last=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(drop_last))</span><br><span class="line">        self.sampler = sampler</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.drop_last = drop_last</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951</span></span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            sampler_iter = <span class="built_in">iter</span>(self.sampler)</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    batch = [<span class="built_in">next</span>(sampler_iter) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.batch_size)]</span><br><span class="line">                    <span class="keyword">yield</span> batch</span><br><span class="line">                <span class="keyword">except</span> StopIteration:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = [<span class="number">0</span>] * self.batch_size</span><br><span class="line">            idx_in_batch = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> self.sampler:</span><br><span class="line">                batch[idx_in_batch] = idx</span><br><span class="line">                idx_in_batch += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> idx_in_batch == self.batch_size:</span><br><span class="line">                    <span class="keyword">yield</span> batch</span><br><span class="line">                    idx_in_batch = <span class="number">0</span></span><br><span class="line">                    batch = [<span class="number">0</span>] * self.batch_size</span><br><span class="line">            <span class="keyword">if</span> idx_in_batch &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">yield</span> batch[:idx_in_batch]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># Can only be called if self.sampler has __len__ implemented</span></span><br><span class="line">        <span class="comment"># We cannot enforce this condition, so we turn off typechecking for the</span></span><br><span class="line">        <span class="comment"># implementation below.</span></span><br><span class="line">        <span class="comment"># Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]</span></span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.sampler) // self.batch_size  <span class="comment"># type: ignore[arg-<span class="built_in">type</span>]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="built_in">len</span>(self.sampler) + self.batch_size - <span class="number">1</span>) // self.batch_size  <span class="comment"># type: ignore[arg-<span class="built_in">type</span>]</span></span><br></pre></td></tr></table></figure><p>其中关键部分是iter方法，其中分别定义了是否有 drop_last 时对最后一个小于 batchsize 大小的样本的处理。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304052001358.png" alt="image-20230405200134569"></p><p>大部分情况下，在设置 batchsize 并且没有设置自定义的 batchsampler 时候，根据系统默认的 batchsampler，这个时候 auto_collation 为 true</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304052003605.png" alt="image-20230405200333317"></p><p>总结：</p><ul><li>构建sampler</li><li>构建batchsampler</li><li>构建collate</li></ul><h3 id="4-3-get-iterator方法分析"><a href="#4-3-get-iterator方法分析" class="headerlink" title="4.3 _get_iterator方法分析"></a>4.3 _get_iterator方法分析</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304052011332.png" alt="image-20230405201103090"></p><p>可以看到这个方法实际上调用了iter这个方法，最后的返回值是一个迭代类，这也就是为什么<code>train_features, train_labels = next(iter(train_dataloader))</code>中的数据集调用需要采用next，iter的方法。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304052011963.png" alt="image-20230405201151436"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_SingleProcessDataLoaderIter</span>(<span class="title class_ inherited__">_BaseDataLoaderIter</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loader</span>):</span><br><span class="line">        <span class="built_in">super</span>(_SingleProcessDataLoaderIter, self).__init__(loader)</span><br><span class="line">        <span class="keyword">assert</span> self._timeout == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> self._num_workers == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Adds forward compatibilities so classic DataLoader can work with DataPipes:</span></span><br><span class="line">        <span class="comment">#   Taking care of distributed sharding</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._dataset, (IterDataPipe, MapDataPipe)):</span><br><span class="line">            torch.utils.data.graph_settings.apply_sharding(self._dataset, self._world_size, self._rank)</span><br><span class="line"></span><br><span class="line">        self._dataset_fetcher = _DatasetKind.create_fetcher(</span><br><span class="line">            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)</span><br></pre></td></tr></table></figure><p>这个是具体的实现方式：_SingleProcessDataLoaderIter</p><p><code>train_features, train_labels = next(iter(train_dataloader))</code>的逻辑为：先调用这个对象的get_iterator方法，然后得到了对应的实例，这个实例中存在一个next方法</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304052020672.png" alt="image-20230405202049709"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/basics/data_tutorial.html&quot;&gt;https://pytorch.org/tutorials/beginner/basics/data_tutorial.ht</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门1-张量运算</title>
    <link href="http://example.com/post/489d1269.html"/>
    <id>http://example.com/post/489d1269.html</id>
    <published>2023-04-05T05:29:58.000Z</published>
    <updated>2023-04-12T12:30:11.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1 准备工作"></a>1 准备工作</h2><p>安装anaconda，cuda，cudnn等内容（略）</p><p>配置jupyter notebook</p><ul><li>如何再jupyter notebook中看到不同的conda环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda activate emotiona_analysis  <span class="comment"># 切换到虚拟环境emotiona_analysis </span></span><br><span class="line"></span><br><span class="line">pip install ipykernel    <span class="comment">#  在tensorflow  中安装 ipykernel 包</span></span><br><span class="line"></span><br><span class="line">python -m ipykernel install --name emotiona_analysis   <span class="comment"># 向 jupyter 中添加虚拟环境</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-tensor-操作"><a href="#2-tensor-操作" class="headerlink" title="2 tensor 操作"></a>2 tensor 操作</h2><p><a href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a> </p><p>Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described</p><ul><li><p>tensor.cat  (Concatenates the given sequence of <code>seq</code> tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.)</p></li><li><p>tensor.chunk (Attempts to split a tensor into the specified number of chunks. Each chunk is a view of the input tensor.)</p></li><li><p>torch.numel (Returns the total number of elements in the <code>input</code> tensor.)</p></li><li><p>torch.set_default_tensor_type(t) </p><p>(Sets the default <code>torch.Tensor</code> type to floating point tensor type <code>t</code>. This type will also be used as default floating point type for type inference in <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>torch.tensor()</code></a>.</p><p>The default floating point tensor type is initially <code>torch.FloatTensor</code>.)</p></li><li><p>torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0</span><br><span class="line">out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1</span><br><span class="line">out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</span><br></pre></td></tr></table></figure><ul><li>Tensor.scatter_(dim, index, src, reduce=None) → Tensor</li><li>torch.reshape</li></ul><p>eturns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape. When possible, the returned tensor will be a view of <code>input</code>. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</p><ul><li><p>torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</p></li><li><p>torch.split(tensor, split_size_or_sections, dim=0)</p></li><li><p>torch.squeeze(input, dim=None) → Tensor</p></li><li><p>torch.stack(tensors, dim=0, *, out=None) → Tensor</p></li><li><p>torch.tile(input, dims) → Tensor</p></li><li><p>torch.transpose(<em>input</em>, <em>dim0</em>, <em>dim1</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></p></li><li><p>torch.unbind(<em>input</em>, <em>dim=0</em>) → seq</p></li><li><p>torch.unsqueeze(<em>input</em>, <em>dim</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></p></li><li><p>torch.where(<em>condition</em>, <em>input</em>, <em>other</em>, ***, <em>out=None</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>  <em>masked</em></p></li><li><p>torch.rand 与 torch.randn 区别，前者是0-1均匀分布中取值，后者是0,1正态分布中取值</p></li><li><p>torch.manual_seed(seed)</p></li><li><p>torch.bernoulli(<em>input</em>, ***, <em>generator=None</em>, <em>out=None</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></p></li><li><p>torch.normal(<em>mean</em>, <em>std</em>, ***, <em>generator=None</em>, <em>out=None</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a> 返回高斯分布</p></li><li><p>torch.randperm(<em>n</em>, ***, <em>generator=None</em>, <em>out=None</em>, <em>dtype=torch.int64</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em>, <em>pin_memory=False</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a> 通常用于构建数据集的索引 随机组合</p></li></ul><h2 id="3-如何理解tensor-gather-和-tensor-tensor-scatter"><a href="#3-如何理解tensor-gather-和-tensor-tensor-scatter" class="headerlink" title="3 如何理解tensor.gather 和 tensor.tensor.scatter"></a>3 如何理解tensor.gather 和 tensor.tensor.scatter</h2><p>tensor.gather:</p><p>用途：方便从批量tensor中获取指定索引下的数据，该索引是<strong>高度自定义化</strong>的，可乱序的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor_0 = torch.arange(<span class="number">3</span>, <span class="number">12</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor_0)</span><br></pre></td></tr></table></figure><p>1  <strong>输入行向量index，并替换行索引(dim=0)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">tensor_1 = tensor_0.gather(<span class="number">0</span>, index)</span><br><span class="line"><span class="built_in">print</span>(tensor_1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result: tensor([[9, 7, 5]])</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051548693.webp" alt="v2-1aed61fbaa97775e816c23d8d907bea3_720w"></p><p>2 <strong>输入行向量index，并替换列索引(dim=1)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">tensor_1 = tensor_0.gather(<span class="number">1</span>, index)</span><br><span class="line"><span class="built_in">print</span>(tensor_1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result: tensor([[5, 4, 3]])</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051549896.webp" alt="v2-a437754fed6b29f5af13927ee06e13f9_720w"></p><p>3 <strong>输入列向量index，并替换列索引(dim=1)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]).t()</span><br><span class="line">tensor_1 = tensor_0.gather(<span class="number">1</span>, index)</span><br><span class="line"><span class="built_in">print</span>(tensor_1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result: tensor([[5],</span><br><span class="line">        [7],</span><br><span class="line">        [9]])</span><br></pre></td></tr></table></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051551671.webp" alt="v2-75bcf4697138165941cb2ed083475a07_720w"></p><p>4 <strong>输入二维矩阵index，并替换列索引(dim=1)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">index = torch.tensor([[<span class="number">0</span>, <span class="number">2</span>], </span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor_1 = tensor_0.gather(<span class="number">1</span>, index)</span><br><span class="line"><span class="built_in">print</span>(tensor_1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result: tensor([[3, 5],</span><br><span class="line">        [7, 8]])</span><br></pre></td></tr></table></figure><h2 id="4-tensor-的数据类型"><a href="#4-tensor-的数据类型" class="headerlink" title="4 tensor 的数据类型"></a>4 tensor 的数据类型</h2><p><a href="https://pytorch.org/docs/stable/tensor_attributes.html">https://pytorch.org/docs/stable/tensor_attributes.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-准备工作&quot;&gt;&lt;a href=&quot;#1-准备工作&quot; class=&quot;headerlink&quot; title=&quot;1 准备工作&quot;&gt;&lt;/a&gt;1 准备工作&lt;/h2&gt;&lt;p&gt;安装anaconda，cuda，cudnn等内容（略）&lt;/p&gt;
&lt;p&gt;配置jupyter notebook&lt;</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>一些常用的小技巧</title>
    <link href="http://example.com/post/20651907.html"/>
    <id>http://example.com/post/20651907.html</id>
    <published>2023-04-05T04:29:49.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-vscode-中导入路径自动补齐"><a href="#1-vscode-中导入路径自动补齐" class="headerlink" title="1 vscode 中导入路径自动补齐"></a>1 vscode 中导入路径自动补齐</h1><ul><li><p>先安装 Path Autocomplete 插件</p></li><li><p>再在 settings.json 中加入如下代码</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 导入文件是是否携带文件的扩展名</span><br><span class="line">&quot;path-autocomplete.extensionOnImport&quot;: true,</span><br><span class="line">// 配置 @ 的路径提示</span><br><span class="line">&quot;path-autocomplete.pathMappings&quot;: &#123;</span><br><span class="line">    &quot;@&quot;: &quot;$&#123;folder&#125;/src&quot;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><h1 id="2-VScode-打开文件夹"><a href="#2-VScode-打开文件夹" class="headerlink" title="2 VScode 打开文件夹"></a>2 VScode 打开文件夹</h1><p><a href="https://www.jianshu.com/p/e8c29211fba9">https://www.jianshu.com/p/e8c29211fba9</a></p><ul><li>右击打开文件</li></ul><p>1,    <code>Win+R</code> 打开运行，输入<code>regedit</code>，打开<code>注册表</code>，找到<code>HKEY_CLASSES_ROOT\*\shell</code>分支，如果没有shell分支，则在*下点击右键，选择“<code>新建</code>－<code>项</code>”，建立shell分支。</p><p>2,    在shell下新建“<code>VisualCode</code>”项，在右侧窗口的“<strong>默认</strong>”双击，在数据里输入“<code>用VSCode打开</code>”。</p><p>3,    在“<code>VisualCode</code>”下再新建<code>Command</code>项，在右侧窗口的“<strong>默认</strong>”键值栏内输入程序所在的安装路径，我的是：<code>&quot;D:\Microsoft VS Code\Code.exe&quot; &quot;%1&quot;</code>。<strong>其中的%1表示要打开的文件参数</strong>。</p><p>4,    配置缩略图。在<code>VisualCode</code>项上新建<code>可扩充字符串值</code>，命名为<code>Icon</code>，双击，把<code>&quot;D:\Microsoft VS Code\Code.exe&quot;</code>放进数据就可以了。</p><p>5,    关闭注册表，即可生效</p><ul><li>右击打开文件夹</li></ul><p>找到<code>HKEY_CLASSES_ROOT\Directory\shell</code>分支</p><ul><li>在空白处打开</li></ul><p>找到<code>HKEY_CLASSES_ROOT\Directory\Background\shell</code>分支</p><p>在“<code>VisualCode</code>”下再新建<code>Command</code>项，在右侧窗口的“<strong>默认</strong>”键值栏内输入程序所在的安装路径，我的是：<code>&quot;D:\Microsoft VS Code\Code.exe&quot; &quot;%V&quot;</code>。</p><h1 id="3-vue-vscode-插件"><a href="#3-vue-vscode-插件" class="headerlink" title="3 vue vscode 插件"></a>3 vue vscode 插件</h1><p><a href="https://zhuanlan.zhihu.com/p/347926284">https://zhuanlan.zhihu.com/p/347926284</a></p><h1 id="4-数组方法"><a href="#4-数组方法" class="headerlink" title="4 数组方法"></a>4 数组方法</h1><h2 id="forEach-与-some"><a href="#forEach-与-some" class="headerlink" title="forEach 与 some"></a>forEach 与 some</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> arr = [<span class="string">&#x27;小红&#x27;</span>, <span class="string">&#x27;你大红&#x27;</span>, <span class="string">&#x27;苏大强&#x27;</span>, <span class="string">&#x27;宝&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// forEach 循环一旦开始，无法在中间被停止</span></span><br><span class="line">arr.<span class="title function_">forEach</span>(<span class="function">(<span class="params">item, index</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">  <span class="keyword">if</span> (item === <span class="string">&#x27;苏大强&#x27;</span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(index)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;) </span><br><span class="line"></span><br><span class="line">arr.<span class="title function_">some</span>(<span class="function">(<span class="params">item, index</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;ok&#x27;</span>)</span><br><span class="line">  <span class="keyword">if</span> (item === <span class="string">&#x27;苏大强&#x27;</span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(index)</span><br><span class="line">    <span class="comment">// 在找到对应的项之后，可以通过 return true 固定的语法，来终止 some 循环</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;) </span><br></pre></td></tr></table></figure><h2 id="every"><a href="#every" class="headerlink" title="every"></a>every</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> arr = [</span><br><span class="line">  &#123; <span class="attr">id</span>: <span class="number">1</span>, <span class="attr">name</span>: <span class="string">&#x27;西瓜&#x27;</span>, <span class="attr">state</span>: <span class="literal">true</span> &#125;,</span><br><span class="line">  &#123; <span class="attr">id</span>: <span class="number">2</span>, <span class="attr">name</span>: <span class="string">&#x27;榴莲&#x27;</span>, <span class="attr">state</span>: <span class="literal">false</span> &#125;,</span><br><span class="line">  &#123; <span class="attr">id</span>: <span class="number">3</span>, <span class="attr">name</span>: <span class="string">&#x27;草莓&#x27;</span>, <span class="attr">state</span>: <span class="literal">true</span> &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 需求：判断数组中，水果是否被全选了！</span></span><br><span class="line"><span class="keyword">const</span> result = arr.<span class="title function_">every</span>(<span class="function"><span class="params">item</span> =&gt;</span> item.<span class="property">state</span>)</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(result)</span><br></pre></td></tr></table></figure><h2 id="reduce-累加"><a href="#reduce-累加" class="headerlink" title="reduce 累加"></a>reduce 累加</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> arr = [</span><br><span class="line">  &#123; <span class="attr">id</span>: <span class="number">1</span>, <span class="attr">name</span>: <span class="string">&#x27;西瓜&#x27;</span>, <span class="attr">state</span>: <span class="literal">true</span>, <span class="attr">price</span>: <span class="number">10</span>, <span class="attr">count</span>: <span class="number">1</span> &#125;,</span><br><span class="line">  &#123; <span class="attr">id</span>: <span class="number">2</span>, <span class="attr">name</span>: <span class="string">&#x27;榴莲&#x27;</span>, <span class="attr">state</span>: <span class="literal">false</span>, <span class="attr">price</span>: <span class="number">80</span>, <span class="attr">count</span>: <span class="number">2</span> &#125;,</span><br><span class="line">  &#123; <span class="attr">id</span>: <span class="number">3</span>, <span class="attr">name</span>: <span class="string">&#x27;草莓&#x27;</span>, <span class="attr">state</span>: <span class="literal">true</span>, <span class="attr">price</span>: <span class="number">20</span>, <span class="attr">count</span>: <span class="number">3</span> &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 需求：把购物车数组中，已勾选的水果，总价累加起来！</span></span><br><span class="line"><span class="keyword">let</span> amt = <span class="number">0</span> <span class="comment">// 总价</span></span><br><span class="line">    arr.<span class="title function_">filter</span>(<span class="function"><span class="params">item</span> =&gt;</span> item.<span class="property">state</span>).<span class="title function_">forEach</span>(<span class="function"><span class="params">item</span> =&gt;</span> &#123;</span><br><span class="line">      amt += item.<span class="property">price</span> * item.<span class="property">count</span></span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(amt) </span><br><span class="line"></span><br><span class="line"><span class="comment">// arr.filter(item =&gt; item.state).reduce((累加的结果, 当前循环项) =&gt; &#123; &#125;, 初始值)</span></span><br><span class="line"><span class="keyword">const</span> result = arr.<span class="title function_">filter</span>(<span class="function"><span class="params">item</span> =&gt;</span> item.<span class="property">state</span>).<span class="title function_">reduce</span>(<span class="function">(<span class="params">amt, item</span>) =&gt;</span> amt += item.<span class="property">price</span> * item.<span class="property">count</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(result)</span><br></pre></td></tr></table></figure><h1 id="5-leanote-mongdb-配置"><a href="#5-leanote-mongdb-配置" class="headerlink" title="5 leanote mongdb 配置"></a>5 leanote mongdb 配置</h1><ol><li>找到 /www/wwwroot/leanote/mongodb_backup</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongorestore -h localhost -d leanote --<span class="built_in">dir</span> leanote_install_data/</span><br></pre></td></tr></table></figure><ol start="2"><li>找到bin文件夹</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> bash run.sh &amp;</span><br></pre></td></tr></table></figure><ol start="3"><li>将mongdb内容保存到本地</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongodump -h localhost -d leanote -o data/</span><br></pre></td></tr></table></figure><ol start="4"><li>如果需要更换服务器，只需要执行一下代码</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongorestore -h localhost -d leanote --<span class="built_in">dir</span> data/</span><br></pre></td></tr></table></figure><h1 id="6-前端"><a href="#6-前端" class="headerlink" title="6 前端"></a>6 前端</h1><h2 id="6-1-文本单行显示，超出内容省略"><a href="#6-1-文本单行显示，超出内容省略" class="headerlink" title="6.1 文本单行显示，超出内容省略"></a>6.1 文本单行显示，超出内容省略</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 文字不允许换行（单行文本）</span><br><span class="line"><span class="attribute">white-space</span>: nowrap;</span><br><span class="line">// 溢出部分隐藏</span><br><span class="line"><span class="attribute">overflow</span>: hidden;</span><br><span class="line">// 文本溢出后，使用 ... 代替</span><br><span class="line"><span class="attribute">text-overflow</span>: ellipsis;</span><br></pre></td></tr></table></figure><h2 id="6-2-常用的字符实体"><a href="#6-2-常用的字符实体" class="headerlink" title="6.2 常用的字符实体"></a>6.2 常用的字符实体</h2><table><thead><tr><th>显示结果</th><th>描述</th><th>实体名称</th><th>实体标号</th></tr></thead><tbody><tr><td></td><td>空格</td><td>&amp;nbsp;</td><td>&amp;#160;</td></tr><tr><td>&gt;</td><td>大于号</td><td>&amp;lt;</td><td>&amp;#60;</td></tr><tr><td>&lt;</td><td>小于号</td><td>&amp;gt;</td><td>&amp;#62;</td></tr><tr><td>&times;</td><td>乘号</td><td>&amp;times;</td><td>&amp;#215;</td></tr><tr><td>&divide;</td><td>除号</td><td>&amp;divide;</td><td>&amp;#247;</td></tr></tbody></table><h2 id="6-3-谷歌浏览器查看json文件插件"><a href="#6-3-谷歌浏览器查看json文件插件" class="headerlink" title="6.3 谷歌浏览器查看json文件插件"></a>6.3 谷歌浏览器查看json文件插件</h2><p><a href="https://github.com/gildas-lormeau/JSONVue">https://github.com/gildas-lormeau/JSONVue</a></p><p>download and plugin(src folder)</p><h1 id="7-IJ操作"><a href="#7-IJ操作" class="headerlink" title="7 IJ操作"></a>7 IJ操作</h1><h2 id="7-1-插件安装"><a href="#7-1-插件安装" class="headerlink" title="7.1 插件安装"></a>7.1 插件安装</h2><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/Maven/202209231631287.png" alt="image-20220923163105626"></p><p><a href="https://plugins.jetbrains.com/">https://plugins.jetbrains.com/</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/MyBatisPlus/202210052210026.png" alt="image-20220923163512224"></p><h2 id="7-2-快捷键"><a href="#7-2-快捷键" class="headerlink" title="7.2 快捷键"></a>7.2 快捷键</h2><ul><li>查看类图：ctrl+alt+u</li><li>查看接口所有方法：ctrl+f12</li><li>查找类：Ctrl + Shift + n   double shift</li><li>通过类图查看源码：f4</li><li>快速补齐对象和返回类型：ctrl+enter</li><li>将对象转换为lamada表达式：ctrl+enter</li><li>快速生成try catch方法:  ctrl+alt+t</li><li>查看父方法 ctrl+alt+鼠标左键</li><li>run context configuration ctrl+shift+f10</li><li>抽取 ctrl+alt+m</li><li>选中相同的类同时修改：ctrl+shift+f6</li></ul><h2 id="7-3-查看源码"><a href="#7-3-查看源码" class="headerlink" title="7.3 查看源码"></a>7.3 查看源码</h2><p>如果你想看到框架真正的源码。则在项目下，pom.<a href="https://so.csdn.net/so/search?q=xml&spm=1001.2101.3001.7020">xml</a>同级目录中执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn dependency:resolve -Dclassifier=sources</span><br></pre></td></tr></table></figure><h2 id="7-4-jdk反射问题"><a href="#7-4-jdk反射问题" class="headerlink" title="7.4 jdk反射问题"></a>7.4 jdk反射问题</h2><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/ssm/202210082122139.png" alt="image-20221008212219767"></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--add-opens java.base/java.lang=ALL-UNNAMED</span><br></pre></td></tr></table></figure><h1 id="8-git常用代码"><a href="#8-git常用代码" class="headerlink" title="8 git常用代码"></a>8 git常用代码</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一次初始化(方式1)：</span></span><br><span class="line">git init</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&#x27;first commit&#x27;</span></span><br><span class="line">git remote add origin git@github.com:帐号名/仓库名.git</span><br><span class="line">git pull origin master</span><br><span class="line">git push origin master <span class="comment"># -f 强推</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一次初始化(方式2)：</span></span><br><span class="line">git <span class="built_in">clone</span> git@github.com:git帐号名/仓库名.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平时工作基本操作：</span></span><br><span class="line">git checkout master <span class="comment"># 切到主分支</span></span><br><span class="line">git fetch origin  <span class="comment"># 获取最新变更</span></span><br><span class="line">git checkout -b dev origin/master <span class="comment"># 基于主分支创建dev分支</span></span><br><span class="line">git add . <span class="comment"># 添加到缓存</span></span><br><span class="line">git commit -m <span class="string">&#x27;xxx&#x27;</span> <span class="comment"># 提交到本地仓库</span></span><br><span class="line">git fetch origin <span class="comment"># 获取最新变更</span></span><br><span class="line">git rebase dev origin/master <span class="comment"># 合并到主分支</span></span><br><span class="line">git push origin dev <span class="comment"># 推送到远程分支</span></span><br><span class="line"></span><br><span class="line">git chekout master <span class="comment"># 切到主分支</span></span><br><span class="line">git merge dev <span class="comment"># 合并开发分支</span></span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> -b 远程分支 仓库地址 <span class="comment"># 本地不存在仓库 拉取远程分支代码</span></span><br><span class="line">git checkout -b 远程分支 origin/远程分支 <span class="comment"># 本地存在仓库，拉取远程分支</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将远程代码clone到本地</span></span><br><span class="line">git fetch --all <span class="comment"># 更新远程仓库的代码为最新的</span></span><br><span class="line">git reset --hard origin/master <span class="comment"># 让本地代码与origin / master完全相同</span></span><br><span class="line">git pull origin master <span class="comment"># git pull拉取远程代码</span></span><br><span class="line">git merge master <span class="comment"># git merge将暂存区代码更新到本地工作区</span></span><br></pre></td></tr></table></figure><h1 id="9-pytorch"><a href="#9-pytorch" class="headerlink" title="9 pytorch"></a>9 pytorch</h1><h2 id="9-1-测试GPU是否可用"><a href="#9-1-测试GPU是否可用" class="headerlink" title="9.1 测试GPU是否可用"></a>9.1 测试GPU是否可用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">flag = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA可使用&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA不可用&quot;</span>)</span><br><span class="line"></span><br><span class="line">ngpu= <span class="number">1</span></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;驱动为：&quot;</span>,device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GPU型号： &quot;</span>,torch.cuda.get_device_name(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><h1 id="10-anaconda"><a href="#10-anaconda" class="headerlink" title="10 anaconda"></a>10 anaconda</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前存在那些虚拟环境</span></span><br><span class="line">conda <span class="built_in">env</span> list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建虚拟环境</span></span><br><span class="line">conda create -n env_name python=x.x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活或者切换虚拟环境</span></span><br><span class="line">conda activate env_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出当前的虚拟环境</span></span><br><span class="line">conda deactivate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除虚拟环境</span></span><br><span class="line">conda remove -n env_name --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除某个虚拟环境中的某个包</span></span><br><span class="line">conda remove -name <span class="variable">$env_name</span>  <span class="variable">$pack_name</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像</span></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复默认的镜像</span></span><br><span class="line">conda config --remove-key channels</span><br></pre></td></tr></table></figure><h1 id="11-常用报错信息"><a href="#11-常用报错信息" class="headerlink" title="11 常用报错信息"></a>11 常用报错信息</h1><h1 id="11-1-Spring-Boot"><a href="#11-1-Spring-Boot" class="headerlink" title="11.1 Spring Boot"></a>11.1 Spring Boot</h1><h3 id="1-报错：Web-server-failed-to-start-Port-8080-was-already-in-use"><a href="#1-报错：Web-server-failed-to-start-Port-8080-was-already-in-use" class="headerlink" title="1 报错：Web server failed to start. Port 8080 was already in use."></a>1 报错：Web server failed to start. Port 8080 was already in use.</h3><p>1 查看端口号占用情况，例如查看端口xxxx，得到对应的进程号xxxx</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano | <span class="built_in">findstr</span> 端口号</span><br></pre></td></tr></table></figure><p>2 菜单栏 -&gt; 右键 - &gt; 任务管理器 -&gt; 详细信息，根据PID排序找到PID为xxxx的进程，选择后点击结束任务。</p><p>1 application.yml文件中输入</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8014</span></span><br></pre></td></tr></table></figure><h1 id="11-2-mysql"><a href="#11-2-mysql" class="headerlink" title="11.2 mysql"></a>11.2 mysql</h1><p>mysql：8.0</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/springcloud/202210291541717.png" alt="image-20221029154155350"></p><p>mysql：5.0</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/springcloud/202210291542563.png" alt="image-20221029154213245"></p><h1 id="12-docker"><a href="#12-docker" class="headerlink" title="12 docker"></a>12 docker</h1><h2 id="卸装"><a href="#卸装" class="headerlink" title="卸装"></a>卸装</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-selinux \</span><br><span class="line">                  docker-engine-selinux \</span><br><span class="line">                  docker-engine \</span><br><span class="line">                  docker-ce</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置docker镜像源</span></span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">    </span><br><span class="line">sed -i <span class="string">&#x27;s/download.docker.com/mirrors.aliyun.com\/docker-ce/g&#x27;</span> /etc/yum.repos.d/docker-ce.repo</span><br><span class="line"></span><br><span class="line">yum makecache fast</span><br><span class="line"></span><br><span class="line">yum install -y docker-ce</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line"><span class="comment"># 禁止开机启动防火墙</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line">systemctl start docker  <span class="comment"># 启动docker服务</span></span><br><span class="line">systemctl stop docker  <span class="comment"># 停止docker服务</span></span><br><span class="line">systemctl restart docker  <span class="comment"># 重启docker服务</span></span><br></pre></td></tr></table></figure><h2 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">docker pull xxx <span class="comment"># 拉取镜像</span></span><br><span class="line">docker images <span class="comment"># 查看拉取到的镜像</span></span><br><span class="line">docker save -o [保存的目标文件名称] [镜像名称]</span><br><span class="line">docker rmi nginx:latest <span class="comment"># 删除镜像</span></span><br><span class="line">docker load -i nginx.tar <span class="comment"># 加载本地文件</span></span><br><span class="line"></span><br><span class="line">docker run</span><br><span class="line">docker pause</span><br><span class="line">docker unpause</span><br><span class="line">docker stop</span><br><span class="line">docker start</span><br><span class="line">docker <span class="built_in">exec</span> <span class="comment"># 进入容器执行命令</span></span><br><span class="line">docker logs <span class="comment"># 查看容器运行日志</span></span><br><span class="line">docker ps <span class="comment"># 查看所有运行的容器及状态</span></span><br><span class="line">docker <span class="built_in">rm</span> <span class="comment"># 删除指定容器</span></span><br><span class="line"></span><br><span class="line">docker run --name containerName -p 80:80 -d nginx   <span class="comment"># 冒号左侧是宿主机端口，一般可以任意；右边端口是容器内端口，一般固定</span></span><br><span class="line">docker -d <span class="comment"># 让容器后台运行</span></span><br><span class="line">docker -p <span class="comment"># 指定端口映射</span></span><br><span class="line">docker logs -f mn <span class="comment"># 跟踪日志</span></span><br><span class="line">docker ps -a <span class="comment"># 查看所有容器</span></span><br><span class="line">docker start mn <span class="comment"># 重新启动容器</span></span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -it mn bash <span class="comment"># 进入容器</span></span><br><span class="line"><span class="built_in">cd</span> /usr/share/nginx/html <span class="comment"># 查看DockerHub网站中的nginx页面，可以知道nginx的html目录位置在`/usr/share/nginx/html`</span></span><br><span class="line"><span class="built_in">cat</span> index.html <span class="comment"># 进入首页</span></span><br><span class="line"><span class="built_in">exit</span> <span class="comment"># 退出容器</span></span><br><span class="line">docker <span class="built_in">rm</span> mn -f <span class="comment"># 强制删除运行中的容器</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker volume [COMMAND]</span><br><span class="line">docker volume命令是数据卷操作，根据命令后跟随的<span class="built_in">command</span>来确定下一步的操作：</span><br><span class="line">- create 创建一个volume</span><br><span class="line">- inspect 显示一个或多个volume的信息</span><br><span class="line">- <span class="built_in">ls</span> 列出所有的volume</span><br><span class="line">- prune 删除未使用的volume</span><br><span class="line">- <span class="built_in">rm</span> 删除一个或多个指定的volume</span><br><span class="line"></span><br><span class="line">docker run \</span><br><span class="line">  --name mn \</span><br><span class="line">  -v html:/root/html \</span><br><span class="line">  -p 8080:80</span><br><span class="line">  nginx \</span><br><span class="line">`-v html:/root/htm` ：把html数据卷挂载到容器内的/root/html这个目录中      </span><br><span class="line"></span><br><span class="line">docker run --name mn -p 80:80 -v html:/usr/share/nginx/html -d nginx</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h1 id="13-jupyter-notebook"><a href="#13-jupyter-notebook" class="headerlink" title="13 jupyter notebook"></a>13 jupyter notebook</h1><h2 id="13-1-如何在jupyter-noteboook-中进行环境切换"><a href="#13-1-如何在jupyter-noteboook-中进行环境切换" class="headerlink" title="13.1 如何在jupyter noteboook 中进行环境切换"></a>13.1 如何在jupyter noteboook 中进行环境切换</h2><p>我们的网络结构是基于 pytorch 的，因此需要使用 pytorch 环境下的内核。过程非常简单，首先关闭 Jupyter Notebook 软件和已经打开的有关界面，运行 Anaconda Prompt 进入自己的 pytorch 环境，分别运行下如下代码(注意 pt 切换成自己的环境名字)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install ipykernel</span><br><span class="line">python -m ipykernel install  --name pt --display-name <span class="string">&quot;pytorch&quot;</span></span><br></pre></td></tr></table></figure><h2 id="13-2-主题调整"><a href="#13-2-主题调整" class="headerlink" title="13.2 主题调整"></a>13.2 主题调整</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyterthemes</span><br><span class="line">pip install -- upgrade jupyterthemes</span><br><span class="line">jt -t oceans16 -f consolamono -nf robotosans -tf robotosans -N -T -cellw 60% -dfs 9 -ofs 9</span><br></pre></td></tr></table></figure><h2 id="13-3-代码补全"><a href="#13-3-代码补全" class="headerlink" title="13.3 代码补全"></a>13.3 代码补全</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter_contrib_nbextensions</span><br><span class="line">jupyter nbextensions_configurator <span class="built_in">enable</span> --user</span><br></pre></td></tr></table></figure><p>重新打开jupyter notebook，会在菜单栏中发现Nbextensions插件，不勾选disable，然后会看到Hinterland，最后把Hinterland勾选上。如果没有出现Hinterland，再运行一下下面的代码：`</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter contrib nbextension install --user --skip-running-check </span><br></pre></td></tr></table></figure><h1 id="14-powershell"><a href="#14-powershell" class="headerlink" title="14 powershell"></a>14 powershell</h1><h2 id="14-1-设置anaconda启动项"><a href="#14-1-设置anaconda启动项" class="headerlink" title="14.1 设置anaconda启动项"></a>14.1 设置anaconda启动项</h2><p>1 进入设置 （ctrl+，）</p><p>2 输入命令行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmd.exe /K C:path\Scripts\activate.bat</span><br></pre></td></tr></table></figure><p>3 启动目录改为： 使用父进程目录</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-vscode-中导入路径自动补齐&quot;&gt;&lt;a href=&quot;#1-vscode-中导入路径自动补齐&quot; class=&quot;headerlink&quot; title=&quot;1 vscode 中导入路径自动补齐&quot;&gt;&lt;/a&gt;1 vscode 中导入路径自动补齐&lt;</summary>
      
    
    
    
    
    <category term="tips" scheme="http://example.com/tags/tips/"/>
    
  </entry>
  
</feed>
