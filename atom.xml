<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jyh blog</title>
  
  
  <link href="https://wangtongyouwen.github.io/atom.xml" rel="self"/>
  
  <link href="https://wangtongyouwen.github.io/"/>
  <updated>2023-04-30T13:07:07.759Z</updated>
  <id>https://wangtongyouwen.github.io/</id>
  
  <author>
    <name>jyh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用pytorch实现基础网络12-Unet</title>
    <link href="https://wangtongyouwen.github.io/post/94472398.html"/>
    <id>https://wangtongyouwen.github.io/post/94472398.html</id>
    <published>2023-04-30T06:12:22.000Z</published>
    <updated>2023-04-30T13:07:07.759Z</updated>
    
    <content type="html"><![CDATA[<h1 id="u-net-convolutional-networks-for-biomedical-image-segmentation">U-Net: Convolutional Networks for Biomedical Image Segmentation</h1><p>https://arxiv.org/pdf/1505.04597.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301414602.png" alt="image-20230430141414836" /><figcaption aria-hidden="true">image-20230430141414836</figcaption></figure><ul><li>为什么输入图片和输出图片的大小不同？---因为输入的图片本质也是388*388的，只是通过了一定的填充方式变成了572*572。这是为了让边缘的像素具有更好的上下文信息。</li></ul><p>example：</p><p>https://github.com/yassouali/pytorch-segmentation</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;u-net-convolutional-networks-for-biomedical-image-segmentation&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/h1&gt;</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门24-损失函数</title>
    <link href="https://wangtongyouwen.github.io/post/24351970.html"/>
    <id>https://wangtongyouwen.github.io/post/24351970.html</id>
    <published>2023-04-27T09:28:25.000Z</published>
    <updated>2023-04-30T13:12:16.502Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># logits shape: [BS,NC]</span></span><br><span class="line">batchsize = <span class="number">2</span></span><br><span class="line">num_class = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">logits = torch.randn(batchsize,num_class)</span><br><span class="line">target = torch.randint(num_class,size=(batchsize,)) <span class="comment"># delta目标分布</span></span><br><span class="line">target_logits = torch.randn(batchsize,num_class) <span class="comment"># 非delta目标分布</span></span><br></pre></td></tr></table></figure><h1 id="crossentropyloss">CROSSENTROPYLOSS</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304271739035.png" alt="image-20230427173911712" /><figcaption aria-hidden="true">image-20230427173911712</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304271743417.png" alt="image-20230427174342229" /><figcaption aria-hidden="true">image-20230427174342229</figcaption></figure><p>https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.调用 cross entropy loss (CE loss)</span></span><br><span class="line"><span class="comment"># 第一种方法调用CEloss</span></span><br><span class="line">ce_loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">ce_loss = ce_loss_fn(logits,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;cross entropy loss: <span class="subst">&#123;ce_loss&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 第二种方法</span></span><br><span class="line">ce_loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">ce_loss = ce_loss_fn(logits,target_logits)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;cross entropy loss: <span class="subst">&#123;ce_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="nllloss">NLLLOSS</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304271754467.png" alt="image-20230427175450218" /><figcaption aria-hidden="true">image-20230427175450218</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.调用Negative Log Likelihood Loss (NLL loss)</span></span><br><span class="line">nll_fn = nn.NLLLoss()</span><br><span class="line">nll_loss = nll_fn(torch.log(torch.softmax(logits,-<span class="number">1</span>)),target_indices)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;neagative log-likelihood loss: <span class="subst">&#123;nll_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>如果得到的logits是没有归一化的值，那么使用CEloss</li><li>如果得到的logits是已经softmax后的log概率值值，则使用NLLloss</li><li>总之，两个损失函数都是用来计算分类问题的</li></ul><p>In classification problems we want to estimate the probability of different outcomes. Let the estimated probability of outcome<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" alt="i" /> be <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/187f4094cdfe699a627b3166c870d4e80a3ddbc9" alt="{q_{}(X=i)}" /> with to-be-optimized parameters <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="" /> and let the frequency (empirical probability) of outcome in the training set be <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4adae4de2fee09ec86818003df233cee809e070" alt="{p(X=i)}" />. Given N <a href="https://en.wikipedia.org/wiki/Conditionally_independent">conditionally independent</a> samples in the training set, then the <a href="https://en.wikipedia.org/wiki/Likelihood">likelihood</a> of the parameters <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="" /> of the model <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ab8eaa7d05b1279a2c3bb4c4451a2305b5de380" alt="{q_{}(X=x)}" /> on the training set is</p><figure><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d452937ed5a5bdfdac361ad273d0c55868f0cdd3" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p>where the last expression is due to the definition of the multinomial PMF. Therefore, the log-likelihood, divided by <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3" alt="N" /> is</p><figure><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9601b9fef9a3e4c9bb43553b1b3a1d523c0f3dfa" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p>so that <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximizing the likelihood</a> with respect to the parameters <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="" /> is the same as minimizing the cross-entropy.</p><h1 id="kullback-leibler-divergence-loss">Kullback-Leibler divergence loss</h1><p>https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301259064.png" alt="image-20230430125950441" /><figcaption aria-hidden="true">image-20230430125950441</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 调用 KL divergence loss(KL loss)</span></span><br><span class="line">KL_loss_fn = nn.KLDivLoss()</span><br><span class="line">kld_loss = KL_loss_fn(torch.log(torch.softmax(logits,-<span class="number">1</span>)),torch.softmax(target_logits,dim=-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;KL divergence loss: <span class="subst">&#123;kld_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 4. CE = IE(信息熵) + KLD </span><br><span class="line">ce_loss_fn_smaple = nn.CrossEntropyLoss(reduction=&quot;none&quot;)</span><br><span class="line">ce_loss_sample = ce_loss_fn_smaple(logits,torch.softmax(target_logits,dim=-1))</span><br><span class="line">print(f&quot;cross entropy loss sample:&#123;ce_loss_sample&#125;&quot;)</span><br><span class="line"></span><br><span class="line">kld_loss_fn_sample = nn.KLDivLoss(reduction=&quot;none&quot;)</span><br><span class="line">kld_loss_sample = kld_loss_fn_sample(torch.log(torch.softmax(logits,-1)),torch.softmax(target_logits,dim=-1)).sum(-1)</span><br><span class="line">print(f&quot;KL divergence loss sample:&#123;kld_loss_sample&#125;&quot;)</span><br><span class="line"></span><br><span class="line">target_information_entropy = torch.distributions.Categorical(probs=torch.softmax(target_logits,dim=-1)).entropy()</span><br><span class="line">print(f&quot;information entropy sample:&#123;target_information_entropy&#125;&quot;)</span><br><span class="line"></span><br><span class="line">torch.allclose(ce_loss_sample,kld_loss_sample+target_information_entropy)</span><br></pre></td></tr></table></figure><h1 id="bceloss">BCELOSS</h1><p>https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. 调用Binary Cross Entropy loss （BCE loss）</span></span><br><span class="line">bce_loss_fn = nn.BCELoss()</span><br><span class="line">logits = torch.randn(batchsize)</span><br><span class="line">prob_1 = torch.sigmoid(logits)</span><br><span class="line">target = torch.randint(<span class="number">2</span>,size=(batchsize,))</span><br><span class="line">bce_loss = bce_loss_fn(prob_1,target.<span class="built_in">float</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;binary cross entropy loss: <span class="subst">&#123;bce_loss&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 用 NLL loss 代替 BCE loss</span></span><br><span class="line">prob_0 = <span class="number">1</span> - prob_1.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">prob = torch.cat([prob_0,prob_1.unsqueeze(-<span class="number">1</span>)],dim=-<span class="number">1</span>)</span><br><span class="line">nll_loss_binary = nll_fn(torch.log(prob),target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;negative likelihood loss binary:<span class="subst">&#123;nll_loss_binary&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="cosineembeddingloss">COSINEEMBEDDINGLOSS</h1><p>https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301347056.png" alt="image-20230430134714165" /><figcaption aria-hidden="true">image-20230430134714165</figcaption></figure><ul><li>计算相似度匹配（图片检索任务：找出相似度最相似的）</li><li>对比学习，自监督学习</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络3-transformer</title>
    <link href="https://wangtongyouwen.github.io/post/2fee727b.html"/>
    <id>https://wangtongyouwen.github.io/post/2fee727b.html</id>
    <published>2023-04-27T09:28:25.000Z</published>
    <updated>2023-04-30T13:07:07.761Z</updated>
    
    <content type="html"><![CDATA[<p>https://arxiv.org/pdf/1706.03762.pdf</p><p>https://nlp.seas.harvard.edu/2018/04/03/attention.html</p><h3 id="attention-is-all-you-need-----transformer">attention is all you need ---&gt; transformer</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725197.png" alt="image-20230408205122663" style="zoom: 80%;" /></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725367.png" alt="image-20230408205258220" style="zoom:50%;" /></p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082110329.jpg" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082209301.png" alt="image-20230408220929314" style="zoom:80%;" /></p><h3 id="pytorch-源码">1 pytorch 源码</h3><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span><br><span class="line">        &gt;&gt;&gt; src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">             num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">             activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">             custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">             device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">    <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.encoder = custom_encoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.decoder = custom_decoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">    self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.nhead = nhead</span><br><span class="line"></span><br><span class="line">    self.batch_first = batch_first</span><br></pre></td></tr></table></figure><p>其中初始化部分最为重要的四部分：TransformerEncoderLayer（通过encoder_layer连接），TransformerDecoderLayer（通过decoder_layer连接）</p><h4 id="transformerencoderlayer">1.1 TransformerEncoderLayer</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Args:</span><br><span class="line">    d_model: the number of expected features <span class="keyword">in</span> the input (required).</span><br><span class="line">    nhead: the number of heads <span class="keyword">in</span> the multiheadattention models (required).</span><br><span class="line">    dim_feedforward: the dimension of the feedforward network model (default=2048).</span><br><span class="line">    dropout: the dropout value (default=0.1).</span><br><span class="line">    activation: the activation <span class="keyword">function</span> of the intermediate layer, can be a string</span><br><span class="line">        (<span class="string">&quot;relu&quot;</span> or <span class="string">&quot;gelu&quot;</span>) or a unary callable. Default: relu</span><br><span class="line">    layer_norm_eps: the eps value <span class="keyword">in</span> layer normalization components (default=1e-5).</span><br><span class="line">    batch_first: If ``True``, <span class="keyword">then</span> the input and output tensors are provided</span><br><span class="line">        as (batch, <span class="built_in">seq</span>, feature). Default: ``False`` (<span class="built_in">seq</span>, batch, feature).</span><br><span class="line">    norm_first: <span class="keyword">if</span> ``True``, layer norm is <span class="keyword">done</span> prior to attention and feedforward</span><br><span class="line">        operations, respectively. Otherwise it<span class="string">&#x27;s done after. Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Examples::</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">              activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">              layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">              device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">     factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">     <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">     self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                         **factory_kwargs)</span><br><span class="line">     <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">     self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">     self.dropout = Dropout(dropout)</span><br><span class="line">     self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">     self.norm_first = norm_first</span><br><span class="line">     self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.dropout1 = Dropout(dropout)</span><br><span class="line">     self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">         activation = _get_activation_fn(activation)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># We can&#x27;t test self.activation in forward() in TorchScript,</span></span><br><span class="line">     <span class="comment"># so stash some information about it instead.</span></span><br><span class="line">     <span class="keyword">if</span> activation <span class="keyword">is</span> F.relu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.ReLU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">1</span></span><br><span class="line">     <span class="keyword">elif</span> activation <span class="keyword">is</span> F.gelu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.GELU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">2</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">0</span></span><br><span class="line">     self.activation = activation</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">x = src</span><br><span class="line">     <span class="keyword">if</span> self.norm_first:</span><br><span class="line">         x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">         x = x + self._ff_block(self.norm2(x))</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">         x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> x</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h4 id="transformerencoder">1.2 TransformerEncoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers. Users can build the</span></span><br><span class="line"><span class="string">    BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        encoder_layer: an instance of the TransformerEncoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-encoder-layers in the encoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string">        enable_nested_tensor: if True, input will automatically convert to nested tensor</span></span><br><span class="line"><span class="string">            (and convert back on output). This will improve the overall performance of</span></span><br><span class="line"><span class="string">            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span>, enable_nested_tensor=<span class="literal">True</span>, mask_check=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">        self.enable_nested_tensor = enable_nested_tensor</span><br><span class="line">        self.mask_check = mask_check</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="transformerdecoderlayer">1.3 TransformerDecoderLayer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of the intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, layer norm is done prior to self attention, multihead</span></span><br><span class="line"><span class="string">            attention and feedforward operations, respectively. Otherwise it&#x27;s done after.</span></span><br><span class="line"><span class="string">            Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">        memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">        tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">        memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">        tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">        memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        see the docs in Transformer class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">    x = tgt</span><br><span class="line">    <span class="keyword">if</span> self.norm_first:</span><br><span class="line">        x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">        x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">        x = x + self._ff_block(self.norm3(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">        x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">        x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="transformerdecoder">1.4 TransformerDecoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        decoder_layer: an instance of the TransformerDecoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-decoder-layers in the decoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><em>Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.</em></p><figure><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption></figure><p>在进行解码过程中，第一个词的预测只与第一个词有关，因此最后的的attention机制是个上三角的形式，如上图所示。</p><h4 id="attention">1.5 Attention</h4><figure><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption></figure><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span>, and values of dimension <span class="math inline">\(d_v\)</span>. We compute the dot products of the query with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax function to obtain the weights on the values.</p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We compute the matrix of outputs as: <span class="math display">\[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304100940797.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="encoder-细节">2 Encoder 细节</h3><h4 id="word-embedding">2.1 word embedding</h4><p>考虑 source sentence 和 target sentence 构建序列，序列的字符以其词表中的索引的形式表示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source sentence 和 target sentence 的初始长度</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 单词表大小</span></span><br><span class="line">max_num_src_words = <span class="number">8</span></span><br><span class="line">max_num_tgt_words = <span class="number">8</span></span><br><span class="line">model_dim = <span class="number">8</span> <span class="comment"># 特征大小，原文是512</span></span><br><span class="line"><span class="comment"># 序列最大长度</span></span><br><span class="line">max_src_seq_len = <span class="number">5</span></span><br><span class="line">max_tgt_seq_len = <span class="number">5</span></span><br><span class="line">max_position_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># src_len = torch.randint(2,5,(batch_size,))</span></span><br><span class="line"><span class="comment"># tgt_len = torch.randint(2,5,(batch_size,)) </span></span><br><span class="line">src_len = torch.Tensor([<span class="number">2</span>,<span class="number">4</span>]).to(torch.int32)  <span class="comment"># 句子长度（2个句子）</span></span><br><span class="line">tgt_len = torch.Tensor([<span class="number">4</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line"><span class="built_in">print</span>(src_len,tgt_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 单词索引构成的源句子和目标句子,pad为最大序列长度,unsqueeze 变为2维张量，然后使用cat拼接起来,padding 默认值为0,构建batch</span></span><br><span class="line">src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_src_words,(L,)),(<span class="number">0</span>,max_src_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]) </span><br><span class="line">tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_tgt_words,(L,)),(<span class="number">0</span>,max_tgt_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_seq,<span class="string">&quot;\n&quot;</span>,tgt_seq,end=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2 构造 word embedding</span></span><br><span class="line"><span class="comment"># 第0行是默认padding的0，第1-9行是每个单词的embedding结果</span></span><br><span class="line">src_embedding_table = nn.Embedding(max_num_src_words + <span class="number">1</span>, model_dim)</span><br><span class="line">tgt_embedding_table = nn.Embedding(max_num_tgt_words + <span class="number">1</span>, model_dim)</span><br><span class="line">src_embedding = src_embedding_table(src_seq) <span class="comment"># embedding 的 forward 方法</span></span><br><span class="line">stgt_embedding = tgt_embedding_table(tgt_seq) </span><br><span class="line"><span class="built_in">print</span>(src_embedding_table.weight)</span><br><span class="line"><span class="built_in">print</span>(src_seq)</span><br><span class="line"><span class="built_in">print</span>(src_embedding)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="position-embedding">2.2 position embedding</h4><p><span class="math display">\[\begin{aligned}P E_{(p o s, 2 i)} &amp; =\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\P E_{(p o s, 2 i+1)} &amp; =\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)\end{aligned}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 构建 position embedding</span></span><br><span class="line">pos_mat = torch.arange(max_position_len).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">i_mat = torch.<span class="built_in">pow</span>(<span class="number">10000</span>,torch.arange(<span class="number">0</span>,model_dim,<span class="number">2</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)/model_dim)</span><br><span class="line">pe_embedding_table = torch.zeros(max_position_len,model_dim)</span><br><span class="line"><span class="comment"># element point</span></span><br><span class="line">pe_embedding_table[:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos_mat/i_mat)  <span class="comment"># 偶数行</span></span><br><span class="line">pe_embedding_table[:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos_mat/i_mat)  <span class="comment"># 奇数行</span></span><br><span class="line"><span class="built_in">print</span>(pos_mat,<span class="string">&#x27;\n&#x27;</span>,i_mat,<span class="string">&#x27;\n&#x27;</span>,pe_embedding_table)</span><br><span class="line"></span><br><span class="line">src_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(src_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> src_len]).to(torch.int32)</span><br><span class="line">tgt_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(tgt_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> tgt_len]).to(torch.int32)</span><br><span class="line"></span><br><span class="line">src_pe_embedding = pe_embedding(src_pos)</span><br><span class="line">tgt_pe_embedding = pe_embedding(tgt_pos)</span><br><span class="line"><span class="built_in">print</span>(src_pe_embedding)</span><br><span class="line"><span class="built_in">print</span>(tgt_pe_embedding)</span><br></pre></td></tr></table></figure><p><span class="math inline">\(10000^{2 i / d_{\mathrm{model}}}\)</span> 表示为<span class="math inline">\(\omega_k\)</span>，pos表示为<span class="math inline">\(t\)</span></p><p>解决 out of demain: 如果是超出序列长度，可以通过之前序列长度的线性组合来表示</p><p>For every sine-cosine pair corresponding to frequency <span class="math inline">\(\omega_k\)</span>, there is a linear transformation $ M ^{2 } $(independent of t) where the following equation holds:</p>$$ M <span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega _k \cdot (t+\phi))\\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}\]</span><p>$$ proof:</p>Let <span class="math inline">\(M\)</span> be a <span class="math inline">\(2\times2\)</span> matrix, we want to find <span class="math inline">\(u_1,v_1,u_2\)</span> and <span class="math inline">\(v_2\)</span> so that: $$<span class="math display">\[\begin{bmatrix} u_1 &amp;v_1 \\ u_2 &amp; v_2\end{bmatrix}\]</span><span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega _k \cdot (t+\phi))\\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}\]</span><span class="math display">\[By applying the addition theorem, we can expand the right hand side as follows:\]</span><span class="math display">\[\begin{bmatrix} u_1 &amp;v_1 \\ u_2 &amp; v_2\end{bmatrix}\]</span><span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega_k \cdot t)cos(\omega_k \cdot \phi) + cos(\omega_k \cdot t)sin(\omega_k \cdot \phi)         \\cos(\omega_k \cdot t)cos(\omega_k \cdot \phi)  - sin(\omega_k \cdot t)sin(\omega_k \cdot \phi)\end{bmatrix}\]</span><span class="math display">\[Which result in the following two equations:\]</span><span class="math display">\[\begin{array}{l}u_{1} \sin \left(\omega_{k} \cdot t\right)+v_{1} \cos \left(\omega_{k} \cdot t\right)=\cos \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\sin \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right) \\u_{2} \sin \left(\omega_{k} \cdot t\right)+v_{2} \cos \left(\omega_{k} \cdot t\right)=-\sin \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\cos \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right)\end{array}\]</span><span class="math display">\[By solving above equations, we get:\]</span><span class="math display">\[\begin{aligned}u_{1}=\cos \left(\omega_{k} \cdot \phi\right) ， v_{1}=\sin \left(\omega_{k} \cdot \phi\right) \\u_{2}=-\sin \left(\omega_{k} \cdot \phi\right) ， v_{2}=\cos \left(\omega_{k} \cdot \phi\right)\end{aligned}\]</span><span class="math display">\[So the final transformation matrix M is:\]</span> M_{,k}=<span class="math display">\[\begin{bmatrix} cos(\omega_k,\phi) &amp; sin(\omega_k,\phi) \\- sin(\omega_k,\phi) &amp; cos(\omega_k,\phi)\end{bmatrix}\]</span><p>$$</p><h4 id="构建encoder的self-attention-mask">2.3 构建encoder的self-attention mask</h4><p>mask的shape：[batch_size,max_src_len,max_tgt_len] 值为1或-inf</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 构建encoder的self-attention mask</span></span><br><span class="line">valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(src_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]),<span class="number">2</span>) <span class="comment"># 有效长度</span></span><br><span class="line">valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效矩阵</span></span><br><span class="line">invalid_encoder_pos_matrix = <span class="number">1</span> - valid_encoder_pos_matrix</span><br><span class="line">mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.<span class="built_in">bool</span>) <span class="comment"># 变为bool</span></span><br><span class="line"><span class="built_in">print</span>(valid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,invalid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,mask_encoder_self_attention) <span class="comment">#(batchsize,maxlen after padding,_)</span></span><br><span class="line"><span class="comment"># true 需要 mask</span></span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(src_len),<span class="built_in">max</span>(src_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_score = score.masked_fill(mask_encoder_self_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_len)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="built_in">print</span>(masked_score)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br><span class="line"><span class="comment"># 无需因果的遮掩</span></span><br></pre></td></tr></table></figure><h4 id="scaled-的重要性">2.4 scaled 的重要性</h4><p><span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]</span></p><p>这里的softmax中为什么要除以<span class="math inline">\(\sqrt{d_k}\)</span>?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># when the varience of prob is too big</span></span><br><span class="line">alpha1 = <span class="number">0.1</span></span><br><span class="line">alpha2 = <span class="number">10</span></span><br><span class="line">score = torch.randn(<span class="number">5</span>)</span><br><span class="line">prob1 = F.softmax(score*alpha1,-<span class="number">1</span>)</span><br><span class="line">prob2 = F.softmax(score*alpha2,-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_func</span>(<span class="params">score</span>):</span><br><span class="line">    <span class="keyword">return</span> F.softmax(score)</span><br><span class="line">jaco_mat1 = torch.autograd.functional.jacobian(softmax_func,score*alpha1)</span><br><span class="line">jaco_mat2 = torch.autograd.functional.jacobian(softmax_func,score*alpha2)</span><br><span class="line"><span class="comment"># jaco matrix is close to zero when the varience is too big</span></span><br><span class="line"><span class="comment"># print(score,prob1,prob2)</span></span><br><span class="line"><span class="built_in">print</span>(jaco_mat1,<span class="string">&#x27;\n&#x27;</span>,jaco_mat2)</span><br></pre></td></tr></table></figure><h3 id="decoder-细节">3 decoder 细节</h3><h4 id="intra-attention-的-mask">3.1 intra-attention 的 mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step5 构造intra-attention的mask</span></span><br><span class="line"><span class="comment"># Q @ k^T shape:(batch_size,tgt_seq_len,src_seq_len)</span></span><br><span class="line">valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len]),<span class="number">2</span>)</span><br><span class="line">valid_cross_pos_matrix = torch.bmm(valid_decoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效位置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;源序列有效位置张量：&quot;</span>,valid_encoder_pos,<span class="string">&quot;\n 目标序列有效位置张量：&quot;</span>,valid_decoder_pos,<span class="string">&quot;\n 目标序列对源头序列有效位置张量：&quot;</span>,valid_cross_pos)</span><br><span class="line"></span><br><span class="line">invalid_cross_pos_matrix = <span class="number">1</span> - valid_cross_pos_matrix</span><br><span class="line">mask_cross_attention = invalid_cross_pos_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(mask_cross_attention)</span><br><span class="line"><span class="comment"># print(valid_cross_pos)</span></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_cross_score = score.masked_fill(mask_cross_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob_cross = F.softmax(masked_cross_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(prob_cross)</span><br></pre></td></tr></table></figure><h4 id="decoder-self-attention">3.2 decoder self-attention</h4><p>下三角形的mask：防止因果</p><p>要把答案遮住，如果预测第四个位置，就要把第四个位置以后的所有内容都遮住。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step6 构造 decoder self-attention 的 mask</span></span><br><span class="line">valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L,<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">invalid_decoder_tri_matrix = <span class="number">1</span> - valid_decoder_tri_matrix</span><br><span class="line">invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(valid_decoder_tri_matrix,invalid_decoder_tri_matrix)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line">masked_score = score.masked_fill(invalid_decoder_tri_matrix,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(tgt_len)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br></pre></td></tr></table></figure><p>流式预测的时候，特别需要这个掩码。</p><h4 id="scaled-self-attention">3.3 scaled self-attention</h4><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101603265.png" alt="image-20230410160332412" /><figcaption aria-hidden="true">image-20230410160332412</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q,K,V,attn_mask</span>):</span><br><span class="line">    <span class="comment"># shape pf Q,k,V: (batch_size * num_head,seq_len,model_dim/num_head)</span></span><br><span class="line">    score = torch.bmn(Q,K.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/torch.sqrt(model_dim)</span><br><span class="line">    masked_score = score.masked_fill(attn_mask,-<span class="number">1e9</span>)</span><br><span class="line">    prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line">    context = torch.bmn(prov,V)</span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>源码：D:\0_python-packages.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multi_head_attention_forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    query: Tensor,</span></span><br><span class="line"><span class="params">    key: Tensor,</span></span><br><span class="line"><span class="params">    value: Tensor,</span></span><br><span class="line"><span class="params">    embed_dim_to_check: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    in_proj_weight: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    in_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_k: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_v: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    add_zero_attn: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">    dropout_p: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    out_proj_weight: Tensor,</span></span><br><span class="line"><span class="params">    out_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    training: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    use_separate_proj_weight: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    q_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    k_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    v_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_k: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_v: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    average_attn_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101653337.png" alt="image-20230410165300158" /><figcaption aria-hidden="true">image-20230410165300158</figcaption></figure><h3 id="loss-function">4 Loss function</h3><p>https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101703643.png" alt="image-20230410170322549" /><figcaption aria-hidden="true">image-20230410170322549</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bath_size = <span class="number">2</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">vocab_size = <span class="number">4</span></span><br><span class="line">logits = torch.randn(bath_size,seq_len,vocab_size)      <span class="comment"># bath_size = 2, seq_len = 3, vocab_size = 4</span></span><br><span class="line">label = torch.randint(<span class="number">0</span>,vocab_size,(bath_size,seq_len))</span><br><span class="line">logits = logits.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">F.cross_entropy(logits,label) <span class="comment"># 六个单词的平均交叉熵</span></span><br><span class="line">F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) <span class="comment"># 返回所有单词的交叉熵</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mask</span></span><br><span class="line">tgt_len =torch.Tensor([<span class="number">2</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line">mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len)-L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">cross_entropy = F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) * mask </span><br><span class="line"><span class="built_in">print</span>(cross_entropy)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/p&gt;
&lt;p&gt;https://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;
&lt;h3 id=&quot;attention-is-all-you-nee</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目4-读取Excel/csv文件格式为PyTorch张量</title>
    <link href="https://wangtongyouwen.github.io/post/9358f042.html"/>
    <id>https://wangtongyouwen.github.io/post/9358f042.html</id>
    <published>2023-04-27T08:33:55.000Z</published>
    <updated>2023-04-30T13:07:07.756Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ExcelDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath=<span class="string">&quot;train.xlsx&quot;</span>, sheet_name=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;filepath&#125;</span>, sheet=<span class="subst">&#123;sheet_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath=<span class="string">&quot;train.xlsx&quot;</span>, sheet_name=<span class="number">0</span></span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;filepath&#125;</span>, sheet=<span class="subst">&#123;sheet_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            df = pandas.read_excel(</span><br><span class="line">                <span class="comment"># filepath,header=0,index_col=0,</span></span><br><span class="line">                filepath, header=<span class="number">0</span>,</span><br><span class="line">                names=[<span class="string">&#x27;admit&#x27;</span>, <span class="string">&#x27;gre&#x27;</span>, <span class="string">&#x27;gpa&#x27;</span>, <span class="string">&#x27;prestige&#x27;</span>],</span><br><span class="line">                sheet_name=sheet_name,</span><br><span class="line">                dtype=&#123;<span class="string">&quot;gre&quot;</span>: np.float32, <span class="string">&quot;gpa&quot;</span>: np.float32, <span class="string">&quot;admit&quot;</span>: np.int8, <span class="string">&quot;prestige&quot;</span>: np.string_&#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;the shape of dataframe is <span class="subst">&#123;df.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            feat = df.iloc[:, <span class="number">1</span>:<span class="number">3</span>].values</span><br><span class="line">            label = df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">            self.x = torch.from_numpy(feat)</span><br><span class="line">            self.y = torch.from_numpy(label)</span><br><span class="line">            <span class="comment"># print(feat,label)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">            <span class="keyword">return</span> self.x[index], self.y[index]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Csv2Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath=<span class="string">&quot;train.csv&quot;</span></span>):</span><br><span class="line">        <span class="comment"># there is no sheet name definition in csv format file</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;filepath&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file=filepath,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">        feat = []</span><br><span class="line">        label = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</span><br><span class="line">            values = line.strip().split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            row_feat = [<span class="built_in">float</span>(v) <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span>  <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> values[<span class="number">1</span>:<span class="number">2</span>]]</span><br><span class="line">            row_label = <span class="built_in">int</span>(values[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            feat.append(row_feat)</span><br><span class="line">            label.append(row_label)</span><br><span class="line">        feat = np.array(feat,dtype=np.float32)</span><br><span class="line">        label = np.array(label,dtype=np.int8)</span><br><span class="line"></span><br><span class="line">        self.x = torch.from_numpy(feat)</span><br><span class="line">        self.y = torch.from_numpy(label)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x[index], self.y[index]</span><br></pre></td></tr></table></figure><p>其中对缺省值进行处理，可以使用平均数来代替这个缺省值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目3-基于ResNet的水果蔬菜分类</title>
    <link href="https://wangtongyouwen.github.io/post/4627104a.html"/>
    <id>https://wangtongyouwen.github.io/post/4627104a.html</id>
    <published>2023-04-26T13:21:35.000Z</published>
    <updated>2023-04-27T08:32:54.859Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据集介绍">1 数据集介绍</h2><p>https://aistudio.baidu.com/aistudio/datasetdetail/119023</p><p>其中的图片有36各类，不同类别在不同的文件夹中，其中文件后缀有"jpg,png,JPG"</p><h2 id="数据集预处理">2 数据集预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Pre_Data</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 对所有图片进行RGB转换，并且统一调整到一致大小，但不让图片发生变形或扭曲,划分训练集和测试集&quot;&quot;&quot;</span></span><br><span class="line">    test_split_ratio = <span class="number">0.05</span></span><br><span class="line">    desired_size =<span class="number">128</span> <span class="comment"># 图片缩放后的同一大小</span></span><br><span class="line">    raw_path = <span class="string">&quot;./raw&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># class files in the raw data</span></span><br><span class="line">    dirs = glob.glob(os.path.join(raw_path,<span class="string">&quot;*&quot;</span>))</span><br><span class="line">    dirs = [d <span class="keyword">for</span> d <span class="keyword">in</span> dirs <span class="keyword">if</span> os.path.isdir(d)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(dirs)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(dirs)&#125;</span> classes: <span class="subst">&#123;dirs&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> path <span class="keyword">in</span> dirs:</span><br><span class="line">        <span class="comment"># 对每个类别单独处理</span></span><br><span class="line">        path = path.split(<span class="string">&quot;\\&quot;</span>)[-<span class="number">1</span>] <span class="comment"># classes</span></span><br><span class="line">        <span class="comment"># print(path)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f&quot;train/<span class="subst">&#123;path&#125;</span>&quot;</span>):</span><br><span class="line">            os.makedirs(<span class="string">f&quot;train/<span class="subst">&#123;path&#125;</span>&quot;</span>,exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>):</span><br><span class="line">            os.makedirs(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>,exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        files = glob.glob(os.path.join(raw_path,path,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">        files += glob.glob(os.path.join(raw_path,path,<span class="string">&quot;*.JPG&quot;</span>))</span><br><span class="line">        files += glob.glob(os.path.join(raw_path,path,<span class="string">&quot;*.png&quot;</span>))</span><br><span class="line">        <span class="comment"># print(files)</span></span><br><span class="line">        random.shuffle(files)</span><br><span class="line"></span><br><span class="line">        boundary = <span class="built_in">int</span>(<span class="built_in">len</span>(files)*test_split_ratio) <span class="comment"># 测试集和训练集的边界</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i,file <span class="keyword">in</span> <span class="built_in">enumerate</span>(files):</span><br><span class="line">            img = Image.<span class="built_in">open</span>(file).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">            <span class="comment"># print(img)</span></span><br><span class="line">            old_size = img.size <span class="comment"># old_size[0] is in (width,height) format</span></span><br><span class="line">            <span class="comment"># print(old_size)</span></span><br><span class="line"></span><br><span class="line">            ratio = <span class="built_in">float</span>(desired_size)/<span class="built_in">max</span>(old_size)</span><br><span class="line"></span><br><span class="line">            new_size = <span class="built_in">tuple</span>([<span class="built_in">int</span>(x*ratio) <span class="keyword">for</span> x <span class="keyword">in</span> old_size])</span><br><span class="line"></span><br><span class="line">            im = img.resize(new_size,Image.LANCZOS) <span class="comment"># 无模糊</span></span><br><span class="line"></span><br><span class="line">            new_im = Image.new(<span class="string">&quot;RGB&quot;</span>,(desired_size,desired_size))</span><br><span class="line">            new_im.paste(im,((desired_size-new_size[<span class="number">0</span>])//<span class="number">2</span>,</span><br><span class="line">                             (desired_size-new_size[<span class="number">1</span>])//<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">assert</span> new_im.mode == <span class="string">&quot;RGB&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &lt;= boundary:</span><br><span class="line">                new_im.save(os.path.join(<span class="string">f&quot;test/<span class="subst">&#123;path&#125;</span>&quot;</span>,file.split(<span class="string">&quot;\\&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]+ <span class="string">&quot;.jpg&quot;</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_im.save(os.path.join(<span class="string">f&quot;train/<span class="subst">&#123;path&#125;</span>&quot;</span>,file.split(<span class="string">&quot;\\&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]+ <span class="string">&quot;.jpg&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;classes <span class="subst">&#123;path&#125;</span> is done !&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    test_files = glob.glob(os.path.join(<span class="string">&quot;test&quot;</span>,<span class="string">&quot;*&quot;</span>,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">    train_files = glob.glob(os.path.join(<span class="string">&quot;train&quot;</span>,<span class="string">&quot;*&quot;</span>,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(test_files)&#125;</span> files for testing&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(train_files)&#125;</span> files for training&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_mean_std</span>():</span><br><span class="line">    train_files = glob.glob(os.path.join(<span class="string">&quot;train&quot;</span>,<span class="string">&quot;*&quot;</span>,<span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;totally <span class="subst">&#123;<span class="built_in">len</span>(train_files)&#125;</span> files for training&quot;</span>)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> train_files:</span><br><span class="line">        img = Image.<span class="built_in">open</span>(file).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        img = np.array(img).astype(np.uint8)</span><br><span class="line">        img = img / <span class="number">255</span></span><br><span class="line">        result.append(img)</span><br><span class="line">    <span class="built_in">print</span>(np.shape(result)) <span class="comment"># [bs,H,W,C]</span></span><br><span class="line">    mean = np.mean(result,axis=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    std = np.std(result,axis=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(mean,std</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="eval">3 eval</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">data_loader, model, device</span>):</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    metric_logger = misc.MetricLogger(delimiter=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    header = <span class="string">&quot;test:&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># switch to evaluation mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> metric_logger.log_every(data_loader, <span class="number">10</span>, header):</span><br><span class="line">        <span class="comment"># print(batch)</span></span><br><span class="line">        images = batch[<span class="number">0</span>]</span><br><span class="line">        target = batch[-<span class="number">1</span>]</span><br><span class="line">        images = images.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">        target = target.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute output</span></span><br><span class="line">        output = model(images)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">        output = F.softmax(output, dim=-<span class="number">1</span>)</span><br><span class="line">        acc1, acc5 = accuracy(output, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">        batch_size = images.shape[<span class="number">0</span>]</span><br><span class="line">        metric_logger.update(loss=loss.item())</span><br><span class="line">        metric_logger.meters[<span class="string">&#x27;acc1&#x27;</span>].update(acc1.item(), n=batch_size)</span><br><span class="line">        metric_logger.meters[<span class="string">&#x27;acc5&#x27;</span>].update(acc5.item(), n=batch_size)</span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;* Acc@1 &#123;top1.global_avg:.3f&#125; Acc@5 &#123;top5.global_avg:.3f&#125; loss &#123;losses.global_avg:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))</span><br><span class="line">    <span class="keyword">return</span> &#123;k: meter.global_avg <span class="keyword">for</span> k, meter <span class="keyword">in</span> metric_logger.meters.items()&#125;</span><br></pre></td></tr></table></figure><h2 id="train">4 train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model: nn.Module, criterion: nn.Module, data_loader: Iterable, optimizer: torch.optim.Optimizer,</span></span><br><span class="line"><span class="params">                    device: torch.device, epoch: <span class="built_in">int</span>, loss_scaler, max_norm: <span class="built_in">float</span> = <span class="number">0</span>, log_writer=<span class="literal">None</span>, args=<span class="literal">None</span></span>):</span><br><span class="line">    model.train(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    accum_iter = args.accum_iter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;log_dir:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_writer.log_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data_iter_step, (samples, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        samples = samples.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">        targets = targets.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        outputs = model(samples)</span><br><span class="line"></span><br><span class="line">        warmup_lr = args.lr</span><br><span class="line">        optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>] = warmup_lr</span><br><span class="line"></span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line">        loss /= accum_iter</span><br><span class="line"></span><br><span class="line">        loss_scaler(loss, optimizer, clip_grad=max_norm,</span><br><span class="line">                    parameters=model.parameters(), create_graph=<span class="literal">False</span>,</span><br><span class="line">                    update_grad=(data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        loss_value = loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> math.isfinite(loss_value):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Loss is &#123;&#125;, stopping training&quot;</span>.<span class="built_in">format</span>(loss_value))</span><br><span class="line">            sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            <span class="string">&quot;&quot;&quot; We use epoch_1000x as the x-axis in tensorboard.</span></span><br><span class="line"><span class="string">            This calibrates different curves when batch size changes.</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            epoch_1000x = <span class="built_in">int</span>((data_iter_step / <span class="built_in">len</span>(data_loader) + epoch) * <span class="number">1000</span>)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;loss&#x27;</span>, loss_value, epoch_1000x)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;lr&#x27;</span>, warmup_lr, epoch_1000x)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch:<span class="subst">&#123;epoch&#125;</span>,Step: <span class="subst">&#123;data_iter_step&#125;</span>,loss: <span class="subst">&#123;loss&#125;</span>,lr: <span class="subst">&#123;warmup_lr&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="dataset">5 dataset</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def build_transform(is_train, args):</span><br><span class="line">    if is_train:</span><br><span class="line">        # this should always dispatch to transforms_imagenet_train</span><br><span class="line">        print(&quot;train transform&quot;)</span><br><span class="line">        return torchvision.transforms.Compose([</span><br><span class="line">            torchvision.transforms.Resize((args.input_size, args.input_size)),</span><br><span class="line">            torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">            torchvision.transforms.RandomVerticalFlip(),</span><br><span class="line">            torchvision.transforms.RandomPerspective(distortion_scale=0.6, p=1.0),</span><br><span class="line">            torchvision.transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),</span><br><span class="line">            torchvision.transforms.ToTensor(),</span><br><span class="line">        ])</span><br><span class="line">    # eval transform</span><br><span class="line">    print(&quot;eval transform&quot;)</span><br><span class="line">    return torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.Resize((args.input_size, args.input_size)),</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def build_dataset(is_train, args):</span><br><span class="line">    transform = build_transform(is_train, args)</span><br><span class="line">    path = os.path.join(args.root_path, &quot;train&quot; if is_train else &quot;test&quot;)</span><br><span class="line">    dataset = torchvision.datasets.ImageFolder(path, transform=transform)</span><br><span class="line">    info = dataset.find_classes(path)</span><br><span class="line">    print(f&quot;finding classes from &#123;path&#125;:\t &#123;info[0]&#125;&quot;)</span><br><span class="line">    print(f&quot;mapping classes from &#123;path&#125; to indexes:\t &#123;info[1]&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    return dataset</span><br></pre></td></tr></table></figure><h2 id="argparse">6 argparse</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_args_parser</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;MAE pre-training&#x27;</span>, add_help=<span class="literal">False</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">72</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, default=<span class="number">400</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--accum_iter&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Accumulate gradient iterations (for increasing the effective batch size under memory constraints)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Model parameters</span></span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--input_size&#x27;</span>, default=<span class="number">128</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;images input size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimizer parameters</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0001</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;weight decay (default: 0.0001)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0001</span>, metavar=<span class="string">&#x27;LR&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;learning rate (absolute lr)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dataset parameters</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--root-path&#x27;</span>, default=<span class="string">&#x27;./dataset_fruit_veg/&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where the train test pic is &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output_dir&#x27;</span>, default=<span class="string">&#x27;./output_dir_pretrained&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to save, empty for no saving&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--log_dir&#x27;</span>, default=<span class="string">&#x27;./output_dir_pretrained&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to tensorboard log&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--resume&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;resume from checkpoint&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--start_epoch&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;start epoch&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_workers&#x27;</span>, default=<span class="number">5</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--pin_mem&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--no_pin_mem&#x27;</span>, action=<span class="string">&#x27;store_false&#x27;</span>, dest=<span class="string">&#x27;pin_mem&#x27;</span>)</span><br><span class="line">    parser.set_defaults(pin_mem=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># distributed training parameters</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--world_size&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of distributed processes&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--local_rank&#x27;</span>, default=-<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dist_on_itp&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dist_url&#x27;</span>, default=<span class="string">&#x27;env://&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;url used to set up distributed training&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure><h2 id="主要代码">7 主要代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args, mode=<span class="string">&quot;train&quot;</span>, test_image_path=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;mode&#125;</span> mode...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建数据批次</span></span><br><span class="line">        dataset_train = build_dataset(is_train=<span class="literal">True</span>, args=args)</span><br><span class="line">        dataset_val = build_dataset(is_train=<span class="literal">False</span>, args=args)</span><br><span class="line"></span><br><span class="line">        sampler_train = torch.utils.data.RandomSampler(dataset_train)</span><br><span class="line">        sampler_val = torch.utils.data.SequentialSampler(dataset_val)</span><br><span class="line"></span><br><span class="line">        data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">            dataset_train, sampler=sampler_train,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">            pin_memory=args.pin_mem,</span><br><span class="line">            drop_last=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">            dataset_val, sampler=sampler_val,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            <span class="comment"># batch_size = 1</span></span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">            pin_memory=args.pin_mem,</span><br><span class="line">            drop_last=<span class="literal">False</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 构建模型</span></span><br><span class="line">        model = timm.create_model(<span class="string">&quot;resnet18&quot;</span>, pretrained=<span class="literal">True</span>, num_classes=<span class="number">36</span>, drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        n_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;number of trainable params (M):%.2f&#x27;</span> % (n_parameters / <span class="number">1.e6</span>))</span><br><span class="line"></span><br><span class="line">        criterion = nn.CrossEntropyLoss()</span><br><span class="line">        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log dir</span></span><br><span class="line">        os.makedirs(args.log_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        log_writer = SummaryWriter(log_dir=args.log_dir)</span><br><span class="line">        loss_scaler = NativeScaler()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读入已有的模型</span></span><br><span class="line">        misc.load_model(args=args, model_without_ddp=model, optimizer=optimizer, loss_scaler=loss_scaler)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch<span class="subst">&#123;epoch&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;length of data_loader_train is <span class="subst">&#123;<span class="built_in">len</span>(data_loader_train)&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Evaluating...&quot;</span>)</span><br><span class="line">                model.<span class="built_in">eval</span>()</span><br><span class="line">                test_stats = evaluate(data_loader_val, model, device)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Accuracy of the network on the <span class="subst">&#123;<span class="built_in">len</span>(dataset_val)&#125;</span> test images: <span class="subst">&#123;test_stats[<span class="string">&#x27;acc1&#x27;</span>]:<span class="number">.1</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    log_writer.add_scalar(<span class="string">&quot;perf/test_acc1&quot;</span>, test_stats[<span class="string">&quot;acc1&quot;</span>], epoch)</span><br><span class="line">                    log_writer.add_scalar(<span class="string">&quot;perf/test_acc5&quot;</span>, test_stats[<span class="string">&quot;acc5&quot;</span>], epoch)</span><br><span class="line">                    log_writer.add_scalar(<span class="string">&quot;perf/test_loss&quot;</span>, test_stats[<span class="string">&quot;loss&quot;</span>], epoch)</span><br><span class="line">                model.train()</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;training...&quot;</span>)</span><br><span class="line">            train_stats = train_one_epoch(</span><br><span class="line">                model, criterion, data_loader_train,</span><br><span class="line">                optimizer, device, epoch + <span class="number">1</span>, loss_scaler, <span class="literal">None</span>, log_writer=log_writer, args=args</span><br><span class="line">            )</span><br><span class="line">            <span class="built_in">print</span>(train_stats)</span><br><span class="line">            <span class="keyword">if</span> args.output_dir:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;saving checkpoint...&quot;</span>)</span><br><span class="line">                misc.save_model(</span><br><span class="line">                    args=args,model=model,model_without_ddp=model,optimizer=optimizer,</span><br><span class="line">                    loss_scaler=loss_scaler,epoch=epoch</span><br><span class="line">                )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model = timm.create_model(<span class="string">&quot;resnet18&quot;</span>, pretrained=<span class="literal">True</span>, num_classes=<span class="number">36</span>, drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        class_dict = &#123;<span class="string">&#x27;apple&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;banana&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;beetroot&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;bell pepper&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;cabbage&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;capsicum&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">                      <span class="string">&#x27;carrot&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;cauliflower&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;chilli pepper&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;corn&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;cucumber&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;eggplant&#x27;</span>: <span class="number">11</span>,</span><br><span class="line">                      <span class="string">&#x27;garlic&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;ginger&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;grapes&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;jalepeno&#x27;</span>: <span class="number">15</span>, <span class="string">&#x27;kiwi&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;lemon&#x27;</span>: <span class="number">17</span>, <span class="string">&#x27;lettuce&#x27;</span>: <span class="number">18</span>,</span><br><span class="line">                      <span class="string">&#x27;mango&#x27;</span>: <span class="number">19</span>, <span class="string">&#x27;onion&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;orange&#x27;</span>: <span class="number">21</span>, <span class="string">&#x27;paprika&#x27;</span>: <span class="number">22</span>, <span class="string">&#x27;pear&#x27;</span>: <span class="number">23</span>, <span class="string">&#x27;peas&#x27;</span>: <span class="number">24</span>, <span class="string">&#x27;pineapple&#x27;</span>: <span class="number">25</span>,</span><br><span class="line">                      <span class="string">&#x27;pomegranate&#x27;</span>: <span class="number">26</span>, <span class="string">&#x27;potato&#x27;</span>: <span class="number">27</span>, <span class="string">&#x27;raddish&#x27;</span>: <span class="number">28</span>, <span class="string">&#x27;soy beans&#x27;</span>: <span class="number">29</span>, <span class="string">&#x27;spinach&#x27;</span>: <span class="number">30</span>, <span class="string">&#x27;sweetcorn&#x27;</span>: <span class="number">31</span>,</span><br><span class="line">                      <span class="string">&#x27;sweetpotato&#x27;</span>: <span class="number">32</span>, <span class="string">&#x27;tomato&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;turnip&#x27;</span>: <span class="number">34</span>, <span class="string">&#x27;watermelon&#x27;</span>: <span class="number">35</span>&#125;</span><br><span class="line"></span><br><span class="line">        n_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;number of trainable params (M):%.2f&#x27;</span> % (n_parameters / <span class="number">1.e6</span>))</span><br><span class="line"></span><br><span class="line">        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">        os.makedirs(args.log_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        loss_scaler = NativeScaler()</span><br><span class="line"></span><br><span class="line">        misc.load_model(args=args, model_without_ddp=model, optimizer=optimizer, loss_scaler=loss_scaler)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        image = Image.<span class="built_in">open</span>(test_image_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        image = image.resize((args.input_size, args.input_size), Image.ANTIALIAS)</span><br><span class="line">        image = torchvision.transforms.ToTensor()(image).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model(image)</span><br><span class="line"></span><br><span class="line">        output = F.softmax(output, dim=-<span class="number">1</span>)</span><br><span class="line">        class_idx = torch.argmax(output, dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        score = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;image path is <span class="subst">&#123;test_image_path&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;score is <span class="subst">&#123;score.item()&#125;</span>, class id is <span class="subst">&#123;class_idx.item()&#125;</span>,  &quot;</span></span><br><span class="line">            <span class="string">f&quot;class name is <span class="subst">&#123;<span class="built_in">list</span>(class_dict.keys())[<span class="built_in">list</span>(class_dict.values()).index(class_idx)]&#125;</span>&quot;</span>)</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>分为两大部分：</p><ul><li>首先进行eval</li><li>然后进行训练</li></ul><h2 id="infer">8 infer</h2><ul><li>修改resume</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&#x27;--resume&#x27;, default=&#x27;./output_dir_pretrained/checkpoint-24.pth&#x27;,help=&#x27;resume from checkpoint&#x27;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;数据集介绍&quot;&gt;1 数据集介绍&lt;/h2&gt;
&lt;p&gt;https://aistudio.baidu.com/aistudio/datasetdetail/119023&lt;/p&gt;
&lt;p&gt;其中的图片有36各类，不同类别在不同的文件夹中，其中文件后缀有&quot;jpg,png,JPG</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络11-GAN</title>
    <link href="https://wangtongyouwen.github.io/post/cad0a038.html"/>
    <id>https://wangtongyouwen.github.io/post/cad0a038.html</id>
    <published>2023-04-24T11:36:29.000Z</published>
    <updated>2023-04-30T13:07:07.758Z</updated>
    
    <content type="html"><![CDATA[<h1 id="generative-adversarial-nets">Generative Adversarial Nets</h1><p>https://arxiv.org/pdf/1406.2661.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241952572.png" alt="image-20230424195239653" /><figcaption aria-hidden="true">image-20230424195239653</figcaption></figure><p>Pearson散度和Jensen-Shannon散度都是衡量两个概率分布之间差异的方法，但它们具有不同的计算方式和特点。</p><ol type="1"><li>Pearson散度（Pearson Divergence）： Pearson散度是衡量两个概率分布之间的差异的一种方法，主要基于卡方统计量（Chi-Square statistic）。对于两个概率分布P和Q，Pearson散度的计算公式如下：</li></ol><p>D_Pearson(P, Q) = Σ[(P(x) - Q(x))^2 / Q(x)]</p><p>其中，x表示数据空间中的元素，P(x)和Q(x)分别表示概率分布P和Q在x处的概率密度。</p><p>Pearson散度的值越大，表示两个概率分布之间的差异越大。需要注意的是，Pearson散度不是一种距离度量，因为它不满足三角不等式。</p><ol type="1"><li>Jensen-Shannon散度（Jensen-Shannon Divergence）： Jensen-Shannon散度是一种对称的、有界的散度度量方法，用于衡量两个概率分布之间的差异。它是基于Kullback-Leibler散度（KL散度）的改进版本。对于两个概率分布P和Q，Jensen-Shannon散度的计算公式如下：</li></ol><p>D_JS(P, Q) = (1/2) * D_KL(P, M) + (1/2) * D_KL(Q, M)</p><p>其中，M = (1/2) * (P + Q)，D_KL(P, M)和D_KL(Q, M)分别表示P和Q相对于M的Kullback-Leibler散度。</p><p>Jensen-Shannon散度的值在0到1之间，值越大表示两个概率分布之间的差异越大。当两个分布完全相同时，Jensen-Shannon散度为0；当两个分布完全不相交时，Jensen-Shannon散度接近1。可以通过计算Jensen-Shannon散度的平方根得到Jensen-Shannon距离，它满足距离度量的性质。</p><p>总结一下，Pearson散度和Jensen-Shannon散度都是衡量两个概率分布之间差异的方法，但它们的计算方式和特点不同。Pearson散度基于卡方统计量，而Jensen-Shannon散度基于Kullback-Leibler散度。Jensen-Shannon散度是对称的、有界的，可以通过计算其平方根得到满足距离度量性质的Jensen-Shannon距离。</p><h1 id="code">code</h1><h2 id="generator">1 Generator</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;基于 MNist 实现对抗生成网络(GAN)&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(latent_dim, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, np.prod(image_size)),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        output = self.model(z)</span><br><span class="line">        image = torch.reshape(output, (z.shape[<span class="number">0</span>], *image_size))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="discriminator">2 Discriminator</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(np.prod(image_size), <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image</span>):</span><br><span class="line">        prob = self.model(torch.reshape(image, (image.shape[<span class="number">0</span>], -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prob</span><br></pre></td></tr></table></figure><h2 id="training">3 training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line">image_size = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">latent_dim = <span class="number">100</span></span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.MNIST(<span class="string">&quot;mnist_data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                     transform=torchvision.transforms.Compose([</span><br><span class="line">                                         torchvision.transforms.Resize(<span class="number">28</span>),</span><br><span class="line">                                         torchvision.transforms.ToTensor(),</span><br><span class="line">                                         torchvision.transforms.Normalize(mean=[<span class="number">0.5</span>], std=[<span class="number">0.5</span>])</span><br><span class="line">                                     ]))</span><br><span class="line"></span><br><span class="line">DataLoader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">generator = Generator().cuda()</span><br><span class="line">discriminator = Discriminator().cuda()</span><br><span class="line"></span><br><span class="line">g_optimizer = torch.optim.Adam(params=generator.parameters(), lr=<span class="number">0.0002</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">d_optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=<span class="number">0.0002</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">raw = os.path.abspath(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">output_dir = os.path.join(raw, <span class="string">&quot;mnist_data/MNIST/result&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">    os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否存在预训练的权重文件</span></span><br><span class="line">generator_weights_path = os.path.join(output_dir, <span class="string">&#x27;generator.pth&#x27;</span>)</span><br><span class="line">discriminator_weights_path = os.path.join(output_dir, <span class="string">&#x27;discriminator.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(generator_weights_path) <span class="keyword">and</span> os.path.exists(discriminator_weights_path):</span><br><span class="line">    generator.load_state_dict(torch.load(generator_weights_path))</span><br><span class="line">    discriminator.load_state_dict(torch.load(discriminator_weights_path))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> i, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(DataLoader):</span><br><span class="line">        gt_images, _ = mini_batch</span><br><span class="line">        gt_images = gt_images.cuda()</span><br><span class="line">        z = torch.randn(batch_size, latent_dim)</span><br><span class="line">        z = z.cuda()</span><br><span class="line">        pred_images = generator(z)</span><br><span class="line">        pred_images = pred_images.cuda()</span><br><span class="line">        g_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        target_ones = torch.ones(batch_size, <span class="number">1</span>)</span><br><span class="line">        target_ones = target_ones.cuda()</span><br><span class="line">        target_zeros = torch.zeros(batch_size, <span class="number">1</span>)</span><br><span class="line">        target_zeros = target_zeros.cuda()</span><br><span class="line">        g_loss = loss_fn(discriminator(pred_images), target_ones)  <span class="comment"># 生成器</span></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        g_optimizer.step()</span><br><span class="line"></span><br><span class="line">        d_optimizer.zero_grad()</span><br><span class="line">        d_loss = loss_fn(discriminator(gt_images), target_ones) + loss_fn(discriminator(pred_images.detach()), target_zeros)  <span class="comment"># 判别器</span></span><br><span class="line">        d_loss.backward()</span><br><span class="line">        d_optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            raw = os.path.abspath(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">            output_dir = os.path.join(raw, <span class="string">&quot;mnist_data/MNIST/result&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">                os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">            torchvision.utils.save_image(pred_images, os.path.join(output_dir, <span class="string">f&quot;image_<span class="subst">&#123;epoch&#125;</span>_<span class="subst">&#123;i // <span class="number">1000</span>&#125;</span>.png&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch&#125;</span>, Batch: <span class="subst">&#123;i&#125;</span>, Generator Loss: <span class="subst">&#123;g_loss.item()&#125;</span>, Discriminator Loss: <span class="subst">&#123;d_loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型权重</span></span><br><span class="line">    torch.save(generator.state_dict(), generator_weights_path)</span><br><span class="line">    torch.save(discriminator.state_dict(), discriminator_weights_path)</span><br></pre></td></tr></table></figure><h1 id="cgan">CGAN</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304262015876.png" alt="image-20230426201511591" /><figcaption aria-hidden="true">image-20230426201511591</figcaption></figure><p>CGAN（条件生成对抗网络，Conditional GAN）是在GAN的基础上引入条件信息的一种变体。与普通GAN相比，CGAN的生成器和判别器都接收额外的条件信息（如类别标签、文本描述等），并根据这些条件信息生成特定类别的数据。这使得CGAN能够有更好的控制生成数据的特征。</p><p>CGAN的特点如下：</p><ol type="1"><li>控制生成数据特征：CGAN可以根据输入的条件信息，生成具有特定特征的数据。这使得CGAN在生成数据时具有更高的可控性。</li><li>引入条件信息：CGAN的生成器和判别器都接收额外的条件信息，使得网络可以在训练过程中学习如何利用这些条件信息来生成和识别数据。</li><li>更好的生成性能：由于条件信息的引入，CGAN可以在生成数据时更好地捕捉数据的特征和结构，从而提高生成数据的质量。</li></ol><p>总之，与普通GAN相比，CGAN通过引入条件信息，实现了对生成数据特征的控制，使得生成数据更具有可控性和更高的质量。在很多应用场景中，如图像生成、文本生成等，CGAN已经表现出很好的性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">10</span>,label_emb_dim)</span><br><span class="line">       ...</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z,labels</span>):</span><br><span class="line">        <span class="comment"># shape of z: [batchsize,latent_dim]</span></span><br><span class="line">        label_embedding = self.embedding(labels)</span><br><span class="line">        z = torch.cat([z,label_embedding], axis=-<span class="number">1</span>)</span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">10</span>,label_emb_dim)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image,labels</span>):</span><br><span class="line">        <span class="comment"># shape of image [batchsize,1,28,28]</span></span><br><span class="line">        label_embedding = self.embedding(labels)</span><br><span class="line">        prob = self.model(torch.cat([torch.reshape(image, (image.shape[<span class="number">0</span>], -<span class="number">1</span>)),label_embedding],axis=-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在训练过程中需要对generator和discriminator的调用中引入参数label</span></span><br><span class="line">gt_images, labels = mini_batch</span><br></pre></td></tr></table></figure><h1 id="least-squares-gan">least-squares-GAN</h1><p>Least Squares Generative Adversarial Networks（LSGAN）是一种生成对抗网络（GAN）的变体，它主要针对传统GAN在训练过程中容易出现的不稳定性和梯度消失问题进行了改进。LSGAN的核心思想是将判别器（Discriminator）的损失函数从交叉熵损失（Cross-Entropy Loss）替换为最小二乘损失（Least Squares Loss），从而提高训练的稳定性和生成数据的质量。</p><p>LSGAN的主要特点如下：</p><ol type="1"><li>稳定性：与传统GAN相比，LSGAN在训练过程中表现出更高的稳定性。这主要归功于最小二乘损失函数的平滑性，它能够减少梯度消失问题的发生，使得生成器（Generator）和判别器（Discriminator）在训练过程中保持更好的平衡。</li><li>生成质量：LSGAN生成的数据质量通常优于传统GAN。最小二乘损失有助于判别器更好地区分真实数据和生成数据，从而为生成器提供更有效的梯度指导，使得生成器能够生成更高质量的数据。</li><li>更低的梯度消失风险：在传统GAN中，由于交叉熵损失在判别器接近最优时可能导致梯度消失问题，这使得生成器的训练变得困难。然而，在LSGAN中，最小二乘损失能够减轻梯度消失问题，使得生成器在整个训练过程中都能够收到有效的梯度信息。</li><li>容易实现：LSGAN的实现非常简单，只需将传统GAN的损失函数替换为最小二乘损失即可。这意味着在现有的GAN架构中，很容易将其升级为LSGAN。</li></ol><p>总之，LSGAN通过使用最小二乘损失改进了传统GAN的训练稳定性和生成数据质量。这使得LSGAN在各种生成任务中，如图像生成、文本生成等，都能取得更好的性能。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304262051780.png" alt="image-20230426205146063" /><figcaption aria-hidden="true">image-20230426205146063</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">logits = torch.linspace(-<span class="number">10</span>,<span class="number">10</span>,<span class="number">2000</span>)</span><br><span class="line">loss = []</span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"><span class="keyword">for</span> lgs <span class="keyword">in</span> logits:</span><br><span class="line">    loss.append(loss_fn(torch.sigmoid(lgs),torch.ones_like(lgs)))</span><br><span class="line"></span><br><span class="line">plt.plot(logits,loss)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304262107840.png" alt="image-20230426210738626" /><figcaption aria-hidden="true">image-20230426210738626</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;generative-adversarial-nets&quot;&gt;Generative Adversarial Nets&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/pdf/1406.2661.pdf&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门12-位置编码</title>
    <link href="https://wangtongyouwen.github.io/post/f94e9029.html"/>
    <id>https://wangtongyouwen.github.io/post/f94e9029.html</id>
    <published>2023-04-24T08:26:35.000Z</published>
    <updated>2023-04-30T06:10:14.229Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241635006.png" alt="image-20230424163501351" /><figcaption aria-hidden="true">image-20230424163501351</figcaption></figure><h1 id="transformer">1 transformer</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241637416.png" alt="image-20230424163709530" /><figcaption aria-hidden="true">image-20230424163709530</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.1d absolute sincos constant embedding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_1d_absolute_sincos_embeddings</span>(<span class="params">n_pos_vec,dim</span>):</span><br><span class="line">    <span class="comment"># pos_vec: torch.arrange(n_pos)</span></span><br><span class="line">    <span class="keyword">assert</span> dim % <span class="number">2</span> ==<span class="number">0</span>,<span class="string">&quot;wrong dimension&quot;</span></span><br><span class="line">    position_embedding = torch.zeros(n_pos_vec.numel(),dim,dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    omega = torch.arange(dim//<span class="number">2</span>,dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    omega /= dim / <span class="number">2.</span></span><br><span class="line">    omega = <span class="number">1.</span> / (<span class="number">10000</span> ** omega)</span><br><span class="line"></span><br><span class="line">    out = n_pos_vec[:,<span class="literal">None</span>] @ omega[<span class="literal">None</span>,:] <span class="comment"># [n_pos_vec,1]*[1,dim//2]</span></span><br><span class="line"></span><br><span class="line">    emb_sin = torch.sin(out)</span><br><span class="line">    emb_cos = torch.cos(out)</span><br><span class="line"></span><br><span class="line">    position_embedding[:,<span class="number">0</span>::<span class="number">2</span>] = emb_sin</span><br><span class="line">    position_embedding[:,<span class="number">1</span>::<span class="number">2</span>] = emb_cos</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>  position_embedding</span><br></pre></td></tr></table></figure><h1 id="swin-transformer">2 Swin transformer</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241914252.png" alt="image-20230424191316681" /><figcaption aria-hidden="true">image-20230424191316681</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.2d relative bias trainable embedding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_2d_relative_bias_trainable_embeddings</span>(<span class="params">n_head,height,width</span>):</span><br><span class="line">    <span class="comment"># width:5   bias=[-width+1, width-1]  2*width-1</span></span><br><span class="line">    <span class="comment"># height:5                            2*height-1</span></span><br><span class="line">    position_embedding = nn.Embedding((<span class="number">2</span>*width-<span class="number">1</span>)*(<span class="number">2</span>*height-<span class="number">1</span>),n_head)</span><br><span class="line">    nn.init.constant_(position_embedding,<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_relative_position_index</span>(<span class="params">height,width</span>):</span><br><span class="line">        cords = torch.stack(torch.meshgrid(torch.arange(height),torch.arange(width))) <span class="comment">#[2,height,width]</span></span><br><span class="line">        cords_flatten = torch.flatten(cords,<span class="number">1</span>) <span class="comment"># [2,height*width]</span></span><br><span class="line">        relative_cords_bias = cords_flatten[:,:,<span class="literal">None</span>] - cords_flatten[:,<span class="literal">None</span>,:] <span class="comment">#[2,height*width,height*width]</span></span><br><span class="line"></span><br><span class="line">        relative_cords_bias[<span class="number">0</span>,:,:] += height - <span class="number">1</span></span><br><span class="line">        relative_cords_bias[<span class="number">1</span>,:,:] += width - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># A:2d,B:1d B[i*cols+j] = a[i,j]</span></span><br><span class="line"></span><br><span class="line">        relative_cords_bias[<span class="number">0</span>,:,:] *= relative_cords_bias[<span class="number">1</span>,:,:].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  relative_cords_bias.<span class="built_in">sum</span>(<span class="number">0</span>) <span class="comment"># [height*width,height*width]</span></span><br><span class="line">    relative_position_bias = get_relative_position_index(height,width)</span><br><span class="line">    bias_embedding = position_embedding(torch.flatten(relative_position_bias)).reshape(height*width,height*width,n_head) <span class="comment"># [height*width,height*width,n_head]</span></span><br><span class="line"></span><br><span class="line">    bias_embedding.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>).unsqueeze(<span class="number">0</span>) <span class="comment"># [1,n_head,height*width,height*width]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bias_embedding</span><br></pre></td></tr></table></figure><h1 id="masked-ae">3 Masked AE</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.2d absolute constant sincos embedding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_2d_absolute_sincos_embeddings</span>(<span class="params">height,width,dim</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> dim % <span class="number">4</span> == <span class="number">0</span>, <span class="string">&quot;wrong dimension&quot;</span></span><br><span class="line">    position_embedding = torch.zeros(height*width,dim)</span><br><span class="line">    cords = torch.stack(torch.meshgrid(torch.arange(height,dtype=torch.<span class="built_in">float</span>), torch.arange(width,dtype=torch.<span class="built_in">float</span>)))  <span class="comment"># [2,height,width]</span></span><br><span class="line">    height_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(cords[<span class="number">0</span>]),dim//<span class="number">2</span>) <span class="comment"># [height*width,dim//2]</span></span><br><span class="line">    width_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(cords[<span class="number">1</span>]),dim//<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    position_embedding[:,:dim//<span class="number">2</span>] = height_embedding</span><br><span class="line">    position_embedding[:,dim//<span class="number">2</span>:] = width_embedding</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> position_embedding</span><br></pre></td></tr></table></figure><h1 id="section"></h1>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304241635006.png&quot; alt=&quot;image-20230424163501351&quot; /&gt;&lt;fig</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门11-Normalization</title>
    <link href="https://wangtongyouwen.github.io/post/87a49949.html"/>
    <id>https://wangtongyouwen.github.io/post/87a49949.html</id>
    <published>2023-04-20T10:24:36.000Z</published>
    <updated>2023-04-30T06:10:05.146Z</updated>
    
    <content type="html"><![CDATA[<h1 id="layer-normalization">Layer Normalization</h1><p><a href="https://arxiv.org/pdf/1607.06450.pdf">1607.06450.pdf (arxiv.org)</a></p><p>这篇文章讨论了一种称为层归一化（Layer Normalization）的方法，用于稳定神经网络的训练过程，并减少训练时间。层归一化的核心思想是通过对单个训练样本的神经元输入求和来计算归一化所需的均值和方差。这与之前的批量归一化（Batch Normalization）方法不同，后者依赖于一个 mini-batch 中所有样本的输入分布。</p><p>以下是该论文的主要观点：</p><ol type="1"><li>层归一化将批量归一化的概念从 mini-batch 级别扩展到层级别，计算用于归一化的均值和方差来自单个训练样本中的所有神经元输入求和。</li><li>与批量归一化类似，层归一化也为每个神经元提供自适应的偏置和增益，这些参数在归一化之后但在非线性激活函数之前应用。</li><li>与批量归一化不同，层归一化在训练和测试阶段执行相同的计算，因此不依赖于 mini-batch 大小。</li><li>层归一化很容易应用于循环神经网络（RNN），因为可以在每个时间步骤单独计算归一化统计数据。</li><li>层归一化在稳定循环神经网络的隐藏状态动态方面非常有效。</li><li>实证研究表明，与以前发布的技术相比，层归一化可以显著减少训练时间。</li></ol><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202124984.png" alt="image-20230420212422293" /><figcaption aria-hidden="true">image-20230420212422293</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202123289.png" alt="image-20230420212333495" /><figcaption aria-hidden="true">image-20230420212333495</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202229447.png" alt="image-20230420222926884" /><figcaption aria-hidden="true">image-20230420222926884</figcaption></figure><h1 id="五种归一化的代码实现">五种归一化的代码实现</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202236940.png" alt="image-20230420223608053" /><figcaption aria-hidden="true">image-20230420223608053</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 首先定义一些常量</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">time_steps = <span class="number">3</span></span><br><span class="line">embedding_dim = <span class="number">4</span></span><br><span class="line">input_x = torch.randn(batch_size,time_steps,embedding_dim)</span><br></pre></td></tr></table></figure><h2 id="batch-normalization">1 batch normalization</h2><p>torch.nn.BatchNorm1d(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm1d</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304202240276.png" alt="parameter and shape" /><figcaption aria-hidden="true">parameter and shape</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 官方API</span></span><br><span class="line"><span class="comment"># nlp: [N,L,C] -&gt; [C]</span></span><br><span class="line"><span class="comment"># cv: [N,C,H,W] -&gt; [C]</span></span><br><span class="line">batch_norm_op = nn.BatchNorm1d(embedding_dim,affine=<span class="literal">False</span>)</span><br><span class="line">bn_y = batch_norm_op(input_x.transpose(-<span class="number">1</span>,-<span class="number">2</span>)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(bn_y)</span><br><span class="line"><span class="comment"># 手写batch_norm</span></span><br><span class="line">bn_mean = input_x.mean(dim=(<span class="number">0</span>,<span class="number">1</span>)).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).repeat(batch_size,time_steps,<span class="number">1</span>)</span><br><span class="line">bn_std = input_x.std(dim=(<span class="number">0</span>,<span class="number">1</span>),unbiased=<span class="literal">False</span>).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).repeat(batch_size,time_steps,<span class="number">1</span>)</span><br><span class="line">verify_bn_y = (input_x-bn_mean)/(bn_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_bn_y)</span><br></pre></td></tr></table></figure><h2 id="layer-normalization-1">2 layer normalization</h2><p>torch.nn.LayerNorm(<em>normalized_shape</em>, <em>eps=1e-05</em>, <em>elementwise_affine=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html?highlight=layer+norm#torch.nn.LayerNorm</p><ul><li>需要保证batchsize是第一个维度，剩下的维度中有embedding_dim即可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现layer_norm并验证API</span></span><br><span class="line"><span class="comment"># nlp: [N,L,C] -&gt; [N,L]</span></span><br><span class="line"><span class="comment"># Cv: [N,C,H,W] -&gt; [N,H,W]</span></span><br><span class="line">layer_norm_op = nn.LayerNorm(embedding_dim,elementwise_affine=<span class="literal">False</span>)</span><br><span class="line">ln_y = layer_norm_op(input_x)</span><br><span class="line"><span class="built_in">print</span>(ln_y)</span><br><span class="line"><span class="comment"># shouxie  layer_norm</span></span><br><span class="line">ln_mean = input_x.mean(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">ln_std = input_x.std(dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>,unbiased=<span class="literal">False</span>)</span><br><span class="line">verify_ln_y = (input_x-ln_mean)/(ln_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_ln_y)</span><br></pre></td></tr></table></figure><h2 id="instance-normalization">3 Instance Normalization</h2><ul><li>一般用于风格迁移中</li></ul><p>https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html?highlight=instance#torch.nn.InstanceNorm1d</p><p>torch.nn.InstanceNorm1d(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em>, <em>device=None</em>, <em>dtype=None</em>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用instance norm API</span></span><br><span class="line"><span class="comment"># nlp: [N,L,C] -&gt; [N,C]</span></span><br><span class="line"><span class="comment"># Cv: [N,C,H,W] -&gt; [N,C]</span></span><br><span class="line">ins_norm_op = nn.InstanceNorm1d(embedding_dim)</span><br><span class="line">in_y = ins_norm_op(input_x.transpose(-<span class="number">1</span>,-<span class="number">2</span>)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(in_y)</span><br><span class="line"><span class="comment"># 手写ins_norm</span></span><br><span class="line">in_mean = input_x.mean(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">in_std = input_x.std(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>,unbiased=<span class="literal">False</span>)</span><br><span class="line">verify_ins_y = (input_x-in_mean)/(in_std+<span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_ins_y)</span><br></pre></td></tr></table></figure><ul><li>因为在同一个mini-batch中，如果在序列上对其加权平均，最后得到的也就是序列中的风格信息。</li></ul><h2 id="group-normalization">4 group normalization</h2><p>与layer norm比较相同</p><p>torch.nn.GroupNorm(<em>num_groups</em>, <em>num_channels</em>, <em>eps=1e-05</em>, <em>affine=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html?highlight=group+norm#torch.nn.GroupNorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用group norm API</span></span><br><span class="line"><span class="comment"># nlp: [N,G,L,C/G] -&gt; [N,G]</span></span><br><span class="line"><span class="comment"># Cv: [N,G,C/G,H,W] -&gt; [N,G]</span></span><br><span class="line">group_norm_op = nn.GroupNorm(num_group,embedding_dim,affine=<span class="literal">False</span>)</span><br><span class="line">gn_y = group_norm_op(input_x.transpose(-<span class="number">1</span>,-<span class="number">2</span>)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(gn_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写 group norm</span></span><br><span class="line">group_input_x = torch.split(input_x,split_size_or_sections=embedding_dim//num_group,dim=-<span class="number">1</span>)</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> g_inputx <span class="keyword">in</span> group_input_x:</span><br><span class="line">    gn_mean = g_inputx.mean(dim=(<span class="number">1</span>,<span class="number">2</span>),keepdim=<span class="literal">True</span>)</span><br><span class="line">    gn_std = g_inputx.std(dim=(<span class="number">1</span>,<span class="number">2</span>),keepdim=<span class="literal">True</span>,unbiased=<span class="literal">False</span>)</span><br><span class="line">    gn_result = (g_inputx-gn_mean)/(gn_std+<span class="number">1e-5</span>)</span><br><span class="line">    results.append(gn_result)</span><br><span class="line">verify_gn_y = torch.cat(results,dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(verify_gn_y)</span><br></pre></td></tr></table></figure><h2 id="weight-normalization">5 Weight Normalization</h2><p>torch.nn.utils.weight_norm(<em>module</em>, <em>name='weight'</em>, <em>dim=0</em>) https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#weight_norm <span class="math display">\[w = g\frac{v}{||v||}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现weight_norm并验证API</span></span><br><span class="line">linear = nn.Linear(embedding_dim,<span class="number">3</span>,bias=<span class="literal">False</span>) <span class="comment"># without weight norm</span></span><br><span class="line">wn_linear = torch.nn.utils.weight_norm(linear)</span><br><span class="line">wn_linear_output = wn_linear(input_x)</span><br><span class="line"><span class="built_in">print</span>(wn_linear_output)</span><br><span class="line"><span class="keyword">for</span> i,k <span class="keyword">in</span> <span class="built_in">enumerate</span>(wn_linear.named_parameters()):</span><br><span class="line">    <span class="built_in">print</span>(i,k)</span><br><span class="line"><span class="comment"># 手写实现 weight norm</span></span><br><span class="line">weight_direction = linear.weight/(linear.weight.norm(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>))</span><br><span class="line">weight_magnitude = wn_linear.weight_g</span><br><span class="line">verify_wn_linear_output = input_x @ (weight_direction.transpose(-<span class="number">1</span>,-<span class="number">2</span>) * weight_magnitude.transpose(-<span class="number">1</span>,-<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(verify_wn_linear_output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">feat_dim = <span class="number">3</span></span><br><span class="line">hid_dim = <span class="number">4</span></span><br><span class="line">inputx = torch.randn(batch_size, feat_dim)</span><br><span class="line">linear = nn.Linear(feat_dim, hid_dim, bias=<span class="literal">False</span>)</span><br><span class="line">wn_linear = torch.nn.utils.weight_norm(linear)</span><br><span class="line"></span><br><span class="line">weight_magnitude = torch.tensor([linear.weight[i, :].norm() <span class="keyword">for</span> i <span class="keyword">in</span> torch.arange(linear.weight.shape[<span class="number">0</span>])],</span><br><span class="line">                                dtype=torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">weight_direction = linear.weight / weight_magnitude</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear.weight:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(linear.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear.magnitude:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_magnitude)  <span class="comment"># 幅度向量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear.direction:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_direction)  <span class="comment"># 单位向量，表示方向</span></span><br><span class="line"><span class="built_in">print</span>((weight_direction ** <span class="number">2</span>).<span class="built_in">sum</span>(dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight_direction*weight_magnitude:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_direction * weight_magnitude)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;inputx @ (weight_direction * weight_magnitude).T:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(inputx @ (weight_direction * weight_magnitude).T)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear(inputx):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(linear(inputx))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;wn_linear(inputx:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(wn_linear(inputx))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters of wn_linear:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> n, p <span class="keyword">in</span> wn_linear.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(n, p)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;construct weight of linear:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(wn_linear.weight_g * (wn_linear.weight_v / torch.tensor(</span><br><span class="line">    [wn_linear.weight_v[i, :].norm() <span class="keyword">for</span> i <span class="keyword">in</span> torch.arange(wn_linear.weight_v.shape[<span class="number">0</span>])],dtype=torch.float32).unsqueeze(-<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;layer-normalization&quot;&gt;Layer Normalization&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.06450.pdf&quot;&gt;1607.06450.pdf (arxiv.org)&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络10-MAE</title>
    <link href="https://wangtongyouwen.github.io/post/f0f056f4.html"/>
    <id>https://wangtongyouwen.github.io/post/f0f056f4.html</id>
    <published>2023-04-19T13:21:04.000Z</published>
    <updated>2023-04-30T06:09:49.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="masked-autoencoders-are-scalable-vision-learners">Masked Autoencoders Are Scalable Vision Learners</h1><p><a href="https://arxiv.org/pdf/2111.06377.pdf">2111.06377.pdf (arxiv.org)</a></p><p>本文表明，掩蔽自编码器（MAE）是计算机视觉领域可扩展的自监督学习方法。我们的 MAE 方法很简单：我们随机遮盖输入图像的一部分区域，并重建丢失的像素。该方法基于两个核心设计。首先，我们开发了一种非对称的编码器-解码器架构，其中编码器仅对可见子图像区域（没有遮盖标记）进行操作，而轻量级的解码器则从潜在表示和遮盖标记中重建原始图像。其次，我们发现遮盖输入图像较大比例（例如 75%）会产生一个具有意义且难度适中的自监督任务。将这两个设计结合起来，使我们能够高效、有效地训练大型模型：我们加速训练（至少提高 3 倍）并提高准确性。我们的可扩展方法允许学习高容量模型以实现更好的泛化：例如，一个普通的 ViT-Huge 模型在仅使用 ImageNet-1K 数据的方法中达到了最高准确率（87.8%）。在下游任务中，迁移性能超过了有监督预训练，并展示出有希望的扩展行为。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304192127666.png" alt="image-20230419212658615" /><figcaption aria-hidden="true">image-20230419212658615</figcaption></figure><p>image-&gt;patch-&gt;random mask-&gt;shuffle the patch-&gt;encoder-&gt;combine the whole embedding-&gt;unshuffle to align all tokens with target-&gt;decoder</p><p>自编码器是一种可以重构原始信号的方法，它有一个编码器，将观察到的信号映射到潜在表示，以及一个解码器，从潜在表示和掩码标记中重构原始信号。与传统的自编码器不同，我们采用了一种不对称的设计，允许编码器仅对部分观察到的信号（不带掩码标记）进行操作，并使用轻量级解码器从潜在表示和掩码标记中重构完整信号。</p><h2 id="mask">mask</h2><p>其中对于掩码的部分，并不是空向量，而是一个可以学习的向量，可以用过对其学习而恢复完整信号。因为图片中存在大量的冗余信息，所以这个掩码比例通常很高，比如75%</p><p>作者将图像分成规则的非重叠块，然后对其进行采样并遮蔽剩余的块。他们的采样策略很简单：随机采样块，不重复，遵循均匀分布。高遮蔽比率的随机采样大大减少了冗余，从而创造了一个不能通过可见邻近块外推来轻松解决的任务。均匀分布防止了潜在的中心偏差（即图像中心附近有更多的遮蔽块）。最后，高度稀疏的输入为设计高效编码器提供了机会。</p><p>重要的是，不需要任何<strong>专门的稀疏操作</strong>。首先，我们为每个输入图像块生成一个标记（通过线性投影并添加<strong>位置嵌入</strong>）。接下来，我们随机打乱标记列表，并根据遮盖比例删除列表的最后一部分。这个过程为<strong>编码器产生了一个小的标记子集</strong>，相当于在不重复采样的情况下采样图像块。编码后，我们将一列遮盖标记添加到编码后的图像块列表中，并对整个列表进行反打乱操作（反转随机打乱操作）以使所有标记与其目标对齐。解码器应用于这个完整列表（添加位置嵌入）。如前所述，不需要稀疏操作。这种简单的实现引入的开销可忽略不计，因为洗牌和反洗牌操作很快。</p><h2 id="encoder">encoder</h2><p>这个编码器类似ViT的结构，但仅应用于可见的、未遮盖的图像块。与标准的 ViT 一样，我们的编码器通过线性投影和添加位置嵌入对图像块进行嵌入，然后通过一系列 Transformer 模块处理生成的集合。然而，我们的编码器仅在完整集合的一小部分（例如，25%）上进行操作。遮盖的图像块被移除；不使用遮盖标记。这使我们能够仅用一部分计算和内存资源训练非常大的编码器。</p><h2 id="decoder">decoder</h2><p>MAE解码器的输入是完整的标记集合，包括：（i）编码后的可见图像块，以及（ii）遮盖标记。每个遮盖标记是一个共享的、可学习的向量，表示需要预测的缺失图像块的存在。我们为完整集合中的所有标记添加位置嵌入；如果没有这个，遮盖标记将无法获取关于它们在图像中的位置的信息。</p><p>MAE 解码器仅在预训练阶段(回归任务)用于执行图像重建任务（只有编码器用于生成图像表示以进行识别）。因此，解码器架构可以灵活地设计，其设计方式独立于编码器设计。我们尝试使用非常小的解码器，比编码器更窄、更浅。例如，我们默认的解码器每个标记的计算量不到编码器的10%。借助这种非对称设计，完整的标记集合仅由轻量级解码器处理，从而大幅减少预训练时间。</p><ul><li>解码器的目标是完成这个自回归任务，是为了更好的获得存在掩码的编码器，通过编码器才能完成cv的常见任务。</li><li>解码器输出中的每个元素都是代表一个图像块的像素值向量。解码器的最后一层是线性投影，其输出通道数量等于图像块中的像素值数量</li><li>损失函数计算像素空间中重建图像与原始图像之间的均方误差(MSE),仅在遮盖的图像块上计算损失</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304192302724.png" alt="image-20230419230225444" /><figcaption aria-hidden="true">image-20230419230225444</figcaption></figure><h1 id="code">code</h1><p>https://github.com/facebookresearch/mae</p><h2 id="models_mae">1 models_mae</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201412692.png" alt="image-20230420141204420" /><figcaption aria-hidden="true">image-20230420141204420</figcaption></figure><h3 id="模型搭建">1.1 模型搭建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedAutoencoderViT</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Masked Autoencoder with VisionTransformer backbone</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">1024</span>, depth=<span class="number">24</span>, num_heads=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, norm_layer=nn.LayerNorm, norm_pix_loss=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># MAE encoder specifics</span></span><br><span class="line">        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)</span><br><span class="line">        num_patches = self.patch_embed.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line"></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># MAE decoder specifics</span></span><br><span class="line">        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.mask_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, decoder_embed_dim))</span><br><span class="line"></span><br><span class="line">        self.decoder_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, decoder_embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line"></span><br><span class="line">        self.decoder_blocks = nn.ModuleList([</span><br><span class="line">            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(decoder_depth)])</span><br><span class="line"></span><br><span class="line">        self.decoder_norm = norm_layer(decoder_embed_dim)</span><br><span class="line">        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**<span class="number">2</span> * in_chans, bias=<span class="literal">True</span>) <span class="comment"># decoder to patch</span></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">        self.norm_pix_loss = norm_pix_loss</span><br><span class="line"></span><br><span class="line">        self.initialize_weights()</span><br></pre></td></tr></table></figure><h3 id="embedding的构造">1.2 embedding的构造</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span>, flatten=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]</span><br><span class="line">        self.flatten = flatten</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="初始化参数">1.3 初始化参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># initialization</span></span><br><span class="line">    <span class="comment"># initialize (and freeze) pos_embed by sin-cos embedding</span></span><br><span class="line">    pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">    self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">    self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize patch_embed like nn.Linear (instead of nn.Conv2d)</span></span><br><span class="line">    w = self.patch_embed.proj.weight.data</span><br><span class="line">    torch.nn.init.xavier_uniform_(w.view([w.shape[<span class="number">0</span>], -<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># timm&#x27;s trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)</span></span><br><span class="line">    torch.nn.init.normal_(self.cls_token, std=<span class="number">.02</span>)</span><br><span class="line">    torch.nn.init.normal_(self.mask_token, std=<span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize nn.Linear and nn.LayerNorm</span></span><br><span class="line">    self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">        <span class="comment"># we use xavier_uniform following official JAX ViT:</span></span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">        nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><ul><li>copy?</li></ul><p><code>pos_embed</code> 是一个位置嵌入矩阵，用于捕捉序列中元素的相对或绝对位置信息。在 Transformer 网络中，位置嵌入用于将位置信息与输入嵌入相结合，从而帮助模型处理输入序列。</p><p><code>get_2d_sincos_pos_embed</code> 函数生成了一个基于正弦和余弦函数的二维位置嵌入矩阵。在这种情况下，<code>pos_embed</code> 是一个预先初始化的 PyTorch 张量，而 <code>get_2d_sincos_pos_embed</code> 返回的是一个 NumPy 数组。</p><p><code>copy_()</code> 函数的目的是将 NumPy 数组的值复制到预先分配的 PyTorch 张量中。直接将值赋给 <code>pos_embed</code> 变量会导致以下问题：</p><ol type="1"><li><p>数据类型不匹配：<code>get_2d_sincos_pos_embed</code> 返回的 NumPy 数组可能具有与 PyTorch 张量不同的数据类型。使用 <code>copy_()</code> 函数可以确保在复制过程中自动执行必要的类型转换。在这里，<code>.float().unsqueeze(0)</code> 用于将 NumPy 数组转换为 PyTorch 张量，并确保其具有正确的维度和数据类型。</p></li><li><p>张量的引用问题：直接将值赋给 <code>pos_embed</code> 变量可能会更改原始张量的引用，这可能会导致意外的行为。<code>copy_()</code> 函数确保只有张量的值被修改，而不更改其引用。这对于在模型中保持预期的参数更新行为很重要。</p></li></ol><p>综上所述，使用 <code>copy_()</code> 函数将位置嵌入矩阵的值复制到预先分配的 PyTorch 张量中，可以确保正确处理数据类型转换，并在保持张量引用不变的情况下更新张量的值。</p><h3 id="patch-and-unpatch">1.4 patch and unpatch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">patchify</span>(<span class="params">self, imgs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: (N, 3, H, W)</span></span><br><span class="line"><span class="string">    x: (N, L, patch_size**2 *3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p = self.patch_embed.patch_size[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> imgs.shape[<span class="number">2</span>] == imgs.shape[<span class="number">3</span>] <span class="keyword">and</span> imgs.shape[<span class="number">2</span>] % p == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    h = w = imgs.shape[<span class="number">2</span>] // p</span><br><span class="line">    x = imgs.reshape(shape=(imgs.shape[<span class="number">0</span>], <span class="number">3</span>, h, p, w, p))</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nchpwq-&gt;nhwpqc&#x27;</span>, x)</span><br><span class="line">    x = x.reshape(shape=(imgs.shape[<span class="number">0</span>], h * w, p**<span class="number">2</span> * <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unpatchify</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x: (N, L, patch_size**2 *3)</span></span><br><span class="line"><span class="string">    imgs: (N, 3, H, W)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p = self.patch_embed.patch_size[<span class="number">0</span>]</span><br><span class="line">    h = w = <span class="built_in">int</span>(x.shape[<span class="number">1</span>]**<span class="number">.5</span>)</span><br><span class="line">    <span class="keyword">assert</span> h * w == x.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    x = x.reshape(shape=(x.shape[<span class="number">0</span>], h, w, p, p, <span class="number">3</span>))</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)</span><br><span class="line">    imgs = x.reshape(shape=(x.shape[<span class="number">0</span>], <span class="number">3</span>, h * p, h * p))</span><br><span class="line">    <span class="keyword">return</span> imgs</span><br></pre></td></tr></table></figure><h3 id="掩码构建">1.5 掩码构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Perform per-sample random masking by per-sample shuffling.</span></span><br><span class="line"><span class="string">    Per-sample shuffling is done by argsort random noise.</span></span><br><span class="line"><span class="string">    x: [N, L, D], sequence</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, L, D = x.shape  <span class="comment"># batch, length, dim</span></span><br><span class="line">    len_keep = <span class="built_in">int</span>(L * (<span class="number">1</span> - mask_ratio))</span><br><span class="line">    </span><br><span class="line">    noise = torch.rand(N, L, device=x.device)  <span class="comment"># noise in [0, 1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sort noise for each sample</span></span><br><span class="line">    ids_shuffle = torch.argsort(noise, dim=<span class="number">1</span>)  <span class="comment"># ascend: small is keep, large is remove</span></span><br><span class="line">    ids_restore = torch.argsort(ids_shuffle, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep the first subset</span></span><br><span class="line">    ids_keep = ids_shuffle[:, :len_keep]</span><br><span class="line">    x_masked = torch.gather(x, dim=<span class="number">1</span>, index=ids_keep.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, D))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate the binary mask: 0 is keep, 1 is remove</span></span><br><span class="line">    mask = torch.ones([N, L], device=x.device)</span><br><span class="line">    mask[:, :len_keep] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># unshuffle to get the binary mask</span></span><br><span class="line">    mask = torch.gather(mask, dim=<span class="number">1</span>, index=ids_restore)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_masked, mask, ids_restore</span><br></pre></td></tr></table></figure><h3 id="encoder-forward">1.6 encoder forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_encoder</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="comment"># embed patches</span></span><br><span class="line">    x = self.patch_embed(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed w/o cls token</span></span><br><span class="line">    x = x + self.pos_embed[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># masking: length -&gt; length * mask_ratio</span></span><br><span class="line">    x, mask, ids_restore = self.random_masking(x, mask_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># append cls token</span></span><br><span class="line">    cls_token = self.cls_token + self.pos_embed[:, :<span class="number">1</span>, :]  <span class="comment"># 第0个位置</span></span><br><span class="line">    cls_tokens = cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, mask, ids_restore</span><br></pre></td></tr></table></figure><h3 id="decoder-forward">1.7 decoder forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_decoder</span>(<span class="params">self, x, ids_restore</span>):</span><br><span class="line">    <span class="comment"># embed tokens</span></span><br><span class="line">    x = self.decoder_embed(x) <span class="comment"># 降维</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># append mask tokens to sequence</span></span><br><span class="line">    mask_tokens = self.mask_token.repeat(x.shape[<span class="number">0</span>], ids_restore.shape[<span class="number">1</span>] + <span class="number">1</span> - x.shape[<span class="number">1</span>], <span class="number">1</span>) <span class="comment"># repeat in batchsize</span></span><br><span class="line">    x_ = torch.cat([x[:, <span class="number">1</span>:, :], mask_tokens], dim=<span class="number">1</span>)  <span class="comment"># no cls token</span></span><br><span class="line">    x_ = torch.gather(x_, dim=<span class="number">1</span>, index=ids_restore.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, x.shape[<span class="number">2</span>]))  <span class="comment"># unshuffle</span></span><br><span class="line">    x = torch.cat([x[:, :<span class="number">1</span>, :], x_], dim=<span class="number">1</span>)  <span class="comment"># append cls token</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed</span></span><br><span class="line">    x = x + self.decoder_pos_embed</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.decoder_blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.decoder_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predictor projection</span></span><br><span class="line">    x = self.decoder_pred(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove cls token</span></span><br><span class="line">    x = x[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="loss-forward">1.8 loss forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_loss</span>(<span class="params">self, imgs, pred, mask</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: [N, 3, H, W]</span></span><br><span class="line"><span class="string">    pred: [N, L, p*p*3]</span></span><br><span class="line"><span class="string">    mask: [N, L], 0 is keep, 1 is remove, </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target = self.patchify(imgs)  <span class="comment"># N, L, patch_size**2 *3</span></span><br><span class="line">    <span class="keyword">if</span> self.norm_pix_loss:</span><br><span class="line">        mean = target.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = target.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        target = (target - mean) / (var + <span class="number">1.e-6</span>)**<span class="number">.5</span> <span class="comment"># 防止方差为0</span></span><br><span class="line"></span><br><span class="line">    loss = (pred - target) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean(dim=-<span class="number">1</span>)  <span class="comment"># [N, L], mean loss per patch</span></span><br><span class="line"></span><br><span class="line">    loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()  <span class="comment"># mean loss on removed patches</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="different-models">1.9 different models</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_base_patch16_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">16</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>, num_heads=<span class="number">12</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_large_patch16_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">16</span>, embed_dim=<span class="number">1024</span>, depth=<span class="number">24</span>, num_heads=<span class="number">16</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_huge_patch14_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">14</span>, embed_dim=<span class="number">1280</span>, depth=<span class="number">32</span>, num_heads=<span class="number">16</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks</span><br><span class="line">mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks</span><br><span class="line">mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks</span><br></pre></td></tr></table></figure><h2 id="main_pretrain">2 main_pretrain</h2><figure><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230420154128379.png" alt="image-20230420154128379" /><figcaption aria-hidden="true">image-20230420154128379</figcaption></figure><h3 id="设置参数的入口">2.1 设置参数的入口</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    args = get_args_parser()</span><br><span class="line">    args = args.parse_args()</span><br><span class="line">    <span class="keyword">if</span> args.output_dir:</span><br><span class="line">        Path(args.output_dir).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="main">2.2 main</h3><p>略。这个函数能够实现单机单卡，多机多卡，单机多卡，cpu等模式，通用性很强</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--accum_iter&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Accumulate gradient iterations (for increasing the effective batch size under memory constraints)&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>此内容可以用在低GPU内存，但是想要训练大batchsize的网络上(时间换空间)</li></ul><h3 id="misc.init_distributed_mode">2.3 misc.init_distributed_mode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_distributed_mode</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> args.dist_on_itp:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_RANK&#x27;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_SIZE&#x27;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_LOCAL_RANK&#x27;</span>])</span><br><span class="line">        args.dist_url = <span class="string">&quot;tcp://%s:%s&quot;</span> % (os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>], os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>])</span><br><span class="line">        os.environ[<span class="string">&#x27;LOCAL_RANK&#x27;</span>] = <span class="built_in">str</span>(args.gpu)</span><br><span class="line">        os.environ[<span class="string">&#x27;RANK&#x27;</span>] = <span class="built_in">str</span>(args.rank)</span><br><span class="line">        os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>] = <span class="built_in">str</span>(args.world_size)</span><br><span class="line">        <span class="comment"># [&quot;RANK&quot;, &quot;WORLD_SIZE&quot;, &quot;MASTER_ADDR&quot;, &quot;MASTER_PORT&quot;, &quot;LOCAL_RANK&quot;]</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;RANK&#x27;</span> <span class="keyword">in</span> os.environ <span class="keyword">and</span> <span class="string">&#x27;WORLD_SIZE&#x27;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;LOCAL_RANK&#x27;</span>])</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;SLURM_PROCID&#x27;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;SLURM_PROCID&#x27;</span>])</span><br><span class="line">        args.gpu = args.rank % torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Not using distributed mode&#x27;</span>)</span><br><span class="line">        setup_for_distributed(is_master=<span class="literal">True</span>)  <span class="comment"># hack</span></span><br><span class="line">        args.distributed = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    args.distributed = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    args.dist_backend = <span class="string">&#x27;nccl&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;| distributed init (rank &#123;&#125;): &#123;&#125;, gpu &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        args.rank, args.dist_url, args.gpu), flush=<span class="literal">True</span>)</span><br><span class="line">    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,</span><br><span class="line">                                         world_size=args.world_size, rank=args.rank)</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    setup_for_distributed(args.rank == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="prepare">2.4 prepare</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">misc.init_distributed_mode(args)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;job dir: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(os.path.dirname(os.path.realpath(__file__))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(args).replace(<span class="string">&#x27;, &#x27;</span>, <span class="string">&#x27;,\n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(args.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fix the seed for reproducibility</span></span><br><span class="line">seed = args.seed + misc.get_rank()</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="simple-augmentation">2.5 simple augmentation</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201555531.png" alt="image-20230420155514559" /><figcaption aria-hidden="true">image-20230420155514559</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># simple augmentation</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(args.input_size, scale=(<span class="number">0.2</span>, <span class="number">1.0</span>), interpolation=<span class="number">3</span>),  <span class="comment"># 3 is bicubic</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(), <span class="comment"># unint8-&gt;float</span></span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line">dataset_train = datasets.ImageFolder(os.path.join(args.data_path, <span class="string">&#x27;train&#x27;</span>), transform=transform_train)</span><br><span class="line"><span class="built_in">print</span>(dataset_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_train, sampler=sampler_train,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="define-model">2.6 define model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">model_without_ddp = model</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model = %s&quot;</span> % <span class="built_in">str</span>(model_without_ddp))</span><br><span class="line"></span><br><span class="line">eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.lr <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># only base_lr is specified</span></span><br><span class="line">    args.lr = args.blr * eff_batch_size / <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;base lr: %.2e&quot;</span> % (args.lr * <span class="number">256</span> / eff_batch_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;actual lr: %.2e&quot;</span> % args.lr)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accumulate grad iterations: %d&quot;</span> % args.accum_iter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;effective batch size: %d&quot;</span> % eff_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=<span class="literal">True</span>)</span><br><span class="line">    model_without_ddp = model.module</span><br><span class="line">    </span><br><span class="line">misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)</span><br></pre></td></tr></table></figure><h3 id="optimizer">2.7 optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># following timm: set wd as 0 for bias and norm layers</span></span><br><span class="line">param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)</span><br><span class="line">optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(<span class="number">0.9</span>, <span class="number">0.95</span>))</span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br><span class="line">loss_scaler = NativeScaler()</span><br></pre></td></tr></table></figure><h3 id="train">2.8 train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Start training for <span class="subst">&#123;args.epochs&#125;</span> epochs&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        data_loader_train.sampler.set_epoch(epoch)</span><br><span class="line">    train_stats = train_one_epoch(</span><br><span class="line">        model, data_loader_train,</span><br><span class="line">        optimizer, device, epoch, loss_scaler,</span><br><span class="line">        log_writer=log_writer,</span><br><span class="line">        args=args</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> (epoch % <span class="number">20</span> == <span class="number">0</span> <span class="keyword">or</span> epoch + <span class="number">1</span> == args.epochs):</span><br><span class="line">        misc.save_model(</span><br><span class="line">            args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,</span><br><span class="line">            loss_scaler=loss_scaler, epoch=epoch)</span><br><span class="line"></span><br><span class="line">    log_stats = &#123;**&#123;<span class="string">f&#x27;train_<span class="subst">&#123;k&#125;</span>&#x27;</span>: v <span class="keyword">for</span> k, v <span class="keyword">in</span> train_stats.items()&#125;,</span><br><span class="line">                    <span class="string">&#x27;epoch&#x27;</span>: epoch,&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> misc.is_main_process():</span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            log_writer.flush()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(args.output_dir, <span class="string">&quot;log.txt&quot;</span>), mode=<span class="string">&quot;a&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(json.dumps(log_stats) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">total_time = time.time() - start_time</span><br><span class="line">total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training time &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_time_str))</span><br></pre></td></tr></table></figure><p>其中核心代码，单个epoch的训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model: torch.nn.Module,</span></span><br><span class="line"><span class="params">                    data_loader: Iterable, optimizer: torch.optim.Optimizer,</span></span><br><span class="line"><span class="params">                    device: torch.device, epoch: <span class="built_in">int</span>, loss_scaler,</span></span><br><span class="line"><span class="params">                    log_writer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    args=<span class="literal">None</span></span>):</span><br><span class="line">    model.train(<span class="literal">True</span>)</span><br><span class="line">    metric_logger = misc.MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    metric_logger.add_meter(<span class="string">&#x27;lr&#x27;</span>, misc.SmoothedValue(window_size=<span class="number">1</span>, fmt=<span class="string">&#x27;&#123;value:.6f&#125;&#x27;</span>))</span><br><span class="line">    header = <span class="string">&#x27;Epoch: [&#123;&#125;]&#x27;</span>.<span class="built_in">format</span>(epoch)</span><br><span class="line">    print_freq = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    accum_iter = args.accum_iter</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;log_dir: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(log_writer.log_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data_iter_step, (samples, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(metric_logger.log_every(data_loader, print_freq, header)):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># we use a per iteration (instead of per epoch) lr scheduler</span></span><br><span class="line">        <span class="keyword">if</span> data_iter_step % accum_iter == <span class="number">0</span>:</span><br><span class="line">            lr_sched.adjust_learning_rate(optimizer, data_iter_step / <span class="built_in">len</span>(data_loader) + epoch, args)</span><br><span class="line"></span><br><span class="line">        samples = samples.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.amp.autocast(): <span class="comment"># 自动混合精度</span></span><br><span class="line">            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)</span><br><span class="line"></span><br><span class="line">        loss_value = loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> math.isfinite(loss_value):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Loss is &#123;&#125;, stopping training&quot;</span>.<span class="built_in">format</span>(loss_value))</span><br><span class="line">            sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        loss /= accum_iter</span><br><span class="line">        loss_scaler(loss, optimizer, parameters=model.parameters(),</span><br><span class="line">                    update_grad=(data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        torch.cuda.synchronize()</span><br><span class="line"></span><br><span class="line">        metric_logger.update(loss=loss_value)</span><br><span class="line"></span><br><span class="line">        lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">        metric_logger.update(lr=lr)</span><br><span class="line"></span><br><span class="line">        loss_value_reduce = misc.all_reduce_mean(loss_value)</span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            <span class="string">&quot;&quot;&quot; We use epoch_1000x as the x-axis in tensorboard.</span></span><br><span class="line"><span class="string">            This calibrates different curves when batch size changes.</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            epoch_1000x = <span class="built_in">int</span>((data_iter_step / <span class="built_in">len</span>(data_loader) + epoch) * <span class="number">1000</span>)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, loss_value_reduce, epoch_1000x)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;lr&#x27;</span>, lr, epoch_1000x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Averaged stats:&quot;</span>, metric_logger)</span><br><span class="line">    <span class="keyword">return</span> &#123;k: meter.global_avg <span class="keyword">for</span> k, meter <span class="keyword">in</span> metric_logger.meters.items()&#125;</span><br></pre></td></tr></table></figure><ul><li><p>如果因为timm报错，需要把 qk_scale=None注释掉</p></li><li><p>如果需要多卡训练 python -m torch.distributed.launch --nproc_per_node=2 main_prerain.py --batchsize=32 --world_size=2 --data_path="..."</p></li><li><p>dataset的目录结构</p></li></ul><p>/path/to/imagenet-1k/： train/ class1/ img1.jpeg class2/ img2.jpeg val/ class1/ img3.jpeg class2/ img4.jpeg</p><h2 id="main_fintune">3 main_fintune</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201725532.png" alt="image-20230420172505860" /><figcaption aria-hidden="true">image-20230420172505860</figcaption></figure><p>大体和main_pretrain相同，这个是对编码器进行微调，微调的目的是为了更好的完成下游任务</p><h3 id="datasetnew">3.1 dataset(new)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">is_train, args</span>):</span><br><span class="line">    transform = build_transform(is_train, args)</span><br><span class="line"></span><br><span class="line">    root = os.path.join(args.data_path, <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">    dataset = datasets.ImageFolder(root, transform=transform)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_transform</span>(<span class="params">is_train, args</span>):</span><br><span class="line">    mean = IMAGENET_DEFAULT_MEAN</span><br><span class="line">    std = IMAGENET_DEFAULT_STD</span><br><span class="line">    <span class="comment"># train transform</span></span><br><span class="line">    <span class="comment"># 强增广</span></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        <span class="comment"># this should always dispatch to transforms_imagenet_train</span></span><br><span class="line">        transform = create_transform(</span><br><span class="line">            input_size=args.input_size,</span><br><span class="line">            is_training=<span class="literal">True</span>,</span><br><span class="line">            color_jitter=args.color_jitter,</span><br><span class="line">            auto_augment=args.aa,</span><br><span class="line">            interpolation=<span class="string">&#x27;bicubic&#x27;</span>,</span><br><span class="line">            re_prob=args.reprob,</span><br><span class="line">            re_mode=args.remode,</span><br><span class="line">            re_count=args.recount,</span><br><span class="line">            mean=mean,</span><br><span class="line">            std=std,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> transform</span><br><span class="line"></span><br><span class="line">    <span class="comment"># eval transform</span></span><br><span class="line">    <span class="comment"># 基本没有增广</span></span><br><span class="line">    t = []</span><br><span class="line">    <span class="keyword">if</span> args.input_size &lt;= <span class="number">224</span>:</span><br><span class="line">        crop_pct = <span class="number">224</span> / <span class="number">256</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        crop_pct = <span class="number">1.0</span></span><br><span class="line">    size = <span class="built_in">int</span>(args.input_size / crop_pct)</span><br><span class="line">    t.append(</span><br><span class="line">        transforms.Resize(size, interpolation=PIL.Image.BICUBIC),  <span class="comment"># to maintain same ratio w.r.t. 224 images</span></span><br><span class="line">    )</span><br><span class="line">    t.append(transforms.CenterCrop(args.input_size))</span><br><span class="line"></span><br><span class="line">    t.append(transforms.ToTensor())</span><br><span class="line">    t.append(transforms.Normalize(mean, std))</span><br><span class="line">    <span class="keyword">return</span> transforms.Compose(t)</span><br></pre></td></tr></table></figure><h3 id="dataloader">3.2 dataloader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_train, sampler=sampler_train,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_val, sampler=sampler_val,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="mixup增广">3.3 mixup增广</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mixup_fn = <span class="literal">None</span></span><br><span class="line">mixup_active = args.mixup &gt; <span class="number">0</span> <span class="keyword">or</span> args.cutmix &gt; <span class="number">0.</span> <span class="keyword">or</span> args.cutmix_minmax <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> mixup_active:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Mixup is activated!&quot;</span>)</span><br><span class="line">    mixup_fn = Mixup(</span><br><span class="line">        mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,</span><br><span class="line">        prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,</span><br><span class="line">        label_smoothing=args.smoothing, num_classes=args.nb_classes)</span><br></pre></td></tr></table></figure><h3 id="model定义">3.4 model定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = models_vit.__dict__[args.model](</span><br><span class="line">    num_classes=args.nb_classes,</span><br><span class="line">    drop_path_rate=args.drop_path,</span><br><span class="line">    global_pool=args.global_pool,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(timm.models.vision_transformer.VisionTransformer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer with support for global average pooling</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, global_pool=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self.global_pool = global_pool</span><br><span class="line">        <span class="keyword">if</span> self.global_pool:</span><br><span class="line">            norm_layer = kwargs[<span class="string">&#x27;norm_layer&#x27;</span>]</span><br><span class="line">            embed_dim = kwargs[<span class="string">&#x27;embed_dim&#x27;</span>]</span><br><span class="line">            self.fc_norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">del</span> self.norm  <span class="comment"># remove the original norm</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line"></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = blk(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.global_pool:</span><br><span class="line">            x = x[:, <span class="number">1</span>:, :].mean(dim=<span class="number">1</span>)  <span class="comment"># global pool without cls token</span></span><br><span class="line">            outcome = self.fc_norm(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">            outcome = x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outcome</span><br></pre></td></tr></table></figure><h3 id="预训练权重的导入">3.5 预训练权重的导入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.finetune <span class="keyword">and</span> <span class="keyword">not</span> args.<span class="built_in">eval</span>:</span><br><span class="line">    checkpoint = torch.load(args.finetune, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Load pre-trained checkpoint from: %s&quot;</span> % args.finetune)</span><br><span class="line">    checkpoint_model = checkpoint[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    state_dict = model.state_dict()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> checkpoint_model <span class="keyword">and</span> checkpoint_model[k].shape != state_dict[k].shape:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Removing key <span class="subst">&#123;k&#125;</span> from pretrained checkpoint&quot;</span>)</span><br><span class="line">            <span class="keyword">del</span> checkpoint_model[k]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># interpolate position embedding</span></span><br><span class="line">    interpolate_pos_embed(model, checkpoint_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load pre-trained model</span></span><br><span class="line">    msg = model.load_state_dict(checkpoint_model, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(msg)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.global_pool:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(msg.missing_keys) == &#123;<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>, <span class="string">&#x27;fc_norm.weight&#x27;</span>, <span class="string">&#x27;fc_norm.bias&#x27;</span>&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(msg.missing_keys) == &#123;<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># manually initialize fc layer</span></span><br><span class="line">    trunc_normal_(model.head.weight, std=<span class="number">2e-5</span>)</span><br></pre></td></tr></table></figure><ul><li>确保head部分的权重没有导入</li><li>线性插值：让微调阶段的position embedding仍然适用(如果模型变大)</li></ul><h2 id="linear-probing">4 linear probing</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201725555.png" alt="image-20230420172530538" /><figcaption aria-hidden="true">image-20230420172530538</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for linear prob only</span></span><br><span class="line"><span class="comment"># hack: revise model&#x27;s head with BN</span></span><br><span class="line">model.head = torch.nn.Sequential(torch.nn.BatchNorm1d(model.head.in_features, affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>), model.head)</span><br><span class="line"><span class="comment"># freeze all but the head</span></span><br><span class="line"><span class="keyword">for</span> _, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> _, p <span class="keyword">in</span> model.head.named_parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;masked-autoencoders-are-scalable-vision-learners&quot;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.o</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络8-ConvNext</title>
    <link href="https://wangtongyouwen.github.io/post/aa153511.html"/>
    <id>https://wangtongyouwen.github.io/post/aa153511.html</id>
    <published>2023-04-18T10:32:10.000Z</published>
    <updated>2023-04-30T06:08:29.723Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-convnet-for-the-2020s">A ConvNet for the 2020s</h1><p>https://arxiv.org/pdf/2201.03545.pdf</p><p>A vanilla <strong>ViT</strong>, on the other hand, faces <strong>difficulties</strong> when applied to general computer vision tasks such as object detection and semantic segmentation</p><p>It is the <strong>hierarchical</strong> Transformers (e.g., <strong>Swin Transformers</strong>) that reintroduced several <strong>ConvNet</strong> priors, making Transformers practically viable as a generic vision backbone and demonstrating <strong>remarkable performance on a wide variety of vision tasks</strong>.</p><h2 id="理解什么是resnet-50">1 理解什么是ResNet-50</h2><ul><li>由48层卷积+1层maxpool+1层avgpool构成，卷积每个block的配比为3:4:6:3</li><li>ResNet50 Architecture</li></ul><figure><img src="https://iq.opengenus.org/content/images/2020/03/Screenshot-from-2020-03-20-15-49-54.png" alt="Table 1" /><figcaption aria-hidden="true">Table 1</figcaption></figure><h2 id="convnext主要宗旨">2 ConVNeXt主要宗旨</h2><ul><li>本文主要是希望基于ReSNet-50结构，并参考Swin-T的思考来升级改造ResNet，最终得到ResNet结构，并实现了新的准确率，并进一步探索了它的可扩展性。</li></ul><h2 id="优化器参数">3 优化器参数</h2><ul><li>AdamW，300epochs</li><li>准确率直接从76.1%提升到了78.8%</li><li>预训练学习率为4e-3，weight_decay=0.05,batchsize=4096</li><li>微调学习率为5e-5,weight_decay=1e-8,batchsize=512,layer-wise Ir decay</li></ul><h2 id="宏观设计">4 宏观设计</h2><ul><li>将[3,4,6,3]的区块比例改成了[3,3,9,3]</li><li>将底层的卷积替换成了4*4 stride=4的卷积，类似于patch</li><li>引入depth-wise conv，并将channels从64提升到96</li><li>引入bottleneck结构{channels分别为96 384 96}，并增大 kernel size 到 7*7</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191428468.png" alt="image-20230419142812305" /><figcaption aria-hidden="true">image-20230419142812305</figcaption></figure><ul><li>至此，ImageNet-1k的准确率从78.8%提升到80.6%</li></ul><h2 id="微观设计">5 微观设计</h2><ul><li>将RELU替换成GELU，将BN替换为LN</li><li>引入更少激活函数和归一化层</li><li>采用2*2，stride=2的卷积进行下采样，并在底层、下采样之前和最后的平均池化之后加入LN层，使得训练更加稳定</li><li>至此，ImageNet-1k的准确率进一步提升到82.0%，击败Swin-T中的81.3%</li></ul><h2 id="可扩展性">6 可扩展性</h2><ul><li>ImageNet-1k训练<ul><li>随着参数数目和计算量的增大，准确率也在逐步提升至85.5%</li></ul></li><li>增加ImageNet-22k训练，在迁移至ImageNet-1k微调<ul><li>伴随预训练，同样的模型，效果涨幅约为2%</li><li>最终，ConvNeXt-XL效果达到87.8%</li></ul></li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191537241.png" alt="image-20230419144017393" /><figcaption aria-hidden="true">image-20230419144017393</figcaption></figure><h1 id="code">CODE</h1><p><a href="https://github.com/facebookresearch/ConvNeXt">facebookresearch/ConvNeXt: Code release for ConvNeXt model (github.com)</a></p><h2 id="block">1 block</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; ConvNeXt Block. There are two equivalent implementations:</span></span><br><span class="line"><span class="string">    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)</span></span><br><span class="line"><span class="string">    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back</span></span><br><span class="line"><span class="string">    We use (2) as we find it slightly faster in PyTorch</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        drop_path (float): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, drop_path=<span class="number">0.</span>, layer_scale_init_value=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dwconv = nn.Conv2d(dim, dim, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, groups=dim) <span class="comment"># depthwise conv</span></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.pwconv1 = nn.Linear(dim, <span class="number">4</span> * dim) <span class="comment"># pointwise/1x1 convs, implemented with linear layers</span></span><br><span class="line">        self.act = nn.GELU()</span><br><span class="line">        self.pwconv2 = nn.Linear(<span class="number">4</span> * dim, dim)</span><br><span class="line">        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), </span><br><span class="line">                                    requires_grad=<span class="literal">True</span>) <span class="keyword">if</span> layer_scale_init_value &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="built_in">input</span> = x</span><br><span class="line">        x = self.dwconv(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="comment"># (N, C, H, W) -&gt; (N, H, W, C)</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.pwconv1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.pwconv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.gamma <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.gamma * x</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (N, H, W, C) -&gt; (N, C, H, W)</span></span><br><span class="line"></span><br><span class="line">        x = <span class="built_in">input</span> + self.drop_path(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="connext">2 ConNeXt</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191505813.png" alt="image-20230419150530038" /><figcaption aria-hidden="true">image-20230419150530038</figcaption></figure><ul><li>stem<ul><li>conv2d: 3-&gt;96 kernel_size =4,stride=4</li><li>layernarm: 96</li></ul></li><li>downsampler_layer<ul><li>layernrom: 96, 192, 384</li><li>conv2d: 96, 192, 384 -&gt; 192, 384, 768 kernel_size = 2,stride = 2 (patch merging)</li></ul></li><li>stage(4个阶段)<ul><li>block数量:3,3,9,3</li><li>block input_channel: 96, 192, 384, 768</li><li>cur记录总的深度：每个深度的dropout是不同的，随着深度增大，dropout比例越大</li></ul></li><li>layernorm：768</li><li>head(Linear): 768 -&gt; num_classes</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, </span></span><br><span class="line"><span class="params">             depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>], dims=[<span class="number">96</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>], drop_path_rate=<span class="number">0.</span>, </span></span><br><span class="line"><span class="params">             layer_scale_init_value=<span class="number">1e-6</span>, head_init_scale=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">             </span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    self.downsample_layers = nn.ModuleList() <span class="comment"># stem and 3 intermediate downsampling conv layers</span></span><br><span class="line">    stem = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_chans, dims[<span class="number">0</span>], kernel_size=<span class="number">4</span>, stride=<span class="number">4</span>),</span><br><span class="line">        LayerNorm(dims[<span class="number">0</span>], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    self.downsample_layers.append(stem)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        downsample_layer = nn.Sequential(</span><br><span class="line">                LayerNorm(dims[i], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>),</span><br><span class="line">                nn.Conv2d(dims[i], dims[i+<span class="number">1</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.downsample_layers.append(downsample_layer)</span><br><span class="line"></span><br><span class="line">    self.stages = nn.ModuleList() <span class="comment"># 4 feature resolution stages, each consisting of multiple residual blocks</span></span><br><span class="line">    dp_rates=[x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))] </span><br><span class="line">    cur = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        stage = nn.Sequential(</span><br><span class="line">            *[Block(dim=dims[i], drop_path=dp_rates[cur + j], </span><br><span class="line">            layer_scale_init_value=layer_scale_init_value) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(depths[i])]</span><br><span class="line">        )</span><br><span class="line">        self.stages.append(stage)</span><br><span class="line">        cur += depths[i]</span><br><span class="line"></span><br><span class="line">    self.norm = nn.LayerNorm(dims[-<span class="number">1</span>], eps=<span class="number">1e-6</span>) <span class="comment"># final norm layer</span></span><br><span class="line">    self.head = nn.Linear(dims[-<span class="number">1</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    self.apply(self._init_weights)</span><br><span class="line">    self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">    self.head.bias.data.mul_(head_init_scale)</span><br></pre></td></tr></table></figure><h1 id="isotropic-convnext-各向同性">Isotropic ConvNeXt 各向同性</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191537246.png" alt="image-20230419153724437" /><figcaption aria-hidden="true">image-20230419153724437</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ef __init__(self, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, </span><br><span class="line">                 depth=<span class="number">18</span>, dim=<span class="number">384</span>, drop_path_rate=<span class="number">0.</span>, </span><br><span class="line">                 layer_scale_init_value=<span class="number">0</span>, head_init_scale=<span class="number">1.</span>,</span><br><span class="line">                 ):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.stem = nn.Conv2d(in_chans, dim, kernel_size=<span class="number">16</span>, stride=<span class="number">16</span>)</span><br><span class="line">        dp_rates=[x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)] </span><br><span class="line">        self.blocks = nn.Sequential(*[Block(dim=dim, drop_path=dp_rates[i], </span><br><span class="line">                                    layer_scale_init_value=layer_scale_init_value)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>) <span class="comment"># final norm layer</span></span><br><span class="line">        self.head = nn.Linear(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line">        self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">        self.head.bias.data.mul_(head_init_scale)</span><br></pre></td></tr></table></figure><ul><li>不需要step，各向同性，通道数目在每个阶段都是相同的</li></ul><h1 id="训练">训练</h1><ul><li>data_path</li><li>data_set {CIFAR,IMNET,image_foler}</li></ul><p>https://timm.fast.ai/这里有大量的cv网络的实现</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;a-convnet-for-the-2020s&quot;&gt;A ConvNet for the 2020s&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/pdf/2201.03545.pdf&lt;/p&gt;
&lt;p&gt;A vanilla &lt;strong&gt;ViT&lt;/strong</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门10-word embedding</title>
    <link href="https://wangtongyouwen.github.io/post/70a386a7.html"/>
    <id>https://wangtongyouwen.github.io/post/70a386a7.html</id>
    <published>2023-04-17T14:27:03.000Z</published>
    <updated>2023-04-30T06:07:54.283Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言建模">语言建模</h2><ul><li>基于已有的人类组织的文本语料，基于无监督学习如何组织一句话并还能得到单词的语义表征</li><li>统计模型：n-gram</li><li>无监督学习：NNLM</li><li>大规模无监督学习：word2vec,BERT</li></ul><h3 id="n-gram">1 n-gram</h3><ul><li>特点：统计性、简单、泛化能力差、无法得到单词的语义信息</li><li>定义：n个相邻字符构成的序列<ul><li>unigram</li><li>bigram</li><li>trigram</li></ul></li><li>用途：基于n-gram的频数分析文本，如垃圾邮件分类</li><li>对于word n-gram，特征维度随着语料词汇增大和n增大而指数增大(curse of dimensionality 维度灾难)</li><li>对于character n-gram，特征维度只随着n增大而增大</li></ul><h3 id="单词的语义表征">2 单词的语义表征</h3><ul><li>稀疏式<ul><li>one-hot encoding 只能反应出单词在单词表中的位置信息，不能得出任何语义上的信息</li></ul></li><li>分布式<ul><li>类似于word embedding 固定长度，每个位置上都是浮点型，这种语义表征是隐式的，是训练获得的(通过向量点积能得到相似度)</li></ul></li><li>应用场景<ul><li>word/character/phrase/sentence/paragraph embedding</li><li>speaker/user/item embedding</li></ul></li></ul><h3 id="基于神经网络的语言模型nnlm">3 基于神经网络的语言模型(NNLM)</h3><ul><li>NNLM包括：<ul><li>输入层(one-hot)</li><li>投影层</li><li>隐含层</li><li>输出层</li></ul></li><li>word embeddings为副产物，隐含的语义表征</li><li>主要复杂度： N*D*H+H*V</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/dQiaQ6INiazLoDx1sOWTQSTiaoLahdlZvZ9gBdtWVSS6gsqz8PLgHAPUesz0mqVCyo2MwjO6yssqnBOPO5BJ8z1lg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="Image" style="zoom:67%;" /></p><ul><li>如何降低复杂度？如何训练大规模数据？</li></ul><h3 id="word2vec">4 word2vec</h3><h4 id="改进1抛弃了隐含层并提出cbow和skip-gram">改进1：抛弃了隐含层，并提出CBOW和Skip-gram</h4><ul><li>continuous Bag-of-Words<ul><li>不同于NNLM，CBOW考虑到了前后上下文</li><li>使用周围单词预测中间单词</li><li>输入：前n个单词和后n个单词</li><li>目标：基于H-softmax预测中间单词</li></ul></li></ul><p><span class="math display">\[J_{\theta}=\frac{1}{T}\sum^T_{t=1}log P(w_t|w_{t-n},\dots,w_{t-1},w_{t+1},\dots,w_{t+n})\]</span></p><ul><li>Skip-gram<ul><li>与CBOW相反，使用中间单词预测周围单词</li><li>输入：中间单词</li><li>目标：基于H-softmax预测前n个单词和后n个单词</li></ul></li></ul><p><span class="math display">\[J_{\theta} = \frac{1}{T}\sum^T_{t=1} \sum_{-n\le j\le n,n\ne 0}log\ \ p(w_{t+j}|w_t)\]</span></p><figure><img src="https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Screenshot-2021-03-05-at-11.29.31-1024x616-1.png" alt="Screenshot-2021-03-05-at-11.29.31" /><figcaption aria-hidden="true">Screenshot-2021-03-05-at-11.29.31</figcaption></figure><h4 id="改进2优化softmax">改进2：优化softmax</h4><ul><li>softmax<ul><li>计算量跟单词表数目K呈线性关系</li></ul></li></ul><p><span class="math display">\[\sigma(\vec z)_i = \frac{e^{z_i}}{\sum^K_{j=1}e^{z_j}}\]</span></p><ul><li>hierarchical softmax(huffman树)</li></ul><ol type="1"><li>将{w1,w2,...,wn}看成是由n棵树的森林(每棵树仅有一个节点)</li><li>在森林中选出两个树节点的权值最小的树合并，作为一棵新树的左右子树，且新树的根节点权值为其左、右子树节点权值之和</li><li>从森林中删除选取的两棵树，并将新树加入森林</li><li>重复2,3步，知道森林中只剩一棵树为止，该树就为所求的Huffman树</li></ol><p><img src="https://www.ruder.io/content/images/2016/05/hierarchical_softmax.png" alt="img" /> <span class="math display">\[p(right|n,c)=\sigma(h^Tv&#39;_n)\]</span></p><p><span class="math display">\[p(left|n,c) = 1-\sigma(h^Tv&#39;_n)\]</span></p><h4 id="改进3引入负采样">改进3：引入负采样</h4><ul><li>continuous bag ofwords<ul><li>输入：前n个单词和后n个单词</li><li>目标：使得预测中间单词的概率最大，负样本单词的概率最小</li></ul></li></ul><p><span class="math display">\[g(w) = \prod_{u\in {w} \bigcup NEG(w)}p(u|Context(w))\]</span></p><ul><li>Skip-gram<ul><li>入：中间单词</li><li>目标：使得上下文单词概率最大，负样本单词的概率最小</li></ul></li></ul><p><span class="math display">\[g(w) = \prod_{\tilde w\in {Context(w)}} \prod_{u\in {w} \bigcup  NEG^{\tilde w}(w)} p(u|\tilde w)\]</span></p><h2 id="word-embeddings-in-pytorch">word embeddings in pytorch</h2><p>https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;语言建模&quot;&gt;语言建模&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;基于已有的人类组织的文本语料，基于无监督学习如何组织一句话并还能得到单词的语义表征&lt;/li&gt;
&lt;li&gt;统计模型：n-gram&lt;/li&gt;
&lt;li&gt;无监督学习：NNLM&lt;/li&gt;
&lt;li&gt;大规模无监督学习：w</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实用工具2-编写技巧</title>
    <link href="https://wangtongyouwen.github.io/post/898bb30b.html"/>
    <id>https://wangtongyouwen.github.io/post/898bb30b.html</id>
    <published>2023-04-17T11:36:18.000Z</published>
    <updated>2023-04-17T14:20:00.293Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304172006805.png" alt="编写技巧" /><figcaption aria-hidden="true">编写技巧</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange,reduce,repeat</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>) <span class="comment"># 4D tensor bs*ic*h*w</span></span><br></pre></td></tr></table></figure><h2 id="rearrange">1 rearrange</h2><h3 id="转置">1.1 转置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转置</span></span><br><span class="line">out1 = x.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">out2 = rearrange(x,<span class="string">&#x27;b i h w -&gt; b h i w&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="变形">1.2 变形</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变形</span></span><br><span class="line">out1 = rearrange(x,<span class="string">&#x27;b i h w -&gt; (b i) h w&#x27;</span>)</span><br><span class="line">out2 = x.reshape(<span class="number">6</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">out3 = rearrange(out2,<span class="string">&#x27;(b i) h w -&gt; b i h w&#x27;</span>,b=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out3,x))</span><br></pre></td></tr></table></figure><h3 id="image2patch">1.3 image2patch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image2patch</span></span><br><span class="line">bs, ic, h, w = x.shape</span><br><span class="line">p = <span class="number">2</span></span><br><span class="line">out1 = rearrange(x,<span class="string">&#x27;b i (h1 p1) (w1 p2) -&gt; b (h1 w1) (i p1 p2)&#x27;</span>,p1=<span class="number">2</span>,p2=<span class="number">2</span>) <span class="comment"># p是patch的边长 [batchsize,num_patch,patch_depth]</span></span><br><span class="line">out2 = F.unfold(x, kernel_size=(p, p),stride=(p, p)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)  <span class="comment"># [bs,num_patch,patch_depth]</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="堆叠">1.4 堆叠</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 堆叠</span></span><br><span class="line"><span class="built_in">print</span>(bs,i,h,w) <span class="comment"># (2,2,4,4)</span></span><br><span class="line">tensor_list = [x,x,x] <span class="comment"># only in the form of einops</span></span><br><span class="line">out1 = rearrange(tensor_list,<span class="string">&#x27;n b i h w -&gt; n b i h w&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(out1.shape)</span><br><span class="line"></span><br><span class="line">y = torch.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">out2 = y.repeat(bs,i,h,w).reshape(<span class="number">3</span>,bs,i,h,w)</span><br><span class="line"><span class="built_in">print</span>(out2.shape)</span><br></pre></td></tr></table></figure><h2 id="reduce">2 reduce</h2><h3 id="求平均池化">2.1 求平均池化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求平均池化</span></span><br><span class="line">out1 = reduce(x,<span class="string">&#x27;b i h w -&gt; b i h&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># mean, min, max, sum, prod</span></span><br><span class="line">out2 = torch.mean(x,(-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="求和">2.2 求和</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求和</span></span><br><span class="line">out1 = reduce(x,<span class="string">&#x27;b i h w -&gt; b i h 1&#x27;</span>,<span class="string">&#x27;sum&#x27;</span>) <span class="comment"># keep dimension</span></span><br><span class="line">out2 = torch.<span class="built_in">sum</span>(x,(-<span class="number">1</span>)).unsqueeze(-<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="多个维度操作">2.3 多个维度操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多个维度操作</span></span><br><span class="line">b, i, h, w = x.shape</span><br><span class="line">out1 = reduce(x,<span class="string">&#x27;b i h w -&gt; b i&#x27;</span>,<span class="string">&#x27;max&#x27;</span>)</span><br><span class="line">out2 = torch.max_pool2d(x,(h,w)).reshape(b,i)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h2 id="repeat">3 repeat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复制</span></span><br><span class="line">out1 = rearrange(x,<span class="string">&#x27;b i h w -&gt; b i h w 1&#x27;</span>)</span><br><span class="line">out2 = repeat(out1,<span class="string">&#x27;b i h w 1 -&gt; b i (2 h) w 2&#x27;</span>)</span><br><span class="line"></span><br><span class="line">out3 = torch.tile(out1,(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,))</span><br><span class="line"><span class="built_in">print</span>(out2.shape)</span><br><span class="line"><span class="built_in">print</span>(out3.shape)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304172006805.png&quot; alt=&quot;编写技巧&quot; /&gt;&lt;figcaption aria-hidden</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="tools" scheme="https://wangtongyouwen.github.io/categories/pytorch/tools/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="tools" scheme="https://wangtongyouwen.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门9-seq2seq2</title>
    <link href="https://wangtongyouwen.github.io/post/86ae38c6.html"/>
    <id>https://wangtongyouwen.github.io/post/86ae38c6.html</id>
    <published>2023-04-17T04:40:39.000Z</published>
    <updated>2023-04-30T06:07:41.140Z</updated>
    
    <content type="html"><![CDATA[<h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">1 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</h1><p>https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171444845.png" alt="image-20230417144427318" /><figcaption aria-hidden="true">image-20230417144427318</figcaption></figure><h1 id="effective-approaches-to-attention-based-neural-machine-translation">2 Effective Approaches to Attention-based Neural Machine Translation</h1><p>https://arxiv.org/pdf/1508.04025)</p><h2 id="global-attention">2.1 global attention</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171447590.png" alt="image-20230417144755015" /><figcaption aria-hidden="true">image-20230417144755015</figcaption></figure><p>The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the <strong>context vector</strong> <span class="math inline">\(c_t\)</span> . In this model type, a <strong>variable-length alignment vector</strong> <span class="math inline">\(a_t\)</span> , whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state ht with each <strong>source hidden state</strong> <span class="math inline">\(\bar h _s\)</span>: <span class="math display">\[\begin{aligned}\boldsymbol{a}_{t}(s) &amp; =\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \\&amp; =\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s^{\prime}}\right)\right)}\end{aligned}\]</span> score is referred as a <strong>content-based function</strong> for which we consider three different alternatives <span class="math display">\[\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{ll}\boldsymbol{h}_{t}^{\top} \overline{\boldsymbol{h}}_{s} &amp; \text { dot } \\\boldsymbol{h}_{t}^{\top} \boldsymbol{W}_{\boldsymbol{a}} \overline{\boldsymbol{h}}_{s} &amp; \text { general } \\\boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{a}}\left[\boldsymbol{h}_{t} ; \overline{\boldsymbol{h}}_{s}\right]\right) &amp; \text { concat }\end{array}\right.\]</span> dot: transformer Q K V 并行计算</p><p>general: 乘法注意力机制</p><p>concat: 加法注意力机制</p><p>基于位置的: location-based function in which the <strong>alignment scores</strong> are computed from solely the target hidden state <span class="math inline">\(h_t\)</span> <span class="math display">\[a_t = softmax(W_ah_t)  \     \ location\]</span></p><p>全局注意力计算量大，有些任务也不需要进行全局注意力计算。</p><h2 id="local-attention">2.2 local attention</h2><p>This model takes inspiration from the tradeoff between the <strong>soft and hard</strong> attentional models</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171454431.png" alt="image-20230417145444733" /><figcaption aria-hidden="true">image-20230417145444733</figcaption></figure><ul><li>generate an aligned position <span class="math inline">\(p_t\)</span> for each target word at time <span class="math inline">\(t\)</span></li><li>the context vector <span class="math inline">\(c_t\)</span> is then derived as a weighted average over the set of source hidden states within the window <span class="math inline">\([p_t-D,p_t+D]\)</span> <span class="math inline">\(D\)</span> is empirically selected</li><li><span class="math inline">\(a_t\)</span> is fixed-dimensional, i.e. <span class="math inline">\(\in \R^{2D+1}\)</span></li><li>Monotonic alignment (<strong>local-m</strong>) <span class="math inline">\(p_t = t\)</span> assuming that source and target sequences are roughly monotonically aligned 编码器的中心位置是经验性的，不是训练出来的。hard 方法</li><li>Predictive alignment (<strong>local-p</strong>) 模型能够计算出编码器的中心位置，这是训练得到的 soft 方法</li></ul><p><span class="math display">\[p_{t}=S \cdot \operatorname{sigmoid}\left(\boldsymbol{v}_{p}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{p}} \boldsymbol{h}_{t}\right)\right)\]</span></p><ul><li><ul><li><span class="math inline">\(W_p,v_p\)</span>: the model parameters which will be learned to predict positions</li><li><span class="math inline">\(S\)</span>: the source sentence length</li></ul></li><li>alignment weights -&gt; Gaussian distribution</li></ul><p><span class="math display">\[\boldsymbol{a}_{t}(s)=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \exp \left(-\frac{\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right)  where \ \sigma = \frac{D}{2}\]</span></p><h1 id="代码实现">3 代码实现</h1><h2 id="encoder">3.1 encoder</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;以离散符号(索引的符号)的分类任务为例，实现基于注意力机制的seq2seq模型&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Seq2SeqEncoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;实现基于LSTM的编码器，也可以是其他类型的，如CNN，TransformerEncoder&quot;&quot;&quot;</span><br><span class="line">    &quot;&quot;&quot;对原序列进行上下文相关的表征&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, embedding_dim, hidden_size, source_vocab_size):</span><br><span class="line">        super(Seq2SeqEncoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)</span><br><span class="line">        # num_embeddings (int): size of the dictionary of embeddings</span><br><span class="line">        # embedding_dim (int): the size of each embedding vector</span><br><span class="line">        self.embedding_table = nn.Embedding(source_vocab_size, embedding_dim)  # 将token转换成 token_embedding # 可训练</span><br><span class="line"></span><br><span class="line">    def forward(self, input_ids):</span><br><span class="line">        # 原序列不是流式的，所以能够一次性拿到全部序列</span><br><span class="line">        input_sequence = self.embedding_table(input_ids)  # [bs,source_length,embedding_dim]</span><br><span class="line">        output_states, (final_h, final_c) = self.lstm_layer(input_sequence)</span><br><span class="line"></span><br><span class="line">        return output_states, final_h</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="attentionmechanism">3.2 AttentionMechanism</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Seq2SeqAttentionMechanism(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;实现dot-product的attention&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Seq2SeqAttentionMechanism, self).__init__()</span><br><span class="line"></span><br><span class="line">    def forward(self, decoder_state_t, encoder_states):</span><br><span class="line">        # encoder_states: 完整的编码序列</span><br><span class="line">        # decoder_State_t: t 时刻的decoder序列</span><br><span class="line">        bs, source_length, hidden_size = encoder_states.shape</span><br><span class="line"></span><br><span class="line">        decoder_state_t = decoder_state_t.unsqueeze(1)  # [bs,1,hidden_size]</span><br><span class="line">        decoder_state_t = torch.tile(decoder_state_t, dims=(1, source_length, 1))  # 复制为 [bs,source_length,hidden_size]</span><br><span class="line"></span><br><span class="line">        score = torch.sum(decoder_state_t * encoder_states, dim=-1)  # [bs,source_length]</span><br><span class="line"></span><br><span class="line">        attn_prob = F.softmax(score, dim=-1)  # 得到序列中每个具体索引在整个序列中的权重值,总和为1 [bs,source_length]</span><br><span class="line"></span><br><span class="line">        # 加权求和</span><br><span class="line">        # 使用了广播机制 attn_prob: [bs,source_length], decoder_state_t: [bs,source_length,hidden_size]</span><br><span class="line">        context = torch.sum(attn_prob.unsqueeze(-1) * encoder_states,1) # [bs,hidden_size] -&gt; 第t时刻解码器需要的上下文向量</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn_prob, context</span><br></pre></td></tr></table></figure><h2 id="decoder">3.3 Decoder</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">class Seq2SeqDecoder(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, embedding_dim, hidden_size, num_classes, target_vocab_size, start_id, end_id):</span><br><span class="line">        super(Seq2SeqDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.lstm_cell = nn.LSTMCell(embedding_dim, hidden_size) # cell 是单步完成，[bs,embedding_dim]</span><br><span class="line">        self.proj_layer = nn.Linear(hidden_size * 2, num_classes) # [context_vector+decode_state_t,num_classes]</span><br><span class="line">        self.attention_mechanism = Seq2SeqAttentionMechanism()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.embedding_table = nn.Embedding(target_vocab_size, embedding_dim)</span><br><span class="line">        self.start_id = start_id # 会传入一个真实的target作为输入，可以进行第一个位置的预测</span><br><span class="line">        self.end_id = end_id # 能够判断序列的截止位置</span><br><span class="line"></span><br><span class="line">    def forward(self, shifted_target_ids, encoder_states):</span><br><span class="line">        # 训练阶段调用，teacher-force mode</span><br><span class="line">        # shifted_target_ids 真实的序列id</span><br><span class="line">        shifted_target = self.embedding_table(shifted_target_ids)</span><br><span class="line"></span><br><span class="line">        bs, target_length, embedding_dim = shifted_target.shape</span><br><span class="line">        bs, source_length, hidden_size = encoder_states.shape</span><br><span class="line"></span><br><span class="line">        logits = torch.zeros(bs, target_length, self.num_classes)</span><br><span class="line">        probs = torch.zeros(bs, target_length, source_length)</span><br><span class="line"></span><br><span class="line">        for t in range(target_length):</span><br><span class="line">            decoder_input_t = shifted_target[:, t, :]  # [bs,embedding_dim]</span><br><span class="line">            if t == 0:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t)</span><br><span class="line">            else:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t, (h_t, c_t))</span><br><span class="line"></span><br><span class="line">            attn_prob, context = self.attention_mechanism(h_t, encoder_states) # [decoder_state_t, encoder_states]</span><br><span class="line">            # context: [bs,hidden_size] h_t: [bs,hidden_size]</span><br><span class="line">            decoder_output = torch.cat((context, h_t), -1)</span><br><span class="line">            logits[:, t, :] = self.proj_layer(decoder_output)</span><br><span class="line">            probs[:, t, :] = attn_prob # [bs,source_length]</span><br><span class="line"></span><br><span class="line">        return probs, logits</span><br><span class="line"></span><br><span class="line">    def inference(self, encoder_states):</span><br><span class="line">        # 推理阶段使用</span><br><span class="line">        target_id = self.start_id</span><br><span class="line">        h_t = None</span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        while True:</span><br><span class="line">            decoder_input_t = self.embedding_table(target_id)</span><br><span class="line">            if h_t is None:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t)</span><br><span class="line">            else:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t, (h_t, c_t))</span><br><span class="line"></span><br><span class="line">            attn_prob, context = self.attention_mechanism(h_t, encoder_states)</span><br><span class="line"></span><br><span class="line">            decoder_output = torch.cat((context, h_t), -1)</span><br><span class="line">            logits = self.proj_layer(decoder_output)</span><br><span class="line"></span><br><span class="line">            target_id = torch.argmax(logits, -1)</span><br><span class="line">            result.append(target_id)</span><br><span class="line"></span><br><span class="line">            if torch.any(target_id == self.end_id): # 解码终止条件,在构造训练数据的时候需要在每个句子后面添加一个字符用做end id</span><br><span class="line">                print(&quot;start coding!&quot;)</span><br><span class="line">                break</span><br><span class="line">        predict_ids = torch.stack(result, dim=0)</span><br><span class="line"></span><br><span class="line">        return predict_ids</span><br></pre></td></tr></table></figure><h2 id="model">3.4 Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_size, num_classes, source_vocab_size, target_vocab_size, start_id, end_id</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = Seq2SeqEncoder(embedding_dim, hidden_size, source_vocab_size)</span><br><span class="line">        self.decoder = Seq2SeqDecoder(embedding_dim, hidden_size, num_classes, target_vocab_size, start_id, end_id)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_sequence_ids, shifted_target_ids</span>):</span><br><span class="line">        <span class="comment"># 训练阶段</span></span><br><span class="line">        encoder_states, final_h = self.encoder(input_sequence_ids)</span><br><span class="line">        probs, logits = self.decoder(shifted_target_ids, encoder_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> probs, logits</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inference</span>(<span class="params">self,predict_ids,input_sequence_ids</span>):</span><br><span class="line">        <span class="comment"># 推理阶段</span></span><br><span class="line">        encoder_states, final_h = self.encoder(input_sequence_ids)</span><br><span class="line">        probs, logits = self.decoder(predict_ids, encoder_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> probs, logits</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171935294.png" alt="seq2seq代码编写技巧" /><figcaption aria-hidden="true">seq2seq代码编写技巧</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;neural-machine-translation-by-jointly-learning-to-align-and-translate&quot;&gt;1 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络7-SwinTransformer</title>
    <link href="https://wangtongyouwen.github.io/post/daaec8c8.html"/>
    <id>https://wangtongyouwen.github.io/post/daaec8c8.html</id>
    <published>2023-04-15T14:34:26.000Z</published>
    <updated>2023-04-30T06:06:57.432Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152245800.png" alt="image-20230415224534491" /><figcaption aria-hidden="true">image-20230415224534491</figcaption></figure><p>将transformer直接使用到CV领域遇到的问题：</p><ul><li>尺度问题：同一张图中代表不用语义信息的block尺度差距很大</li><li>处理像素问题的计算成本大(形成的序列很长)</li></ul><p>本文提出一种hierarchical transformer,主要使用了shifted windows</p><ul><li>自注意力机制是在这个窗口中计算的，序列长度大大降低</li><li>通过shifting这个操作能够让相邻的两个窗口之间产生交互，上下层之间有了cross-window-connection</li></ul><p>image classification/object detection/semantic segmentation</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152255423.png" alt="image-20230415225520894" /><figcaption aria-hidden="true">image-20230415225520894</figcaption></figure><h1 id="如何基于图片生成-patch-embedding">1.如何基于图片生成 patch embedding?</h1><h2 id="方法一">方法一</h2><ul><li>基于 pytorch unfold 的API将图片进行分块，也就是模仿卷积的思路，设置kernel_size=patch_size，得到分块后的图片</li><li>得到格式为[bs,num_patch,patch_depth]的张量</li><li>将张量与形状为[patch_depth,model_dim_C]的权重矩阵进行乘法操作，即可得到形状为[bs,num_patch,model_dim_C]的path_embedding ## 方法二</li><li>patch_depth等于input_channel*patch_size*patch_size</li><li>model_dim_C相当于二维卷积的输出通道数目</li><li>将形状为[patch_depth,model_dim_C]的权重矩阵转换为[model_dim_C,input_channel,patch_size,path_size]的卷积核</li><li>调用pytorch中的conv2d API得到卷积的输出张量，形状为[bs,output_channel,height,width]</li><li>转换为[bs,num_patch,model_dim_C]的格式，即为patch embedding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_naive</span>(<span class="params">image,patch_size,weight</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;直观方法实现patch_embedding&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># image shape: bs*channel*h*w</span></span><br><span class="line">    patch = F.unfold(image,kernel_size=(patch_size,patch_size),</span><br><span class="line">                    stride=(patch_size,patch_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    patch_embedding = patch @ weight</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_conv</span>(<span class="params">image,kernel,stride</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于二维卷积来实现patch_embedding，embedding的维度就是卷积的输出通道数&quot;&quot;&quot;</span></span><br><span class="line">    conv_output = F.conv2d(image,kernel,stride=stride) <span class="comment">#[bs,oc,oh.ow]</span></span><br><span class="line">    bs, oc, oh, ow = conv_output.shape</span><br><span class="line">    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">    </span><br></pre></td></tr></table></figure><h1 id="如何构建mhsa并计算其复杂度">2.如何构建MHSA并计算其复杂度？</h1><ul><li>基于输入x进行三个映射分别得到q,k,v<ul><li>此步复杂度为<span class="math inline">\(3LC^2\)</span>,其中<span class="math inline">\(L\)</span>为序列长度，<span class="math inline">\(C\)</span>为特征大小<br /></li><li><span class="math inline">\(q,k,v\)</span>维度:<span class="math inline">\([L,C]\)</span></li></ul></li><li>将q,k,v拆分成多头的形式，注意这里的多头各自计算不受影响，所以可以与bs维度进行统一看待(c-&gt;c/n，把embedding看成一个个小的embedding)</li><li>计算<span class="math inline">\(qk^T\)</span>,并考虑可能的掩码，即让无效的两两位置之间的能量为负无穷，掩码在shift window MHSA中会需要，而在window MHSA中暂不需要<ul><li><span class="math inline">\(attn\_prob=\frac{q\times k^T}{\sqrt{d}}\)</span></li><li>此步复杂度为<span class="math inline">\(L^2C\)</span></li></ul></li><li>计算概率值与<span class="math inline">\(v\)</span>的乘积<ul><li>概率维度:<span class="math inline">\([L,L]\)</span>;<span class="math inline">\(v\)</span>维度:<span class="math inline">\([L,C]\)</span></li><li>此步复杂度为<span class="math inline">\(L^2C\)</span></li></ul></li><li>对输出进行再次映射<ul><li>映射矩阵:<span class="math inline">\([C,C]\)</span>;输出矩阵:<span class="math inline">\([L,C]\)</span></li><li>此步复杂度为<span class="math inline">\(LC^2\)</span></li></ul></li><li>总体复杂度为<span class="math inline">\(4LC^2+2L^2C\)</span></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadSelfAttention</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model_dim,num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadSelfAttention,self).__init__()</span><br><span class="line">        self.num_head = num_head</span><br><span class="line">        </span><br><span class="line">        self.proj_linear_layer = nn.Linear(model_dim,<span class="number">3</span>*model_dim)</span><br><span class="line">        self.final_linear_layer = nn.Linear(model_dim,model_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span>,additive_mask = <span class="literal">None</span></span>):</span><br><span class="line">        bs, seqlen, model_dim = <span class="built_in">input</span>.shape</span><br><span class="line">        num_head = self.num_head</span><br><span class="line">        head_dim = model_dim // num_head</span><br><span class="line">        </span><br><span class="line">        proj_output = self.proj_linear_layer(<span class="built_in">input</span>)</span><br><span class="line">        q,k,v = proj_output.chunk(<span class="number">3</span>,dim=-<span class="number">1</span>) <span class="comment"># [bs,seqlen,seqlen,model_dim]</span></span><br><span class="line">        </span><br><span class="line">        q = q.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        q = q.reshape(bs*num_head,seqlen,head_dim)</span><br><span class="line">        </span><br><span class="line">        k = k.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        k = k.reshape(bs*num_head,seqlen,head_dim)</span><br><span class="line">        </span><br><span class="line">        v = v.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        v = v.reshape(bs*num_head,seqlen,head_dim) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> additive_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attn_prob = F.softmax(torch.bmm(q,k.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/math.sqrt(head_dim),dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            additive_mask = additive_mask.tile((num_head,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">            attn_prob = F.softmax(bs,num_head,seqlen,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs*num_head,seqlen,seqlen]</span></span><br><span class="line">            </span><br><span class="line">        output = torch.bmm(attn_prob,v) <span class="comment"># [bs*num_head,seqlen,head_dim]</span></span><br><span class="line">        output = output.reshape(bs,num_head,seqlen,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,seqlen,num_head,head_dim]</span></span><br><span class="line">        output = output.reshape(bs,seqlen,model_dim)</span><br><span class="line">        </span><br><span class="line">        output = self.final_linear_layer(output)</span><br><span class="line">        <span class="keyword">return</span> attn_prob,ouput</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="如何构建window-mhsa并计算其复杂度">3.如何构建Window MHSA并计算其复杂度？</h1><ul><li><p>将patch组成的图片进一步划分成一个个更大的window</p><ul><li>首先需要将三维的patch embedding转换成图片格式 [bs,channel,h,w] num_patch = h*w</li><li>使用unfold来将patch换分成window</li></ul></li><li><p>在每个window内部计算MHSA</p><ul><li><p>window数目其实可以跟batchsize进行统一对待，因为window与window之间没有交互计算</p></li><li><p>关于计算复杂度</p><ul><li><p>假设窗的边长为<span class="math inline">\(W\)</span>，起那么计算每个窗的总体复杂度是<span class="math inline">\(4W^2C^2+2W^4C\)</span></p></li><li><p>假设patch的总数目为<span class="math inline">\(L\)</span>,那么窗的数目为<span class="math inline">\(\frac{L}{W^2}\)</span></p></li><li><p>因此，W-HMSA的总体复杂度为<span class="math inline">\((4W^2C^2+2W^4C)\times\frac{L}{W^2} = 4LC^2+2LW^2C\)</span></p></li></ul></li><li><p>此处不需要mask</p></li><li><p>将计算结果转换成window的四维张量形式</p></li></ul></li><li><p>复杂度对比</p><ul><li>MHSA:<span class="math inline">\(4LC^2+2L^2C\)</span></li><li>W-MHSA:<span class="math inline">\(4LC^2+2LW^2C\)</span></li></ul><p>​</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_multi_head_self_attention</span>(<span class="params">patch_embedding,mhsa,window_size=<span class="number">4</span>,num_head=<span class="number">2</span></span>):</span><br><span class="line">    num_patch_in_window = widow_size * widow_size</span><br><span class="line">    bs, num_patch, patch_depth = patch_embedding.shape</span><br><span class="line">    image_height = image_width = <span class="built_in">int</span>(math.sqrt(num_patch))</span><br><span class="line">    </span><br><span class="line">    patch_embedding = patch_embedding,transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    patch = patch_embedding.reshape(bs,patch_depth,image_height,image_width)</span><br><span class="line">    window = F.unfold(patch,kernel_size=(widow_size,widow_size),</span><br><span class="line">                     stride=(window_size,window_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># [bs,num_window,window_depth]</span></span><br><span class="line">    bs,num_window,patch_depth_times_num_patch_in_window = window.shape</span><br><span class="line">    window = window.reshape(bs*num_window,patch_depth,num_patch_in_window).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    attn_prob, output = mhsa(window) <span class="comment"># [bs*num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    output = output.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h1 id="如何构建shift-window-mhsa及其mask">4.如何构建Shift Window MHSA及其Mask？</h1><ul><li>将上一步的W-MHSA的结果转换成图片格式</li><li>假设已经做了新的window划分，这一步叫做shift-window</li><li>为了保持window数目不变从而有高效的计算，需要将图片的patch往左和往上各自滑动半个窗口大小的步长，保持patch所属window类型不变</li><li>将图片patch还原成window的数据格式</li><li>由于cycle shift后，每个window虽然形状规整，但部分window中存在原本不属于同一个窗口的patch，所以需要生成mask</li><li>如何生成mask？<ul><li>首先构建一个shift-window的patch所属的window类别矩阵</li><li>对该矩阵进行同样的往左往上各自滑动半个窗口大小的步长的操作</li><li>通过unfold操作得到[bs,num_window,num_patch_in_window]形状的类别矩阵</li><li>对该矩阵进行扩维成[bs,num_window,num_patch_in_window,1]</li><li>将该矩阵与其转置矩阵进行作差，得到同类关系矩阵(为0的位置上的patch属于同类，否则属于不同类)</li><li>对同类关系矩阵中非零的位置用负无穷数进行填充，对于零的位置用0去填充，这样就构建好了MHSA所需要的mask</li><li>此mask的形状为[bs,num_window,num_patch_in_window,num_patch_in_window]</li></ul></li><li>将window转换成三维的格式，[bs*num_window,num_patch_in_window,patch_depth]</li><li>将三维格式的特征连同mask一起送入MHSA中计算得到注意力输出</li><li>将注意力输出转换成图片patch格式，[bs,num_window,num_patch_in_window,patch_depth]</li><li>为了恢复位置，需要将图片的patch往右和往下各自滑动半个窗口大小的步长，至此，SW-MHSA计算完毕</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/other/image-20230416152318837.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个辅助函数，window2image，也就是将transformer block的结构转换成图片的形式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window2image</span>(<span class="params">msa_output</span>):</span><br><span class="line">    bs,num_window,num_patch_in_window,patch_depth = msa_output.shape</span><br><span class="line">    window_size = <span class="built_in">int</span>(math.sqrt(num_patch_in_window))</span><br><span class="line">    image_height = <span class="built_in">int</span>(math.sqrt(num_window)) * window_size</span><br><span class="line">    image_width = image_height</span><br><span class="line">    </span><br><span class="line">    msa_output = msa_output.reshape(bs,<span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                       <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                       window_size,</span><br><span class="line">                                       window_size,</span><br><span class="line">                                       patch_depth)</span><br><span class="line">    msa_output = msa_output.transpose(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    image = msa_output.reshape(bs,image_height*image_width,patch_depth)</span><br><span class="line">    image = image.transpose(-<span class="number">1</span>,-<span class="number">2</span>).reshape(bs,patch_depth,image_height,image_width)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义辅助函数 shift_window，即高效计算sw-msa</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_window</span>(<span class="params">w_msa_output,window_size,shift_size,generate_mask=<span class="literal">False</span></span>): <span class="comment"># 只有在正向的时候才会 shift</span></span><br><span class="line">    bs,num_window,num_patch_in_window,patch_depth = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    w_msa_output = window2image(w_msa_output) <span class="comment"># [bs,depth,h,w]</span></span><br><span class="line">    bs,patch_depth,image_height,image_width = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    rolled_w_msa_output = torch.roll(w_msa_output,shifts=(shift_size,shift_size),dim=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = rolled_w_msa_output.reshape(bs,patch_depth,</span><br><span class="line">                                                     <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                                     window_size,</span><br><span class="line">                                                     <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                                     window_size)</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.transpose(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.reshape(bs,patch_depth,num_window*num_patch_in_window)</span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># [bs,num_window*num_patch_in_window,patch_depth]</span></span><br><span class="line">    shifted_window = shifted_w_msa_input.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> generate_mask:</span><br><span class="line">        additive_mask = build_mask_for_shifted_wmsa(bs,image_height,image_width,window_size) <span class="comment"># [bs,num_window,num_patch_in_windows,num_patch_in_windows]</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        additive_mask = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> shifted_window, additive_mask</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建shift window multi-head attention mask</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_mask_for_shifted_wmsa</span>(<span class="params">batch_size,image_height,image_width,window_size</span>):</span><br><span class="line">    index_metrix = torch.zeros(image_height,image_width)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(image_height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(image_width):</span><br><span class="line">            row_times = (i + window_size // <span class="number">2</span>) // window_size</span><br><span class="line">            col_times = (j + window_size // <span class="number">2</span>) // window_size</span><br><span class="line">            index_metrix[i,j] = row_times * (image_height/window_size) + col_times + <span class="number">1</span></span><br><span class="line">    rolled_index_matrix = torch.roll(index_metrix,shifts=(-window_size//<span class="number">2</span>,-window_size//<span class="number">2</span>),dims=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    rolled_index_matrix = rolled_index_matrix.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>) <span class="comment"># [bs,ch,h,w]</span></span><br><span class="line">    </span><br><span class="line">    c = F.unfold(rolled_index_matrix,kernel_size=(window_size,window_size),</span><br><span class="line">                stride=(window_size,window_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    c = c.tile(batch_size,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># [bs.num_window,num_patch_in_window]</span></span><br><span class="line">    </span><br><span class="line">    bs, num_window,num_patch_in_window = c.shape</span><br><span class="line">    </span><br><span class="line">    c1 = c.unsqueeze(-<span class="number">1</span>) <span class="comment"># [bs,num_window,num_patch_in_windows,1]</span></span><br><span class="line">    c2 = (c1-c1.transpose(-<span class="number">1</span>,-<span class="number">2</span>) == -<span class="number">0</span>) <span class="comment"># [bs,num_window,num_patch_in_windows,num_patch_in_windows]</span></span><br><span class="line">    valid_matrix = c2.to(torch.float32)</span><br><span class="line">    additive_mask = (<span class="number">1</span> - valid_matrix) * (-<span class="number">1e-9</span>) </span><br><span class="line">    </span><br><span class="line">    additive_mask = additive_mask.reshape(bs*num_window,num_patch_in_window,num_patch_in_window)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> additive_mask</span><br><span class="line">            </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_window_multi_head_self_attention</span>(<span class="params">w_msa_output,mhsa,window_size=<span class="number">4</span>,num_head=<span class="number">2</span></span>):</span><br><span class="line">    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input, additive_mask = shift_window(w_msa_output,window_size,</span><br><span class="line">                                                      shift_size=(-window_size//<span class="number">2</span>),</span><br><span class="line">                                                      generate_mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(shifted_w_msa_input.shape) <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    <span class="built_in">print</span>(additive_mask.shape) <span class="comment"># [bs*num_window,num_patch_in_window,num_patch_in_window]</span></span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.reshape(bs*num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    attn_prob,output = mhsa(shifted_w_msa_input,additive_mask=additive_mask)</span><br><span class="line">    </span><br><span class="line">    output = output.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    output, _ = shift_window(output,window_size,shift_size=window_size//<span class="number">2</span>,generate_mask=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape) <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h1 id="如何构建patch-merging">5.如何构建Patch Merging？</h1><ul><li>将window格式的特征转换成图片patch格式</li><li>利用unfold操作，按照merge_size*merge_size大小得到新的patch,形状为[bs,num_patch_new,merge_size*merge_size*patch_depth_old]</li><li>使用一个全连接层对depth进行降维成0.5倍，也就是从merge_size*merge_size*patch_depth_old映射到0.5*merge_size*merge_size*patch_depth_old</li><li>输出的是patch embedding的形状格式,[bs,num_patch,patch_depth]</li><li>举例说明：以merge_size=2为例，经过PatchMerging后，patch数目减少为之前的<span class="math inline">\(\frac{1}{4}\)</span>，但是depth增大为原来的2倍，而不是4倍</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_dim, merge_size,output_depth_scale=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PatchMerging, self).__init__()</span><br><span class="line">        self.merge_size = merge_size</span><br><span class="line">        self.model_dim = model_dim</span><br><span class="line">        self.proj_layer = nn.Linear(</span><br><span class="line">            model_dim * merge_size * merge_size,</span><br><span class="line">            <span class="built_in">int</span>(model_dim * merge_size * merge_size * output_depth_scale))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        bs, num_window, num_patch_in_window, patch_depth = <span class="built_in">input</span>.shape</span><br><span class="line">        window_size = <span class="built_in">int</span>(math.sqrt(num_patch_in_window))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">input</span> = window2image(<span class="built_in">input</span>)  <span class="comment"># [bs,patch_depth,image_h,image_w]</span></span><br><span class="line"></span><br><span class="line">        merged_window = F.unfold(<span class="built_in">input</span>, kernel_size=(self.merge_size, self.merge_size),</span><br><span class="line">                                 stride=(self.merge_size, self.merge_size)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line">        merged_window = self.proj_layer(merged_window)  <span class="comment"># [bs,num_patch,new_patch_depth]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> merged_window</span><br></pre></td></tr></table></figure><h1 id="如何构建swin-transformerblock">6.如何构建Swin TransformerBlock?</h1><ul><li>每个block包含LayerNorm，W-MHSA，MLP，SW-MHSA，残差连接等模块</li><li>输入是patch embedding格式</li><li>每个MLP包括两层，分别是4*mode_dim和mode_dim大小</li><li>输出的是window的数据格式，[bs,num_window,num_patch_in_window,patch_depth]</li><li>需要注意残差连接对数据形状的要求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model_dim,window_size,num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(SwinTransformerBlock,self).__init__()</span><br><span class="line">        self.layer_norm1 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm2 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm3 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm4 = nn.LayerNorm(model_dim)</span><br><span class="line">        </span><br><span class="line">        self.wsma_mlp1 = nn.Linear(model_dim,<span class="number">4</span>*model_dim)</span><br><span class="line">        self.wsma_mlp2 = nn.Linear(<span class="number">4</span>*model_dim,model_dim)</span><br><span class="line">        self.swsma_mlp1 = nn.Linear(model_dim,<span class="number">4</span>*model_dim)</span><br><span class="line">        self.swsma_mlp2 = nn.Linear(<span class="number">4</span>*model_dim,model_dim)</span><br><span class="line">        </span><br><span class="line">        self.mhsa1 = MultiHeadSelfAttention(model_dim,num_head)</span><br><span class="line">        self.mhsa2 = MultiHeadSelfAttention(model_dim,num_head)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        </span><br><span class="line">        bs,num_patch,patch_depth = <span class="built_in">input</span>.shape</span><br><span class="line">        </span><br><span class="line">        input1 = self.layer_norm1(<span class="built_in">input</span>)</span><br><span class="line">        w_msa_output = window_multi_head_self_attention(<span class="built_in">input</span>,self.mhsa1,window_size=<span class="number">4</span>,num_head=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#         bs, num_head, num_patch_in_window, patch_depth = w_msa_output.shape </span></span><br><span class="line">        w_msa_output = <span class="built_in">input</span> + w_msa_output.reshape(bs,num_patch,patch_depth)</span><br><span class="line">        output1 = self.wsma_mlp2(self.wsma_mlp1(self.layer_norm2(w_msa_output)))</span><br><span class="line">        output1 += w_msa_output</span><br><span class="line">        </span><br><span class="line">        input2 = self.layer_norm3(input1)</span><br><span class="line">        input2 = input2.reshape(bs,num_patch,num_patch_in_window,patch_depth)</span><br><span class="line">        sw_msa_output = shift_window_multi_head_self_attention(input2,self.mhsa2,window_size=<span class="number">4</span>,num_head=<span class="number">2</span>)</span><br><span class="line">        sw_msa_output = output1 + sw_msa_output.reshape(bs,num_patch,patch_depth)</span><br><span class="line">        output2 = self.swsma_mlp2(self.swsma_mlp1(self.layer_norm4(sw_msa_output)))</span><br><span class="line">        output2 += sw_msa_output</span><br><span class="line">        </span><br><span class="line">        output2 = output2.reshape(bs,num_patch,num_patch_in_window.patch_depth)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output2</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="如何构建swintransformermodel">7.如何构建SwinTransformerModel？</h1><ul><li>输入是图片</li><li>首先对图片进行分块并得到Patch embedding</li><li>经过第一个stage</li><li>进行patch merging,在进行第二个stage</li><li>以此类推...</li><li>对最后一个block的输出转换成patch embedding的格式[bs,num_patch_depth]</li><li>对patch embedding在时间维度进行平均池化，并映射到分类层得到分类的logits</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_image_channel=<span class="number">3</span>, patch_size=<span class="number">4</span>, model_dim_C=<span class="number">8</span>, num_classes=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">4</span>, num_head=<span class="number">2</span>, merge_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SwinTransformerModel, self).__init__()</span><br><span class="line">        patch_depth = patch_size * patch_size * input_image_channel</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.model_dim_C = model_dim_C</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.patch_embedding_weight = nn.Parameter(torch.randn(patch_depth, model_dim_C))  <span class="comment"># 模型可训练的一部分参数</span></span><br><span class="line"></span><br><span class="line">        self.block1 = SwinTransformerBlock(model_dim_C, window_size, num_head)</span><br><span class="line">        self.block2 = SwinTransformerBlock(model_dim_C * <span class="number">2</span>, window_size, num_head)</span><br><span class="line">        self.block3 = SwinTransformerBlock(model_dim_C * <span class="number">4</span>, window_size, num_head)</span><br><span class="line">        self.block4 = SwinTransformerBlock(model_dim_C * <span class="number">8</span>, window_size, num_head)</span><br><span class="line"></span><br><span class="line">        self.patch_merging1 = PatchMerging(model_dim_C , merge_size)</span><br><span class="line">        self.patch_merging2 = PatchMerging(model_dim_C * <span class="number">2</span>, merge_size)</span><br><span class="line">        self.patch_merging3 = PatchMerging(model_dim_C * <span class="number">4</span>, merge_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类</span></span><br><span class="line">        self.final_layer = nn.Linear(model_dim_C * <span class="number">8</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image</span>):</span><br><span class="line">        patch_embedding_naive = image2emb_naive(image, self.patch_size, self.patch_embedding_weight)</span><br><span class="line">        <span class="built_in">print</span>(patch_embedding_naive.shape)</span><br><span class="line"></span><br><span class="line">        kernel = self.patch_embedding_weight.transpose(<span class="number">0</span>, <span class="number">1</span>).reshape((-<span class="number">1</span>, ic, patch_size, patch_size))  <span class="comment"># oc*ic*kh*kw</span></span><br><span class="line">        patch_embedding_conv = image2emb_conv(image, kernel, self.patch_size)  <span class="comment"># 二维卷积的方法得到embedding</span></span><br><span class="line">        <span class="built_in">print</span>(patch_embedding_conv.shape)</span><br><span class="line">        <span class="comment"># block1</span></span><br><span class="line">        patch_embedding = patch_embedding_naive</span><br><span class="line">        <span class="built_in">print</span>(patch_embedding.shape)</span><br><span class="line">        sw_msa_output = self.block1(patch_embedding)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block1_output&quot;</span>, sw_msa_output.shape)  <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line"></span><br><span class="line">        merged_patch1 = self.patch_merging1(sw_msa_output)</span><br><span class="line">        sw_msa_output_1 = self.block2(merged_patch1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block2_output&quot;</span>, sw_msa_output_1.shape)</span><br><span class="line"></span><br><span class="line">        merged_patch2 = self.patch_merging2(sw_msa_output_1)</span><br><span class="line">        sw_msa_output_2 = self.block3(merged_patch2)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block3_output&quot;</span>, sw_msa_output_2.shape)</span><br><span class="line"></span><br><span class="line">        merged_patch3 = self.patch_merging3(sw_msa_output_2)</span><br><span class="line">        sw_msa_output_3 = self.block4(merged_patch3)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block4_output&quot;</span>, sw_msa_output_3.shape)</span><br><span class="line"></span><br><span class="line">        bs, num_window, num_patch_in_window, patch_depth = sw_msa_output_3.shape</span><br><span class="line">        sw_msa_output_3 = sw_msa_output_3.reshape(bs, -<span class="number">1</span>, patch_depth)</span><br><span class="line"></span><br><span class="line">        pool_output = torch.mean(sw_msa_output_3, dim=<span class="number">1</span>)</span><br><span class="line">        logits = self.final_layer(pool_output)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;logits&quot;</span>,logits.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="模型测试代码">8.模型测试代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get parameters amount in the network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_n_params</span>(<span class="params">model</span>):</span><br><span class="line">    pp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">list</span>(model.parameters()):</span><br><span class="line">        nn = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">list</span>(p.size()):</span><br><span class="line">            nn = nn * s</span><br><span class="line">        pp += nn</span><br><span class="line">    <span class="keyword">return</span> pp</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    bs, ic, image_h, image_w = <span class="number">4</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">    patch_size = <span class="number">4</span></span><br><span class="line">    model_dim_C = <span class="number">8</span>  <span class="comment"># 一开始的patch embedding大小</span></span><br><span class="line">    max_num_token = <span class="number">16</span></span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    window_size = <span class="number">4</span></span><br><span class="line">    num_head = <span class="number">2</span></span><br><span class="line">    merge_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    patch_depth = patch_size * patch_size * ic</span><br><span class="line">    image = torch.randn(bs, ic, image_h, image_w)</span><br><span class="line"></span><br><span class="line">    model = SwinTransformerModel(ic, patch_size, model_dim_C, num_classes, window_size, num_head, merge_size)</span><br><span class="line"></span><br><span class="line">    model(image)</span><br><span class="line">    pp = get_n_params(model)</span><br><span class="line">    <span class="built_in">print</span>(pp)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152245800.png&quot; alt=&quot;image-20230415224534491&quot; /&gt;&lt;fig</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目2-基于GCN(DNN)的文本分类模型(GPU)</title>
    <link href="https://wangtongyouwen.github.io/post/ceb60e40.html"/>
    <id>https://wangtongyouwen.github.io/post/ceb60e40.html</id>
    <published>2023-04-15T11:19:44.000Z</published>
    <updated>2023-04-17T12:56:58.717Z</updated>
    
    <content type="html"><![CDATA[<p>https://pytorch.org/tutorials/distributed/home.html</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152009298.png" alt="image-20230415200908110" /><figcaption aria-hidden="true">image-20230415200908110</figcaption></figure><h2 id="单机单卡">1 单机单卡</h2><p>在 main 函数前加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is available!&quot;</span>)</span><br><span class="line">    os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is not available! Exit!&quot;</span>)</span><br></pre></td></tr></table></figure><p>在train函数中，第一次调用模型的地方加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.cuda() <span class="comment"># 拷贝到GPU上，模型拷贝</span></span><br></pre></td></tr></table></figure><p>在train函数中，每次使用到数据的地方加入(train and eval)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">token_index = token_index.cuda()  <span class="comment"># tensor cuda拷贝方法,数据拷贝</span></span><br><span class="line">target = target.cuda() <span class="comment"># 数据拷贝</span></span><br><span class="line">eval_target = eval_target.cuda()  <span class="comment"># tensor cuda拷贝方法,数据拷贝</span></span><br><span class="line">eval_token_index = eval_token_index.cuda()  <span class="comment"># 数据拷贝</span></span><br></pre></td></tr></table></figure><h2 id="单机多卡">2 单机多卡</h2><p>在 main 函数前加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is available!&quot;</span>)</span><br><span class="line">    os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">        logging.warning(<span class="string">f&quot;find <span class="subst">&#123;torch.cuda.device_count()&#125;</span> GPUs!&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logging.warning(<span class="string">&quot;Too few GPU!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is not available! Exit!&quot;</span>)</span><br></pre></td></tr></table></figure><p>在train函数中，第一次调用模型的地方加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model.cuda(),device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 拷贝到GPU上，模型拷贝,放入DataParallel</span></span><br></pre></td></tr></table></figure><p>在模型的保存中，修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % save_step_interval == <span class="number">0</span>:</span><br><span class="line">    os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    save_file = os.path.join(save_path, <span class="string">f&quot;step_<span class="subst">&#123;step&#125;</span>.pt&quot;</span>)</span><br><span class="line">    torch.save(&#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">&quot;model_state_dict&quot;</span>: model.module.state_dict(),</span><br><span class="line">...</span><br><span class="line">    &#125;, save_file)</span><br><span class="line">    logging.warning(<span class="string">f&quot;checkpoint has been saved in <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这种方式更加合理：</p><figure><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230415211135334.png" alt="image-20230415211135334" /><figcaption aria-hidden="true">image-20230415211135334</figcaption></figure><h3 id="init_process_group">2.1 init_process_group</h3><p>This sets up the communication backend for distributed training (such as NCCL, Gloo, etc.) and initializes the process group.</p><h4 id="nccl">2.1.1 nccl</h4><p>https://developer.nvidia.com/nccl</p><h4 id="world_size">2.1.2 world_size</h4><p>当前节点上有多少张GPU</p><h4 id="local_rank">2.1.3 local_rank</h4><p>当前进程在某张确定的GPU卡上(因为这里采用了多线程，每个线程表示一张GPU)</p><h3 id="torch.cuda.set_deviceargs.local_rank">2.2 torch.cuda.set_device(args.local_rank)</h3><p>设定某张卡进行训练</p><h3 id="对模型进行包裹">2.3 对模型进行包裹</h3><p>类似DataParallel中的操作</p><h3 id="train_sampler">2.4 train_sampler</h3><p>https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler</p><p>把dataset中的样本分配当不同的GPU上面，这是随机分配的</p><p>It is especially useful in <strong>conjunction</strong> with class:torch.nn.parallel.<strong>DistributedDataParallel</strong></p><p>每张卡上面的参数的总数量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.num_samples = math.ceil(<span class="built_in">len</span>(self.dataset) / self.num_replicas)  <span class="comment"># type: ignore[arg-<span class="built_in">type</span>]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span><br><span class="line">        <span class="keyword">if</span> self.shuffle:</span><br><span class="line">            <span class="comment"># deterministically shuffle based on epoch and seed</span></span><br><span class="line">            g = torch.Generator()</span><br><span class="line">            g.manual_seed(self.seed + self.epoch)  <span class="comment"># 不改变种子和epoch，顺序是相同的(显然不合理)</span></span><br><span class="line">            indices = torch.randperm(<span class="built_in">len</span>(self.dataset), generator=g).tolist()  <span class="comment"># type: </span></span><br></pre></td></tr></table></figure><p>显然在需要在每个训练之前对此进行修改，调用sampler.set_epoch方法，把epoch传入进来。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152228839.png" alt="image-20230415222851173" /><figcaption aria-hidden="true">image-20230415222851173</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://pytorch.org/tutorials/distributed/home.html&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/bl</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目1-基于GCN+DNN的文本分类模型</title>
    <link href="https://wangtongyouwen.github.io/post/27f58942.html"/>
    <id>https://wangtongyouwen.github.io/post/27f58942.html</id>
    <published>2023-04-15T07:04:41.000Z</published>
    <updated>2023-04-17T12:55:13.613Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目介绍">1 项目介绍</h2><p>IMDB dataset having 50K movie reviews for natural language processing or Text analytics.</p><p>This is a dataset for <strong>binary</strong> sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of <strong>25,000</strong> highly polar movie reviews for training and <strong>25,000</strong> for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.</p><p>代码构成：</p><ul><li>GCNN模型</li><li>简单版embeddingbag+DNN模型</li><li>yield_tokens: 对源 comment 进行分词处理</li><li>collate_fn： 对DataLoader所生成的mini-batch进行后处理</li><li>train</li><li>main函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> IMDB</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.imdb <span class="keyword">import</span> NUM_LINES</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.WARN, stream=sys.stdout,</span><br><span class="line">                    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s (%(module)s:%(lineno)d) %(levelname)s:%(message)s&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = <span class="number">15000</span> <span class="comment"># 这里的统计是根据数据集进行处理后得到的</span></span><br><span class="line">train_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;train&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&quot;basic_english&quot;</span>) </span><br><span class="line">vocab = build_vocab_from_iterator(yield_tokens(train_data_iter, tokenizer), min_freq=<span class="number">20</span>, specials=[<span class="string">&quot;&lt;unk&gt;&quot;</span>]) <span class="comment"># 只把出现频率高于20个的词语取出来，其他的单词都变成&lt;unk&gt; # 构建词表</span></span><br><span class="line">vocab.set_default_index(<span class="number">0</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;单词表大小：<span class="subst">&#123;<span class="built_in">len</span>(vocab)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果长时间下载失败，建议直接在以下链接进行下载：</p><p>http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</p><p>之后把下载好的文件放在当前目录下的<code>.data</code>中</p><h2 id="gcnn模型">2 GCNN模型</h2><h3 id="介绍">2.1 介绍</h3><p>https://arxiv.org/pdf/1612.08083v3.pdf</p><p>门控卷积网络是一种将卷积网络与门控机制相结合的语言模型。使用零填充以确保未来的语境无法被观察。门控卷积层可以层次化地叠加在其他层之上。通过自适应softmax层来获取模型预测结果。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304151758805.png" alt="image-20230415175810671" style="zoom:67%;" /></p><h3 id="代码">2.2 代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocal_size=VOCAB_SIZE, embedding_dim=<span class="number">64</span>, num_class=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GCNN, self).__init__()  <span class="comment"># 对父类进行初始化</span></span><br><span class="line"></span><br><span class="line">        self.embedding_table = nn.Embedding(vocal_size, embedding_dim)</span><br><span class="line">        nn.init.xavier_uniform_(self.embedding_table.weight)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;xavier_uniform的出现是为了训练过程中前后的方差稳定问题，正确的初始化有利于训练的稳定；</span></span><br><span class="line"><span class="string">        Xavier初始化表明，对于每⼀层，输出的⽅差不受输⼊数量的影响，任何梯度的⽅差不受输出数量的影响。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.conv_A_1 = nn.Conv1d(embedding_dim, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)  <span class="comment"># input_dim,output_dim,kernel_Size</span></span><br><span class="line">        self.conv_B_1 = nn.Conv1d(embedding_dim, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        self.conv_A_2 = nn.Conv1d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line">        self.conv_B_2 = nn.Conv1d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        self.output_linear1 = nn.Linear(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.output_linear2 = nn.Linear(<span class="number">128</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, word_index</span>):</span><br><span class="line">        <span class="comment"># 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1.通过word_index得到word_embedding</span></span><br><span class="line">        <span class="comment"># word_index shape: [bs,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        word_embedding = self.embedding_table(word_index)  <span class="comment"># [bs,max_seq_len,embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2.编写第一层ID门卷积模块</span></span><br><span class="line">        word_embedding = word_embedding.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [bs,embedding_dim,max_seq_len]</span></span><br><span class="line">        A = self.conv_A_1(word_embedding)</span><br><span class="line">        B = self.conv_B_1(word_embedding)</span><br><span class="line">        H = A * torch.sigmoid(B)  <span class="comment"># [bs,64,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        A = self.conv_A_2(H)</span><br><span class="line">        B = self.conv_B_2(H)</span><br><span class="line">        H = A * torch.sigmoid(B)  <span class="comment"># [bs,64,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.池化并进过全连接层</span></span><br><span class="line">        pool_output = torch.mean(H, dim=-<span class="number">1</span>)  <span class="comment"># 平均池化 得到 [bs,64]</span></span><br><span class="line">        linear1_output = self.output_linear1(pool_output)</span><br><span class="line">        logits = self.output_linear2(linear1_output)  <span class="comment"># [bs,2]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h2 id="简单版embeddingbagdnn模型">3 简单版embeddingbag+DNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextClassificationModel</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=VOCAB_SIZE, embed_dim=<span class="number">64</span>, num_class=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TextClassificationModel, self).__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token_index</span>):</span><br><span class="line">        embedded = self.embedding(token_index)  <span class="comment"># shape: [bs,embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure><p>通过embeddingbag可以省略平均池化层操作，变得更加简单</p><figure><img src="https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=1024" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="yield_tokens">4 yield_tokens</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">train_data_iter, tokenizer</span>):</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_iter):</span><br><span class="line">        label, comment = sample</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(comment)  <span class="comment"># 把一句话转换成一个个token的列表</span></span><br></pre></td></tr></table></figure><h2 id="collate_fn">5 collate_fn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>): </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;mini-batch 中最重要的就是对同一个批次内的数据进行统一处理，比如某些句子很短，需要padding，这样才能进行batch操作&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对dataloader所生成的mini-batch进行后处理&quot;&quot;&quot;</span></span><br><span class="line">    target = []</span><br><span class="line">    token_index = []</span><br><span class="line">    max_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (label, comment) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch):</span><br><span class="line">        tokens = tokenizer(comment)</span><br><span class="line">        token_index.append(vocab(tokens))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens) &gt; max_length:</span><br><span class="line">            max_length = <span class="built_in">len</span>(tokens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> label == <span class="string">&quot;pos&quot;</span>:</span><br><span class="line">            target.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    token_index = [index + [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(index)) <span class="keyword">for</span> index <span class="keyword">in</span> token_index]</span><br><span class="line">    <span class="keyword">return</span> (torch.tensor(target).to(torch.int64), torch.tensor((token_index)).to(torch.int32))  <span class="comment"># target：pos/neg token_index</span></span><br></pre></td></tr></table></figure><h2 id="train">6 train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_data_loader, eval_data_loader, model, optimizer, num_epoch, log_step_interval, save_step_interval,</span></span><br><span class="line"><span class="params">          eval_step_interval, save_path, resume=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;此处的data_loader是map-style dataset&quot;&quot;&quot;</span></span><br><span class="line">    start_epoch = <span class="number">0</span></span><br><span class="line">    start_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> resume != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="comment"># 加载之前训练过的模型的参数文件</span></span><br><span class="line">        logging.warning(<span class="string">f&quot;loading from <span class="subst">&#123;resume&#125;</span>&quot;</span>)</span><br><span class="line">        checkpoint = torch.load(resume)</span><br><span class="line">        model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">        optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">        start_epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">        start_step = checkpoint[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch_index <span class="keyword">in</span> <span class="built_in">range</span>(start_epoch, num_epoch):</span><br><span class="line">        ema_loss = <span class="number">0.</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;https://www.investopedia.com/terms/e/ema.asp&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        num_bathes = <span class="built_in">len</span>(train_data_loader)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch_index, (target, token_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_loader):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            step = num_bathes * epoch_index + batch_index + <span class="number">1</span></span><br><span class="line">            logits = model(token_index)</span><br><span class="line">            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits),</span><br><span class="line">                                              F.one_hot(target, num_classes=<span class="number">2</span>).to(torch.float32))  <span class="comment"># 维度需要相同，把target转换</span></span><br><span class="line">            ema_loss = <span class="number">0.9</span> * ema_loss + <span class="number">0.1</span> * bce_loss  <span class="comment"># 指数平均loss</span></span><br><span class="line">            bce_loss.backward()  <span class="comment"># 梯度回传</span></span><br><span class="line">            nn.utils.clip_grad_norm(model.parameters(), <span class="number">0.1</span>)</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;https://blog.csdn.net/Mikeyboi/article/details/119522689&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % log_step_interval == <span class="number">0</span>:</span><br><span class="line">                logging.warning(</span><br><span class="line">                    <span class="string">f&quot;epoch_index:<span class="subst">&#123;epoch_index&#125;</span>,batch_index:<span class="subst">&#123;batch_index&#125;</span>,ema_loss:<span class="subst">&#123;ema_loss.item()&#125;</span>&quot;</span>)  <span class="comment"># 避免使用张量的形式打印，而是使用python格式，防止印象性能</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % save_step_interval == <span class="number">0</span>:</span><br><span class="line">                os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">                save_file = os.path.join(save_path, <span class="string">f&quot;step_<span class="subst">&#123;step&#125;</span>.pt&quot;</span>)</span><br><span class="line">                torch.save(&#123;</span><br><span class="line">                    <span class="string">&quot;epoch&quot;</span>: epoch_index,</span><br><span class="line">                    <span class="string">&quot;step&quot;</span>: step,</span><br><span class="line">                    <span class="string">&quot;model_state_dict&quot;</span>: model.state_dict(),</span><br><span class="line">                    <span class="string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: bce_loss</span><br><span class="line">                &#125;, save_file)</span><br><span class="line">                logging.warning(<span class="string">f&quot;checkpoint has been saved in <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % eval_step_interval == <span class="number">0</span>:</span><br><span class="line">                logging.warning(<span class="string">&quot;start to do evaluation...&quot;</span>)</span><br><span class="line">                model.<span class="built_in">eval</span>()  <span class="comment"># evaluation ,only forward calculation</span></span><br><span class="line">                eval_ema_loss = <span class="number">0</span></span><br><span class="line">                total_acc_account = <span class="number">0</span></span><br><span class="line">                total_account = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> eval_batch_index, (eval_target, eval_token_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_data_loader):</span><br><span class="line">                    total_account += eval_target.shape[<span class="number">0</span>]</span><br><span class="line">                    eval_logits = model(eval_token_index)</span><br><span class="line">                    total_acc_account += (torch.argmax(eval_logits, dim=-<span class="number">1</span>) == eval_target).<span class="built_in">sum</span>().item()</span><br><span class="line">                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(logits),</span><br><span class="line">                                                           F.one_hot(target, num_classes=<span class="number">2</span>).to(torch.float32))</span><br><span class="line">                    eval_ema_loss = <span class="number">0.9</span> * eval_ema_loss + <span class="number">0.1</span> * eval_bce_loss  <span class="comment"># 指数平均loss</span></span><br><span class="line">                    acc = total_acc_account / total_account  <span class="comment"># 精确度：一样的次数/总次数</span></span><br><span class="line">                logging.warning(</span><br><span class="line">                    <span class="string">f&quot;eval_ema_loss:<span class="subst">&#123;eval_ema_loss.item()&#125;</span>,eval_acc:<span class="subst">&#123;acc.item()&#125;</span>&quot;</span>)</span><br><span class="line">                model.train()</span><br></pre></td></tr></table></figure><h2 id="main">7 main</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = GCNN()</span><br><span class="line">    <span class="comment"># model = TextClassificationModel()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型总参数&quot;</span>, <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()))</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    train_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;train&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">    train_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(train_data_iter), batch_size=BATCH_SIZE,</span><br><span class="line">                                                    collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">    eval_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;test&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">    eval_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(eval_data_iter), batch_size=<span class="number">8</span>,</span><br><span class="line">                                                   collate_fn=collate_fn) <span class="comment"># 变成map_style </span></span><br><span class="line">    resume = <span class="string">&#x27;F:\study\code\pytorch\logs_imdb_text_classification\step_500.pt&#x27;</span></span><br><span class="line">    train(train_data_loader, eval_data_loader, model, optimizer, num_epoch=<span class="number">10</span>, log_step_interval=<span class="number">20</span>,</span><br><span class="line">          save_step_interval=<span class="number">500</span>,</span><br><span class="line">          eval_step_interval=<span class="number">300</span>, save_path=<span class="string">&#x27;./logs_imdb_text_classification&#x27;</span>, resume=resume)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;项目介绍&quot;&gt;1 项目介绍&lt;/h2&gt;
&lt;p&gt;IMDB dataset having 50K movie reviews for natural language processing or Text analytics.&lt;/p&gt;
&lt;p&gt;This is a dat</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络6-GRU</title>
    <link href="https://wangtongyouwen.github.io/post/f22b56ba.html"/>
    <id>https://wangtongyouwen.github.io/post/f22b56ba.html</id>
    <published>2023-04-14T09:47:29.000Z</published>
    <updated>2023-04-30T06:06:46.513Z</updated>
    
    <content type="html"><![CDATA[<p>torch.nn.GRU(<strong>args<em>, </em></strong>kwargs*)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU <span class="math display">\[\begin{aligned}r_{t} &amp; =\sigma\left(W_{i r} x_{t}+b_{i r}+W_{h r} h_{(t-1)}+b_{h r}\right) \\z_{t} &amp; =\sigma\left(W_{i z} x_{t}+b_{i z}+W_{h z} h_{(t-1)}+b_{h z}\right) \\n_{t} &amp; =\tanh \left(W_{i n} x_{t}+b_{i n}+r_{t} *\left(W_{h n} h_{(t-1)}+b_{h n}\right)\right) \\h_{t} &amp; =\left(1-z_{t}\right) * n_{t}+z_{t} * h_{(t-1)}\end{aligned}\]</span> 同等<code>hidden size</code>的参数量，GRU是LSTM的<span class="math inline">\(\frac{3}{4}\)</span></p><p>何时使用GRU，何时使用LSTM？</p><p>https://arxiv.org/pdf/1412.3555.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304141848732.png" alt="image-20230414180541344" /><figcaption aria-hidden="true">image-20230414180541344</figcaption></figure><h2 id="api">1 API</h2><h3 id="parameters">1.1 Parameters</h3><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional GRU. Default: <code>False</code></li></ul><h2 id="实现">2 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru_forward</span>(<span class="params"><span class="built_in">input</span>,initial_states,w_ih,w_hh,b_ih,b_hh</span>):</span><br><span class="line">    prev_h = initial_states</span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对权重扩维，复制成batch_size倍</span></span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    output = torch.zeros(bs,T,h_size) <span class="comment"># GRU网络的输出状态序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># t时刻的GRU cell的输入特征向量 [bs,i_size]</span></span><br><span class="line">        w_time_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment">#[bs,3*i_size,1]</span></span><br><span class="line">        w_time_x = w_time_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_time_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment">#[bs,3*i_size,1]</span></span><br><span class="line">        w_time_h_prev = w_time_h_prev.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        r_t = torch.sigmoid(w_time_x[:,:h_size] + w_time_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size]) <span class="comment"># 重置门</span></span><br><span class="line">        z_t = torch.sigmoid(w_time_x[:,h_size:<span class="number">2</span>*h_size] + w_time_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size]) <span class="comment"># 更新门</span></span><br><span class="line">        </span><br><span class="line">        n_t = torch.tanh(w_time_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size]+b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size]+</span><br><span class="line">                         r_t*(w_time_h_prev[:,<span class="number">2</span>*h_size: <span class="number">3</span>*h_size]+b_hh[<span class="number">2</span>*h_size: <span class="number">3</span>*h_size]))   <span class="comment"># 候选状态</span></span><br><span class="line">        </span><br><span class="line">        prev_h = (<span class="number">1</span>-z_t)*n_t + z_t*prev_h <span class="comment"># 增量更新得到当前时刻最新隐含状态</span></span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,prev_h</span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size)</span><br><span class="line">h0 = torch.randn(bs,h_size)</span><br><span class="line"></span><br><span class="line">gru_layer = nn.GRU(i_size,h_size,batch_first = <span class="literal">True</span>)</span><br><span class="line">output,h_final = gru_layer(<span class="built_in">input</span>,h0.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">output_custom,h_final_custom = gru_forward(<span class="built_in">input</span>,h0,gru_layer.weight_ih_l0,gru_layer.weight_hh_l0,gru_layer.bias_ih_l0,gru_layer.bias_hh_l0)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,h_final_custom))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(output,output_custom))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;torch.nn.GRU(&lt;strong&gt;args&lt;em&gt;, &lt;/em&gt;&lt;/strong&gt;kwargs*)&lt;/p&gt;
&lt;p&gt;https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#t</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络5-LSTM</title>
    <link href="https://wangtongyouwen.github.io/post/23f44fab.html"/>
    <id>https://wangtongyouwen.github.io/post/23f44fab.html</id>
    <published>2023-04-13T10:13:13.000Z</published>
    <updated>2023-04-30T06:05:06.917Z</updated>
    
    <content type="html"><![CDATA[<p>https://colah.github.io/posts/2015-08-Understanding-LSTMs/</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131816130.png" alt="image-20230413181558550" /><figcaption aria-hidden="true">image-20230413181558550</figcaption></figure><h2 id="整体介绍">1 整体介绍</h2><p><span class="math display">\[\begin{array}{l}i_{t}=\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \\f_{t}=\sigma\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \\g_{t}=\tanh \left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \\o_{t}=\sigma\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\right) \\c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot g_{t} \\h_{t}=o_{t} \odot \tanh \left(c_{t}\right)\end{array}\]</span></p><ul><li><p><span class="math inline">\(h_t\)</span>: hidden state at time <span class="math inline">\(t\)</span></p></li><li><p><span class="math inline">\(c_t\)</span>: cell state at time <span class="math inline">\(t\)</span></p></li><li><p><span class="math inline">\(x_t\)</span>: input at time <span class="math inline">\(t\)</span></p></li><li><p><span class="math inline">\(h_{t-1}\)</span>: hidden state of the layer at time <span class="math inline">\(t-1\)</span> or the initial hidden state at time o</p></li><li><p><span class="math inline">\(i_t\)</span>: input</p></li><li><p><span class="math inline">\(f_t\)</span>: forget</p></li><li><p><span class="math inline">\(g_t\)</span>: cell</p></li><li><p><span class="math inline">\(o_t\)</span>: output gates</p></li><li><p><span class="math inline">\(\sigma\)</span>: sigmoid function</p></li><li><p><span class="math inline">\(\odot\)</span>: Hadamard product</p></li><li><p><span class="math inline">\(N\)</span> = batch size</p></li><li><p><span class="math inline">\(L\)</span> = sequence length</p></li><li><p><span class="math inline">\(D\)</span> = 2 if bidirectional = True otherwise 1</p></li><li><p><span class="math inline">\(H_{in}\)</span> = input_size</p></li><li><p><span class="math inline">\(H_{cell}\)</span> = hidden_size</p></li><li><p><span class="math inline">\(H_{out}\)</span> = pro_size if pro_size &gt; 0 otherwise hidden_size</p></li></ul><h3 id="parameters">1.1 Parameters</h3><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional LSTM. Default: <code>False</code></li><li><strong>proj_size</strong> – If <code>&gt; 0</code>, will use LSTM with projections of corresponding size. Default: 0 减少LTSM的参数和计算量</li></ul><h3 id="inputs-input-h_0-c_0">1.2 Inputs: input, (h_0, c_0)</h3><ul><li>input：<ul><li><span class="math inline">\((L,H_{in})\)</span>-&gt; unbatched input</li><li><span class="math inline">\((L,N,H_{in})\)</span>-&gt;<code>batch_first=False</code></li><li><span class="math inline">\((N,L,H_{in})\)</span>-&gt;<code>batch_first=True</code></li></ul></li><li>h_0: Defaults to zeros if (h_0, c_0) is not provided<ul><li><span class="math inline">\((D*num\_layers,H_{out})\)</span> for unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{out})\)</span> containing the initial hidden state for each element in the input sequence.</li></ul></li><li>c_0: Defaults to zeros if (h_0, c_0) is not provided<ul><li><span class="math inline">\((D*num\_layers,H_{cell})\)</span> for unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{cell})\)</span> containing the initial cell state for each element in the input sequence.</li></ul></li></ul><h3 id="outputs-output-h_n-c_n">1.3 Outputs: output, (h_n, c_n)</h3><ul><li>output:<ul><li><span class="math inline">\((L,D*H_{out})\)</span>-&gt;unbatched input</li><li><span class="math inline">\((L,N,D*H_{out})\)</span>-&gt;<code>batch_first=False</code> containing the output features (h_t) from the last layer of the LSTM,for each t</li><li><span class="math inline">\((N,L,D*H_{out})\)</span>-&gt;<code>batch_first=True</code> containing the output features (h_t) from the last layer of the LSTM,for each t</li><li>If a <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> has been given as the input, the output will also be a packed sequence</li><li>When <code>bidirectional=True</code>, output will contain a concatenation of the forward and reverse hidden states at each time step in the sequence</li></ul></li><li>h_n:<ul><li><span class="math inline">\((D*num\_layers,H_{out})\)</span>-&gt; unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{out})\)</span>-&gt;containing the final hidden state for each element in the sequence.</li><li>When <code>bidirectional=True</code>, h_n will contain a concatenation of the final forward and reverse hidden states, respectively.</li></ul></li><li>c_n:<ul><li><span class="math inline">\((D*num\_layers,H_{cell})\)</span>-&gt; unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{cell})\)</span>-&gt;containing the final hidden state for each element in the sequence.</li><li>When <code>bidirectional=True</code>, h_n will contain a concatenation of the final forward and reverse hidden states, respectively.</li></ul></li></ul><h3 id="variables">1.4 Variables</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131856525.png" alt="image-20230413185629132" /><figcaption aria-hidden="true">image-20230413185629132</figcaption></figure><h2 id="调用api">2 调用API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line"><span class="comment"># proj_size = </span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size) <span class="comment"># 输入序列</span></span><br><span class="line">c0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line">h0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用官方LSTM API</span></span><br><span class="line">lstm_layer = nn.LSTM(i_size,h_size,batch_first=<span class="literal">True</span>)</span><br><span class="line">output,(h_final,c_final) = lstm_layer(<span class="built_in">input</span>,(h0.unsqueeze(<span class="number">0</span>),c0.unsqueeze(<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k,v.shape)</span><br></pre></td></tr></table></figure><h2 id="lstm-without-proj_size">3 LSTM without proj_size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自己写一个LSTM模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_forward</span>(<span class="params"><span class="built_in">input</span>,inittal_states,w_ih,w_hh,b_ih,b_hh</span>):</span><br><span class="line">    h0,c0 = inittal_states <span class="comment"># 初始状态</span></span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = c0</span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,i_size)</span></span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,h_size)</span></span><br><span class="line">    </span><br><span class="line">    output_size = h_size</span><br><span class="line">    output = torch.zeros(bs,T,output_size) <span class="comment"># 输出序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># 当前时刻的输入向量 (bs,i_size)</span></span><br><span class="line">        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_x = w_times_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_h_prev = w_times_h_prev.squeeze(-<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分別计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)</span></span><br><span class="line">        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])</span><br><span class="line">        f_t = torch.sigmoid(w_times_x[:,h_size:<span class="number">2</span>*h_size] + w_times_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size])</span><br><span class="line">        g_t = torch.tanh(w_times_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + w_times_h_prev[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_hh[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size])</span><br><span class="line">        o_t = torch.sigmoid(w_times_x[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + w_times_h_prev[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_ih[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_hh[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size])</span><br><span class="line"></span><br><span class="line">        prev_c = f_t * prev_c + i_t * g_t</span><br><span class="line">        prev_h = o_t * torch.tanh(prev_c)</span><br><span class="line">        </span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,(prev_h,prev_c)</span><br><span class="line">    </span><br><span class="line">custom_output,(custom_h_final,custom_c_final) = lstm_forward(<span class="built_in">input</span>,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)    </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_output,output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,custom_h_final))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(c_final,custom_c_final))</span><br></pre></td></tr></table></figure><h2 id="lstm-with-proj_size">4 LSTM with proj_size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">proj_size = <span class="number">3</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size) <span class="comment"># 输入序列</span></span><br><span class="line">c0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line">h0 = torch.randn(bs,proj_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用官方LSTM API</span></span><br><span class="line">lstm_layer = nn.LSTM(i_size,h_size,batch_first=<span class="literal">True</span>,proj_size=proj_size)</span><br><span class="line">output,(h_final,c_final) = lstm_layer(<span class="built_in">input</span>,(h0.unsqueeze(<span class="number">0</span>),c0.unsqueeze(<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k,v)</span><br></pre></td></tr></table></figure><p>eight_ih_l0 torch.Size([20, 4]) h_size<em>4,i_size weight_hh_l0 torch.Size([20, 3]) h_size</em>4,proj_size bias_ih_l0 torch.Size([20]) h_size<em>4 bias_hh_l0 torch.Size([20]) h_size</em>4 weight_hr_l0 torch.Size([3, 5]) proj_size,h_size 只会对 h_state 进行改变，不会对 c_state 进行改变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_forward</span>(<span class="params"><span class="built_in">input</span>,inittal_states,w_ih,w_hh,b_ih,b_hh,w_hr=<span class="literal">None</span></span>):</span><br><span class="line">    h0,c0 = inittal_states <span class="comment"># 初始状态</span></span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = c0</span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,i_size)</span></span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,h_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> w_hr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_size = w_hr.shape[<span class="number">0</span>]</span><br><span class="line">        output_size = p_size</span><br><span class="line">        batch_w_hr = w_hr.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,proj_size,h_size)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output_size = h_size</span><br><span class="line">    output = torch.zeros(bs,T,output_size) <span class="comment"># 输出序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># 当前时刻的输入向量 (bs,i_size)</span></span><br><span class="line">        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_x = w_times_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_h_prev = w_times_h_prev.squeeze(-<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分別计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)</span></span><br><span class="line">        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])</span><br><span class="line">        f_t = torch.sigmoid(w_times_x[:,h_size:<span class="number">2</span>*h_size] + w_times_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size])</span><br><span class="line">        g_t = torch.tanh(w_times_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + w_times_h_prev[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_hh[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size])</span><br><span class="line">        o_t = torch.sigmoid(w_times_x[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + w_times_h_prev[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_ih[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_hh[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size])</span><br><span class="line"></span><br><span class="line">        prev_c = f_t * prev_c + i_t * g_t</span><br><span class="line">        prev_h = o_t * torch.tanh(prev_c)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> w_hr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 做projection</span></span><br><span class="line">            prev_h = torch.bmm(batch_w_hr,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># bs,proj_size,1</span></span><br><span class="line">            prev_h = prev_h.squeeze(-<span class="number">1</span>) <span class="comment"># bs,proj_size</span></span><br><span class="line">            </span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,(prev_h,prev_c)</span><br><span class="line">    </span><br><span class="line">custom_output,(custom_h_final,custom_c_final) = lstm_forward(<span class="built_in">input</span>,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,</span><br><span class="line">                                                             lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0,lstm_layer.weight_hr_l0)    </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_output,output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,custom_h_final))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(c_final,custom_c_final))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络4-RNN</title>
    <link href="https://wangtongyouwen.github.io/post/b945630.html"/>
    <id>https://wangtongyouwen.github.io/post/b945630.html</id>
    <published>2023-04-13T06:29:13.000Z</published>
    <updated>2023-04-30T06:04:14.307Z</updated>
    
    <content type="html"><![CDATA[<p>https://www.cs.toronto.edu/~graves/preprint.pdf</p><p>Supervised Sequence Labelling with Recurrent Neural Networks</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131437010.png" alt="image-20230413143700100" style="zoom:67%;" /></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131437818.png" alt="image-20230413143710020" style="zoom:67%;" /></p><p>delay: 能看到前几帧的信息，牺牲一定的时间，预测的更加准确。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131448069.png" alt="image-20230413144847329" /><figcaption aria-hidden="true">image-20230413144847329</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131502377.png" alt="image-20230413150234647" /><figcaption aria-hidden="true">image-20230413150234647</figcaption></figure><h2 id="api">1 API</h2><p>torch.nn.RNN(<strong>args<em>, </em></strong>kwargs*)</p><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN</p><p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.</p><p>For each element in the input sequence, each layer computes the following function: <span class="math display">\[h_t = tanh(x_tW_{ih}^T+b_{ih}+h_{t-1}W_{hh}^T+b_{hh})\]</span> where <span class="math inline">\(h_t\)</span> is the hidden state at time t, <span class="math inline">\(x_t\)</span> is the input at time t, and <span class="math inline">\(h_{t-1}\)</span> is the hidden state of the previous layer at time t-1 or the inital hidden state at time 0. If nonlinearity is "relu", then ReLU is used instead of tanh.</p><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</li><li><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code>'tanh'</code> or <code>'relu'</code>. Default: <code>'tanh'</code></li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional RNN. Default: <code>False</code></li></ul><p>Inputs: input, h_0</p><ul><li><p>inputs: tensor of shape <span class="math inline">\((L,H_{in})\)</span> for unbatched input, <span class="math inline">\((L,N,H_{in})\)</span> when <code>batch_first=False</code> or <span class="math inline">\((N,L,H_{in})\)</span> when <code>batch_first=True</code></p></li><li><p>h_0: tensor of shape <span class="math inline">\((D*num\_layer,H_{out})\)</span> for unbatched input or$ (D*num_layers,N,H_{out})$containing the initial hidden state for the input sequence batch. Defaults to zeros if not provided.</p></li></ul><p><span class="math display">\[N=batch\ size \\L = sequence\ length \\D = 2\ if\ bidirectional = True\ otherwise\ 1 \\H_{in} = input \_size \\H_{out} = hidden\_size\]</span></p><p>Outputs: output, h_n</p><h2 id="代码实现">2 代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 单向，单层RNN</span></span><br><span class="line">single_rnn = nn.RNN(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,batch_first=<span class="literal">True</span>) <span class="comment"># feature_size * hidden_size * layer_num</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>) <span class="comment"># batch_size*sequence_length*feature_size</span></span><br><span class="line">output,h_n = single_rnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output) <span class="comment"># batch_size*sequence_length*hidden_size</span></span><br><span class="line"><span class="built_in">print</span>(h_n) <span class="comment"># layer_num*batch_size*hidden_size</span></span><br><span class="line"><span class="comment"># 双向，单层RNN</span></span><br><span class="line">bidirectional_rnn = nn.RNN(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,batch_first=<span class="literal">True</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line">bi_output,bi_h_n = bidirectional_rnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(bi_output) <span class="comment"># batch_size*sequence_length*(2*hidden_size)</span></span><br><span class="line"><span class="built_in">print</span>(bi_h_n) <span class="comment"># (2*layer_num)*batch_size*hidden_size</span></span><br></pre></td></tr></table></figure><h2 id="实现单向单层rnn">3 实现单向单层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step2: 手写一个rnn_forward函数,实现单向rnn的计算原理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_forward</span>(<span class="params"><span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev</span>):</span><br><span class="line">    bs,T,input_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_dim = weight_ih.shape[<span class="number">0</span>]</span><br><span class="line">    h_out = torch.zeros(bs,T,h_dim) <span class="comment"># 初始化一个输出(状态)矩阵</span></span><br><span class="line">    <span class="comment"># h_prev: bs*hidden_size</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:].unsqueeze(<span class="number">2</span>) <span class="comment">#获取当前时刻输入 # bs*input_size*1</span></span><br><span class="line">        w_ih_batch = weight_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># bs*h_dim*input_size</span></span><br><span class="line">        w_hh_batch = weight_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># bs*h_dim*h_dim</span></span><br><span class="line">        </span><br><span class="line">        w_time_x = torch.bmm(w_ih_batch,x).squeeze(-<span class="number">1</span>) <span class="comment"># bs*h_dim</span></span><br><span class="line">        w_time_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(<span class="number">2</span>)).squeeze(-<span class="number">1</span>) <span class="comment">#bs*h_dim</span></span><br><span class="line">        h_prev = torch.tanh(w_time_x + bias_ih + w_time_h + bias_hh) <span class="comment"># t时刻的输出</span></span><br><span class="line">        </span><br><span class="line">        h_out[:,t,:] = h_prev</span><br><span class="line">    <span class="keyword">return</span> h_out,h_prev.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 验证一下rnn_forward的正确性</span></span><br><span class="line"><span class="comment"># for k,v in rnn.named_parameters():</span></span><br><span class="line"><span class="comment">#     print(k,v)</span></span><br><span class="line">custom_rnn_output,custom_state_final = run_forward(<span class="built_in">input</span>,rnn.weight_ih_l0,rnn.weight_hh_l0,rnn.bias_ih_l0,rnn.bias_hh_l0,h_prev)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_rnn_output,rnn_output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_state_final,state_final))</span><br></pre></td></tr></table></figure><h2 id="实现双向单层rnn">4 实现双向单层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3: 手写一个bidirectional_rnn_forward函数，实现双向RNN的计算原理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bidirectional_run_forward</span>(<span class="params"><span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev,</span></span><br><span class="line"><span class="params">                              weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse</span>):</span><br><span class="line">    bs,T,input_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_dim = weight_ih.shape[<span class="number">0</span>]</span><br><span class="line">    h_out = torch.zeros(bs,T,h_dim*<span class="number">2</span>) <span class="comment"># 初始化一个输出(状态)矩阵</span></span><br><span class="line">    </span><br><span class="line">    forward_output = run_forward(<span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev)[<span class="number">0</span>] <span class="comment"># forward layer</span></span><br><span class="line">    backward_output = run_forward(torch.flip(<span class="built_in">input</span>,[<span class="number">1</span>]),</span><br><span class="line">                      weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse)[<span class="number">0</span>]  <span class="comment"># flip 对dim进行翻转, backward layer</span></span><br><span class="line">    </span><br><span class="line">    h_out[:,:,:h_dim] = forward_output</span><br><span class="line">    h_out[:,:,h_dim:] = torch.flip(backward_output,[<span class="number">1</span>]) <span class="comment"># 反向的结果需要在T维度上再次反向，才能与api结果相同</span></span><br><span class="line">    </span><br><span class="line">    h_n = torch.zeros(<span class="number">2</span>,bs,h_dim)</span><br><span class="line">    h_n[<span class="number">0</span>,:,:] = forward_output[:,-<span class="number">1</span>,:]</span><br><span class="line">    h_n[<span class="number">1</span>,:,:] = backward_output[:,-<span class="number">1</span>,:]</span><br><span class="line">        <span class="comment"># h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)</span></span><br><span class="line">    <span class="keyword">return</span> h_out, h_n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证一下 bidirectional_rnn_forward正确性</span></span><br><span class="line">bi_rnn = nn.RNN(input_size,hidden_size,batch_first=<span class="literal">True</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line">h_prev =torch.zeros(<span class="number">2</span>,bs,hidden_size)</span><br><span class="line">bi_rnn_output,bi_state_final = bi_rnn(<span class="built_in">input</span>,h_prev)</span><br><span class="line"></span><br><span class="line">custom_bi_rnn_output,custom_bi_state_final = bidirectional_run_forward(<span class="built_in">input</span>,</span><br><span class="line">                                                                       bi_rnn.weight_ih_l0,</span><br><span class="line">                                                                       bi_rnn.weight_hh_l0,</span><br><span class="line">                                                                       bi_rnn.bias_ih_l0,</span><br><span class="line">                                                                       bi_rnn.bias_hh_l0,</span><br><span class="line">                                                                       h_prev[<span class="number">0</span>],</span><br><span class="line">                                                                       bi_rnn.weight_ih_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.weight_hh_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.bias_ih_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.bias_hh_l0_reverse,</span><br><span class="line">                                                                       h_prev[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(bi_rnn_output,custom_bi_rnn_output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(bi_state_final,custom_bi_state_final))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://www.cs.toronto.edu/~graves/preprint.pdf&lt;/p&gt;
&lt;p&gt;Supervised Sequence Labelling with Recurrent Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;ht</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>用pytorch实现基础网络1-卷积网络</title>
    <link href="https://wangtongyouwen.github.io/post/1229d3b9.html"/>
    <id>https://wangtongyouwen.github.io/post/1229d3b9.html</id>
    <published>2023-04-12T15:59:34.000Z</published>
    <updated>2023-04-30T06:35:26.305Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101930110.png" alt="image-20230410193006020" style="zoom:80%;" /></p><h2 id="卷积的api">1 卷积的API</h2><h3 id="conv2d">1.1 CONV2D</h3><p>torch.nn.Conv2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode='zeros'</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101932268.png" alt="image-20230410193201892" style="zoom:67%;" /></p><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels in the input image</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – Size of the convolving kernel</li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – Padding added to all four sides of the input. Default: 0</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304111953529.png" alt="image-20230411195325050" /><figcaption aria-hidden="true">image-20230411195325050</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">in_channels = <span class="number">1</span></span><br><span class="line">out_channels = <span class="number">1</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">bias = <span class="literal">False</span></span><br><span class="line">input_size = [in_channels,<span class="number">4</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">conv_layer = nn.Conv2d(in_channels,out_channels,kernel_size,bias=bias)</span><br><span class="line">input_feature_map = torch.randn(input_size)</span><br><span class="line">output_feature_map = conv_layer(input_feature_map) <span class="comment"># 直接调用卷积这个方法</span></span><br><span class="line"><span class="built_in">print</span>(input_feature_map,<span class="string">&#x27;\n&#x27;</span>,output_feature_map)</span><br><span class="line"><span class="built_in">print</span>(conv_layer.weight) <span class="comment"># 1*1*3*3 out_channels*in_channels*height*width</span></span><br></pre></td></tr></table></figure><h3 id="functional.conv2d">1.2 FUNCTIONAL.CONV2D</h3><p>torch.nn.functional.conv2d(<em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112015956.png" alt="image-20230411201534888" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_feature_map1 = F.conv2d(input_feature_map,conv_layer.weight) <span class="comment"># functional,需要传入卷积的weight</span></span><br><span class="line"><span class="built_in">print</span>(output_feature_map)</span><br><span class="line"><span class="built_in">print</span>(output_feature_map1)</span><br></pre></td></tr></table></figure><h2 id="padding-and-stride">2 padding and stride</h2><p>https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html</p><h3 id="padding">2.1 padding</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112028509.png" alt="image-20230411202806742" /><figcaption aria-hidden="true">image-20230411202806742</figcaption></figure><p>In general, if we add a total of <span class="math inline">\(p_h\)</span> rows of padding (roughly half on top and half on bottom) and a total of <span class="math inline">\(p_w\)</span> columns of padding (roughly half on the left and half on the right), the output shape will be <span class="math display">\[(n_k - k_h + p_h + 1)\times (n_w - k_w + p_w + 1)\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We define a helper function to calculate convolutions. It initializes the</span></span><br><span class="line"><span class="comment"># convolutional layer weights and performs corresponding dimensionality</span></span><br><span class="line"><span class="comment"># elevations and reductions on the input and output</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># (1, 1) indicates that batch size and the number of channels are both 1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># Strip the first two dimensions: examples and channels</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 row and column is padded on either side, so a total of 2 rows or columns</span></span><br><span class="line"><span class="comment"># are added</span></span><br><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We use a convolution kernel with height 5 and width 3. The padding on either</span></span><br><span class="line"><span class="comment"># side of the height and width are 2 and 1, respectively</span></span><br><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><h3 id="stride">2.2 stride</h3><p>In general, when the stride for the height is <span class="math inline">\(s_h\)</span> and the stride for the width is <span class="math inline">\(s_w\)</span>, the output shape is <span class="math display">\[[(n_h-k_h+p_h+s_h)/s_h] \times [((n_w-k_w+p_w+s_w)/s_w)]\]</span> f we set<span class="math inline">\(p_h =k_h -1\)</span> and <span class="math inline">\(p_w = k_w -1\)</span>, then the output shape can be simplified to <span class="math inline">\([(n_h+s_h-1)/s_h\times (n_w+s_w-1)/s_w]\)</span>. Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be <span class="math inline">\((n_h/s_h)\times (n_w/s_w)\)</span></p><p>note: the [] means floor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><h3 id="demo">2.3 demo</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># With square kernels and equal stride</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding and dilation</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>), dilation=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><h2 id="multiple-input-and-multiple-output-channels">3 Multiple Input and Multiple Output Channels</h2><h3 id="multiple-input-channels">3.1 Multiple Input Channels</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112057860.png" alt="image-20230411205730182" /><figcaption aria-hidden="true">image-20230411205730182</figcaption></figure><h3 id="multiple-output-channels">3.2 Multiple Output Channels</h3><p>we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater <em>channel depth</em>. Intuitively, you could think of each channel as responding to a different set of features.</p><p>把每个输出通道都看做一个单独的操作，最后stack起来得到结果</p><h2 id="矩阵运算实现卷积操作">4 矩阵运算实现卷积操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 卷积的输入特征图</span></span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># 卷积核</span></span><br><span class="line">bias = torch.randn(<span class="number">1</span>) <span class="comment"># 卷积偏置，默认输出通道数目等于1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 用原始的矩阵运算来实现二维卷积,先不考虑 batchsize 维度和 channel 维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d</span>(<span class="params"><span class="built_in">input</span>,kernel,bias = <span class="number">0</span>,stride = <span class="number">1</span>,padding = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding))</span><br><span class="line">        </span><br><span class="line">    input_h,input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros((output_h,output_w))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">            region = <span class="built_in">input</span>[i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">            output[<span class="built_in">int</span>(i/stride)][<span class="built_in">int</span>(j/stride)] = torch.<span class="built_in">sum</span>(region * kernel) + bias <span class="comment"># 点乘，并赋值给输出位置的元素</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">mat_mul_conv_output = matrix_multiplication_for_conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(mat_mul_conv_output)</span><br><span class="line">pytorch_api_conv_output = F.conv2d(<span class="built_in">input</span>.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="built_in">input</span>.shape[<span class="number">0</span>],<span class="built_in">input</span>.shape[<span class="number">1</span>]),kernel.reshape(<span class="number">1</span>,<span class="number">1</span>,kernel.shape[<span class="number">0</span>],kernel.shape[<span class="number">1</span>]),bias=bias,padding=<span class="number">1</span>).squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(pytorch_api_conv_output)</span><br></pre></td></tr></table></figure><h2 id="向量内积实现卷积操作">5 向量内积实现卷积操作</h2><h3 id="flatten">5.1 flatten</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step2 用原始的矩阵运算来实现二维卷积,先不考虑 batchsize 维度和 channel 维度, flatten 版本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_flatten</span>(<span class="params"><span class="built_in">input</span>,kernel,bias = <span class="number">0</span>,stride = <span class="number">1</span>,padding = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding))</span><br><span class="line">        </span><br><span class="line">    input_h,input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros((output_h,output_w))</span><br><span class="line">    region_matrix = torch.zeros(output.numel(),kernel.numel()) <span class="comment"># 存储所有的拉平后的特征区域</span></span><br><span class="line">    kernel_matrix = kernel.reshape((kernel.numel(),<span class="number">1</span>)) <span class="comment"># 变为列向量,kernel的列向量形式</span></span><br><span class="line">    row_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">            region = <span class="built_in">input</span>[i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">            region_vetor = torch.flatten(region) </span><br><span class="line">            region_matrix[row_index] = region_vetor</span><br><span class="line">            row_index += <span class="number">1</span></span><br><span class="line">    output_matrix = region_matrix @ kernel_matrix</span><br><span class="line">    output = output_matrix.reshape((output_h,output_w)) + bias</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># flatten input</span></span><br><span class="line">pytorch_api_conv_output = F.conv2d(<span class="built_in">input</span>.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="built_in">input</span>.shape[<span class="number">0</span>],<span class="built_in">input</span>.shape[<span class="number">1</span>]),</span><br><span class="line">                                   kernel.reshape(<span class="number">1</span>,<span class="number">1</span>,kernel.shape[<span class="number">0</span>],kernel.shape[<span class="number">1</span>]),</span><br><span class="line">                                   bias=bias,padding=<span class="number">1</span>).squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line">mat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_api_conv_output,mat_mul_conv_output_flatten))</span><br></pre></td></tr></table></figure><p>这里可以使用torch.unfold实现flatten操作</p><p>torch.nn.Unfold(<em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html</p><h3 id="考虑batchsize维度和channel维度">5.2 考虑batchsize维度和channel维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 用原始的矩阵运算来实现二维卷积，考虑batchsize维度和channel维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_full</span>(<span class="params"><span class="built_in">input</span>,kernel,bias=<span class="number">0</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span></span>):</span><br><span class="line">    <span class="comment"># input和kernel 都是4维张量 </span></span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)) <span class="comment"># w,h,input_channel,batchsize</span></span><br><span class="line">        </span><br><span class="line">    bs,in_channel,input_h,input_w = <span class="built_in">input</span>.shape <span class="comment"># batchsize,in_channel,input_h,input_w</span></span><br><span class="line">    out_channel,in_channel,kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        bias = torch.zeros(out_channel)</span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros(bs,out_channel,output_h,output_w)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(bs):</span><br><span class="line">        <span class="keyword">for</span> oc <span class="keyword">in</span> <span class="built_in">range</span>(out_channel):</span><br><span class="line">            <span class="keyword">for</span> ic <span class="keyword">in</span> <span class="built_in">range</span>(in_channel):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">                        region = <span class="built_in">input</span>[ind,ic,i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">                        output[ind,oc,<span class="built_in">int</span>(i/stride),<span class="built_in">int</span>(j/stride)] += torch.<span class="built_in">sum</span>(region * kernel[oc,ic]) <span class="comment"># 点乘，并赋值给输出位置的元素</span></span><br><span class="line">            output[ind,oc] += bias[oc]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 卷积的输入特征图(batchsize,in_channel,in_h,in_w)</span></span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># 卷积核(out_channel,in_channel,kernel_h,kernel_w)</span></span><br><span class="line">bias = torch.randn(<span class="number">3</span>) <span class="comment"># 卷积偏置，默认输出通道数目等于1</span></span><br><span class="line">pytorch_conv2d_api_output = F.conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line">mm_conv2d_full_output = matrix_multiplication_for_conv2d_full(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_conv2d_api_output,mm_conv2d_full_output))</span><br></pre></td></tr></table></figure><h2 id="转置卷积">6 转置卷积</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积</span></span><br><span class="line"><span class="comment"># 把input 和 kernel 都 resize 列向量(把kernel空缺的位置用0填充) 不考虑padding,假设stride=1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_kernel_matrix</span>(<span class="params">kernel,input_size,stride=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠后的矩阵&quot;&quot;&quot;</span></span><br><span class="line">    kernel_h, kernel_w = kernel.shape</span><br><span class="line">    input_h,input_w = input_size</span><br><span class="line">    num_out_feature_map = (math.floor((input_h - kernel_h)/stride) + <span class="number">1</span>) * (math.floor((input_w - kernel_w)/stride) + <span class="number">1</span>)</span><br><span class="line">    result = torch.zeros((num_out_feature_map,input_h*input_w)) <span class="comment"># 初始化结果矩阵，输出特征图元素个数*输入特征图元素个数</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h-kernel_h+<span class="number">1</span>,stride):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w-kernel_w+<span class="number">1</span>,stride):</span><br><span class="line">            padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j),) <span class="comment"># 上下左右</span></span><br><span class="line">            result[count] = padded_kernel.flatten()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">kernel_matrix = get_kernel_matrix(kernel,<span class="built_in">input</span>.shape) <span class="comment"># 4*16</span></span><br><span class="line"><span class="built_in">print</span>(kernel)</span><br><span class="line"><span class="built_in">print</span>(kernel_matrix)</span><br></pre></td></tr></table></figure><h3 id="验证二维卷积">6.1 验证二维卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试1：验证二维卷积</span></span><br><span class="line">pytorch_conv2d_output = F.conv2d(<span class="built_in">input</span>.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),kernel.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)) <span class="comment"># output 2*2</span></span><br><span class="line"><span class="comment"># 因为计算得到的 mm_conv2d_output 是列向量，所以需要reshape为大小一致的矩阵，再进行比较</span></span><br><span class="line">mm_conv2d_output = (kernel_matrix @ <span class="built_in">input</span>.reshape((-<span class="number">1</span>,<span class="number">1</span>))).reshape(pytorch_conv2d_output.shape).transpose(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 通过矩阵乘积来计算卷积</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(mm_conv2d_output,pytorch_conv2d_output))</span><br></pre></td></tr></table></figure><h3 id="验证二维转置卷积">6.2 验证二维转置卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试2：验证二维转置卷积</span></span><br><span class="line"><span class="comment"># 2*2 -&gt; 4*4 一般用于上采样过程 output 的 feature map 恢复到输入的 feature map 的 size</span></span><br><span class="line"><span class="comment"># 卷积的梯度,后项传播实现 √</span></span><br><span class="line"><span class="comment"># 使用填充的方式实现 ×</span></span><br><span class="line"><span class="comment"># kernel_matrix 转置(16*4) * mm_conv2d_output(4*1)</span></span><br><span class="line">pytorch_transposed_conv2d_output = F.conv_transpose2d(pytorch_conv2d_output,kernel.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)) <span class="comment"># API</span></span><br><span class="line">mm_transposed_conv2d_output = kernel_matrix.transpose(-<span class="number">1</span>,-<span class="number">2</span>) @ mm_conv2d_output <span class="comment"># 通过矩阵乘积来计算卷积(反卷积)</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_transposed_conv2d_output,mm_transposed_conv2d_output.reshape(pytorch_transposed_conv2d_output.shape)))</span><br></pre></td></tr></table></figure><p>torch.nn.ConvTranspose2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em>, <em>padding_mode='zeros'</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d</p><p>torch.nn.functional.conv_transpose2d(<em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html</p><h2 id="group">7 group</h2><ul><li><strong>groups</strong> (int,optional) – Number of blocked connections from input channels to output channels. Default: 1</li></ul><h2 id="dilation">8 dilation</h2><ul><li><strong>dilation</strong> (int or tuple,optional) – Spacing between kernel elements. Default: 1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">7</span>,<span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">3</span>])     <span class="comment"># dilation=1</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">5</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># dilation=2</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">7</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">7</span>:<span class="number">3</span>]) <span class="comment"># dilation=3</span></span><br></pre></td></tr></table></figure><h2 id="最终版本">9 最终版本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_final</span>(<span class="params"><span class="built_in">input</span>,kernel,bias=<span class="literal">None</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>,dilation=<span class="number">1</span>,groups=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    bs, in_channel, input_h, input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    out_channel,_, kernel_h, kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> out_channel % groups == <span class="number">0</span> <span class="keyword">and</span> in_channel % groups == <span class="number">0</span>, <span class="string">&quot;group必须要同时被输入通道数和输出通道数整除！&quot;</span></span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.reshape((bs, groups, in_channel//groups, input_h, input_w))</span><br><span class="line">    kernel = kernel.reshape((groups, out_channel//groups, in_channel//groups, kernel_h, kernel_w))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># dilation (往原来的kernel中加入空洞)</span></span><br><span class="line">    kernel_h = (kernel_h - <span class="number">1</span>) * (dilation - <span class="number">1</span>) + kernel_h</span><br><span class="line">    kernel_w = (kernel_w - <span class="number">1</span>) * (dilation - <span class="number">1</span>) + kernel_w    </span><br><span class="line">    <span class="comment"># 输出结果的 feature map</span></span><br><span class="line">    output_h = math.floor((input_h - kernel_h) / stride) + <span class="number">1</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w) / stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    output_shape = (bs,groups,out_channel//groups,output_h,output_w)</span><br><span class="line">    output = torch.zeros(output_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        bias = torch.zeros(out_channel)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(bs):                                                                           <span class="comment"># 对 batchsize进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> <span class="built_in">range</span>(groups):                                                                     <span class="comment"># 对群组进行遍历</span></span><br><span class="line">            <span class="keyword">for</span> oc <span class="keyword">in</span> <span class="built_in">range</span>(out_channel // groups):                                                 <span class="comment"># 对分组后的输出通道进行遍历</span></span><br><span class="line">                <span class="keyword">for</span> ic <span class="keyword">in</span> <span class="built_in">range</span>(in_channel // groups):                                              <span class="comment"># 对分组厚的输入通道进行遍历</span></span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h-kernel_h+<span class="number">1</span>,stride):                                    <span class="comment"># 对kernel高度遍历</span></span><br><span class="line">                        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w-kernel_w+<span class="number">1</span>,stride):                                <span class="comment"># 对kernel宽度遍历</span></span><br><span class="line">                            region = <span class="built_in">input</span>[ind, g, ic, i:i+kernel_h:dilation,j:j+kernel_w:dilation]   <span class="comment"># 特征区域</span></span><br><span class="line">                            output[ind,g,oc,<span class="built_in">int</span>(i/stride),<span class="built_in">int</span>(j/stride)] += torch.<span class="built_in">sum</span>(region * kernel[g,oc,ic])</span><br><span class="line">                output[ind,g,oc] += bias[g*(out_channel//groups)+oc]                                <span class="comment"># 考虑偏置</span></span><br><span class="line">    output = output.reshape((bs,out_channel,output_h,output_w))                                     <span class="comment"># 还原成4维</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">bs, in_channel, input_h, input_w = <span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span></span><br><span class="line">out_channel = <span class="number">4</span></span><br><span class="line">groups, dilation, stride, padding = <span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs, in_channel, input_h, input_w)</span><br><span class="line">kernel = torch.randn(out_channel,in_channel//groups,kernel_size,kernel_size)</span><br><span class="line">bias = torch.randn(out_channel)</span><br><span class="line"></span><br><span class="line">pytorch_conv2d_api_output = F.conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=padding,stride=stride,dilation=dilation,groups=groups)</span><br><span class="line">mm_conv2d_final_output = matrix_multiplication_for_conv2d_final(<span class="built_in">input</span>,kernel,bias=bias,stride=stride,padding=padding,dilation=dilation,groups=groups)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_conv2d_api_output,mm_conv2d_final_output))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101930110.png&quot; alt=&quot;image-20230410193006020&quot; style=&quot;zoom:8</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/categories/pytorch/network/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="network" scheme="https://wangtongyouwen.github.io/tags/network/"/>
    
  </entry>
  
</feed>
