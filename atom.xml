<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jyh blog</title>
  
  
  <link href="https://wangtongyouwen.github.io/atom.xml" rel="self"/>
  
  <link href="https://wangtongyouwen.github.io/"/>
  <updated>2023-04-20T09:28:35.505Z</updated>
  <id>https://wangtongyouwen.github.io/</id>
  
  <author>
    <name>jyh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch基础入门20-MAE</title>
    <link href="https://wangtongyouwen.github.io/post/f0f056f4.html"/>
    <id>https://wangtongyouwen.github.io/post/f0f056f4.html</id>
    <published>2023-04-19T13:21:04.000Z</published>
    <updated>2023-04-20T09:28:35.505Z</updated>
    
    <content type="html"><![CDATA[<h1 id="masked-autoencoders-are-scalable-vision-learners">Masked Autoencoders Are Scalable Vision Learners</h1><p><a href="https://arxiv.org/pdf/2111.06377.pdf">2111.06377.pdf (arxiv.org)</a></p><p>本文表明，掩蔽自编码器（MAE）是计算机视觉领域可扩展的自监督学习方法。我们的 MAE 方法很简单：我们随机遮盖输入图像的一部分区域，并重建丢失的像素。该方法基于两个核心设计。首先，我们开发了一种非对称的编码器-解码器架构，其中编码器仅对可见子图像区域（没有遮盖标记）进行操作，而轻量级的解码器则从潜在表示和遮盖标记中重建原始图像。其次，我们发现遮盖输入图像较大比例（例如 75%）会产生一个具有意义且难度适中的自监督任务。将这两个设计结合起来，使我们能够高效、有效地训练大型模型：我们加速训练（至少提高 3 倍）并提高准确性。我们的可扩展方法允许学习高容量模型以实现更好的泛化：例如，一个普通的 ViT-Huge 模型在仅使用 ImageNet-1K 数据的方法中达到了最高准确率（87.8%）。在下游任务中，迁移性能超过了有监督预训练，并展示出有希望的扩展行为。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304192127666.png" alt="image-20230419212658615" /><figcaption aria-hidden="true">image-20230419212658615</figcaption></figure><p>image-&gt;patch-&gt;random mask-&gt;shuffle the patch-&gt;encoder-&gt;combine the whole embedding-&gt;unshuffle to align all tokens with target-&gt;decoder</p><p>自编码器是一种可以重构原始信号的方法，它有一个编码器，将观察到的信号映射到潜在表示，以及一个解码器，从潜在表示和掩码标记中重构原始信号。与传统的自编码器不同，我们采用了一种不对称的设计，允许编码器仅对部分观察到的信号（不带掩码标记）进行操作，并使用轻量级解码器从潜在表示和掩码标记中重构完整信号。</p><h2 id="mask">mask</h2><p>其中对于掩码的部分，并不是空向量，而是一个可以学习的向量，可以用过对其学习而恢复完整信号。因为图片中存在大量的冗余信息，所以这个掩码比例通常很高，比如75%</p><p>作者将图像分成规则的非重叠块，然后对其进行采样并遮蔽剩余的块。他们的采样策略很简单：随机采样块，不重复，遵循均匀分布。高遮蔽比率的随机采样大大减少了冗余，从而创造了一个不能通过可见邻近块外推来轻松解决的任务。均匀分布防止了潜在的中心偏差（即图像中心附近有更多的遮蔽块）。最后，高度稀疏的输入为设计高效编码器提供了机会。</p><p>重要的是，不需要任何<strong>专门的稀疏操作</strong>。首先，我们为每个输入图像块生成一个标记（通过线性投影并添加<strong>位置嵌入</strong>）。接下来，我们随机打乱标记列表，并根据遮盖比例删除列表的最后一部分。这个过程为<strong>编码器产生了一个小的标记子集</strong>，相当于在不重复采样的情况下采样图像块。编码后，我们将一列遮盖标记添加到编码后的图像块列表中，并对整个列表进行反打乱操作（反转随机打乱操作）以使所有标记与其目标对齐。解码器应用于这个完整列表（添加位置嵌入）。如前所述，不需要稀疏操作。这种简单的实现引入的开销可忽略不计，因为洗牌和反洗牌操作很快。</p><h2 id="encoder">encoder</h2><p>这个编码器类似ViT的结构，但仅应用于可见的、未遮盖的图像块。与标准的 ViT 一样，我们的编码器通过线性投影和添加位置嵌入对图像块进行嵌入，然后通过一系列 Transformer 模块处理生成的集合。然而，我们的编码器仅在完整集合的一小部分（例如，25%）上进行操作。遮盖的图像块被移除；不使用遮盖标记。这使我们能够仅用一部分计算和内存资源训练非常大的编码器。</p><h2 id="decoder">decoder</h2><p>MAE解码器的输入是完整的标记集合，包括：（i）编码后的可见图像块，以及（ii）遮盖标记。每个遮盖标记是一个共享的、可学习的向量，表示需要预测的缺失图像块的存在。我们为完整集合中的所有标记添加位置嵌入；如果没有这个，遮盖标记将无法获取关于它们在图像中的位置的信息。</p><p>MAE 解码器仅在预训练阶段(回归任务)用于执行图像重建任务（只有编码器用于生成图像表示以进行识别）。因此，解码器架构可以灵活地设计，其设计方式独立于编码器设计。我们尝试使用非常小的解码器，比编码器更窄、更浅。例如，我们默认的解码器每个标记的计算量不到编码器的10%。借助这种非对称设计，完整的标记集合仅由轻量级解码器处理，从而大幅减少预训练时间。</p><ul><li>解码器的目标是完成这个自回归任务，是为了更好的获得存在掩码的编码器，通过编码器才能完成cv的常见任务。</li><li>解码器输出中的每个元素都是代表一个图像块的像素值向量。解码器的最后一层是线性投影，其输出通道数量等于图像块中的像素值数量</li><li>损失函数计算像素空间中重建图像与原始图像之间的均方误差(MSE),仅在遮盖的图像块上计算损失</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304192302724.png" alt="image-20230419230225444" /><figcaption aria-hidden="true">image-20230419230225444</figcaption></figure><h1 id="code">code</h1><p>https://github.com/facebookresearch/mae</p><h2 id="models_mae">1 models_mae</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201412692.png" alt="image-20230420141204420" /><figcaption aria-hidden="true">image-20230420141204420</figcaption></figure><h3 id="模型搭建">1.1 模型搭建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedAutoencoderViT</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Masked Autoencoder with VisionTransformer backbone</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">1024</span>, depth=<span class="number">24</span>, num_heads=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, norm_layer=nn.LayerNorm, norm_pix_loss=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># MAE encoder specifics</span></span><br><span class="line">        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)</span><br><span class="line">        num_patches = self.patch_embed.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line"></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># MAE decoder specifics</span></span><br><span class="line">        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.mask_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, decoder_embed_dim))</span><br><span class="line"></span><br><span class="line">        self.decoder_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, decoder_embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line"></span><br><span class="line">        self.decoder_blocks = nn.ModuleList([</span><br><span class="line">            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(decoder_depth)])</span><br><span class="line"></span><br><span class="line">        self.decoder_norm = norm_layer(decoder_embed_dim)</span><br><span class="line">        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**<span class="number">2</span> * in_chans, bias=<span class="literal">True</span>) <span class="comment"># decoder to patch</span></span><br><span class="line">        <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">        self.norm_pix_loss = norm_pix_loss</span><br><span class="line"></span><br><span class="line">        self.initialize_weights()</span><br></pre></td></tr></table></figure><h3 id="embedding的构造">1.2 embedding的构造</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span>, flatten=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]</span><br><span class="line">        self.flatten = flatten</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="初始化参数">1.3 初始化参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># initialization</span></span><br><span class="line">    <span class="comment"># initialize (and freeze) pos_embed by sin-cos embedding</span></span><br><span class="line">    pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">    self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">    self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize patch_embed like nn.Linear (instead of nn.Conv2d)</span></span><br><span class="line">    w = self.patch_embed.proj.weight.data</span><br><span class="line">    torch.nn.init.xavier_uniform_(w.view([w.shape[<span class="number">0</span>], -<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># timm&#x27;s trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)</span></span><br><span class="line">    torch.nn.init.normal_(self.cls_token, std=<span class="number">.02</span>)</span><br><span class="line">    torch.nn.init.normal_(self.mask_token, std=<span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize nn.Linear and nn.LayerNorm</span></span><br><span class="line">    self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">        <span class="comment"># we use xavier_uniform following official JAX ViT:</span></span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">        nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><ul><li>copy?</li></ul><p><code>pos_embed</code> 是一个位置嵌入矩阵，用于捕捉序列中元素的相对或绝对位置信息。在 Transformer 网络中，位置嵌入用于将位置信息与输入嵌入相结合，从而帮助模型处理输入序列。</p><p><code>get_2d_sincos_pos_embed</code> 函数生成了一个基于正弦和余弦函数的二维位置嵌入矩阵。在这种情况下，<code>pos_embed</code> 是一个预先初始化的 PyTorch 张量，而 <code>get_2d_sincos_pos_embed</code> 返回的是一个 NumPy 数组。</p><p><code>copy_()</code> 函数的目的是将 NumPy 数组的值复制到预先分配的 PyTorch 张量中。直接将值赋给 <code>pos_embed</code> 变量会导致以下问题：</p><ol type="1"><li><p>数据类型不匹配：<code>get_2d_sincos_pos_embed</code> 返回的 NumPy 数组可能具有与 PyTorch 张量不同的数据类型。使用 <code>copy_()</code> 函数可以确保在复制过程中自动执行必要的类型转换。在这里，<code>.float().unsqueeze(0)</code> 用于将 NumPy 数组转换为 PyTorch 张量，并确保其具有正确的维度和数据类型。</p></li><li><p>张量的引用问题：直接将值赋给 <code>pos_embed</code> 变量可能会更改原始张量的引用，这可能会导致意外的行为。<code>copy_()</code> 函数确保只有张量的值被修改，而不更改其引用。这对于在模型中保持预期的参数更新行为很重要。</p></li></ol><p>综上所述，使用 <code>copy_()</code> 函数将位置嵌入矩阵的值复制到预先分配的 PyTorch 张量中，可以确保正确处理数据类型转换，并在保持张量引用不变的情况下更新张量的值。</p><h3 id="patch-and-unpatch">1.4 patch and unpatch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">patchify</span>(<span class="params">self, imgs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: (N, 3, H, W)</span></span><br><span class="line"><span class="string">    x: (N, L, patch_size**2 *3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p = self.patch_embed.patch_size[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> imgs.shape[<span class="number">2</span>] == imgs.shape[<span class="number">3</span>] <span class="keyword">and</span> imgs.shape[<span class="number">2</span>] % p == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    h = w = imgs.shape[<span class="number">2</span>] // p</span><br><span class="line">    x = imgs.reshape(shape=(imgs.shape[<span class="number">0</span>], <span class="number">3</span>, h, p, w, p))</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nchpwq-&gt;nhwpqc&#x27;</span>, x)</span><br><span class="line">    x = x.reshape(shape=(imgs.shape[<span class="number">0</span>], h * w, p**<span class="number">2</span> * <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unpatchify</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x: (N, L, patch_size**2 *3)</span></span><br><span class="line"><span class="string">    imgs: (N, 3, H, W)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p = self.patch_embed.patch_size[<span class="number">0</span>]</span><br><span class="line">    h = w = <span class="built_in">int</span>(x.shape[<span class="number">1</span>]**<span class="number">.5</span>)</span><br><span class="line">    <span class="keyword">assert</span> h * w == x.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    x = x.reshape(shape=(x.shape[<span class="number">0</span>], h, w, p, p, <span class="number">3</span>))</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)</span><br><span class="line">    imgs = x.reshape(shape=(x.shape[<span class="number">0</span>], <span class="number">3</span>, h * p, h * p))</span><br><span class="line">    <span class="keyword">return</span> imgs</span><br></pre></td></tr></table></figure><h3 id="掩码构建">1.5 掩码构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Perform per-sample random masking by per-sample shuffling.</span></span><br><span class="line"><span class="string">    Per-sample shuffling is done by argsort random noise.</span></span><br><span class="line"><span class="string">    x: [N, L, D], sequence</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, L, D = x.shape  <span class="comment"># batch, length, dim</span></span><br><span class="line">    len_keep = <span class="built_in">int</span>(L * (<span class="number">1</span> - mask_ratio))</span><br><span class="line">    </span><br><span class="line">    noise = torch.rand(N, L, device=x.device)  <span class="comment"># noise in [0, 1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sort noise for each sample</span></span><br><span class="line">    ids_shuffle = torch.argsort(noise, dim=<span class="number">1</span>)  <span class="comment"># ascend: small is keep, large is remove</span></span><br><span class="line">    ids_restore = torch.argsort(ids_shuffle, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep the first subset</span></span><br><span class="line">    ids_keep = ids_shuffle[:, :len_keep]</span><br><span class="line">    x_masked = torch.gather(x, dim=<span class="number">1</span>, index=ids_keep.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, D))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate the binary mask: 0 is keep, 1 is remove</span></span><br><span class="line">    mask = torch.ones([N, L], device=x.device)</span><br><span class="line">    mask[:, :len_keep] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># unshuffle to get the binary mask</span></span><br><span class="line">    mask = torch.gather(mask, dim=<span class="number">1</span>, index=ids_restore)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_masked, mask, ids_restore</span><br></pre></td></tr></table></figure><h3 id="encoder-forward">1.6 encoder forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_encoder</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="comment"># embed patches</span></span><br><span class="line">    x = self.patch_embed(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed w/o cls token</span></span><br><span class="line">    x = x + self.pos_embed[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># masking: length -&gt; length * mask_ratio</span></span><br><span class="line">    x, mask, ids_restore = self.random_masking(x, mask_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># append cls token</span></span><br><span class="line">    cls_token = self.cls_token + self.pos_embed[:, :<span class="number">1</span>, :]  <span class="comment"># 第0个位置</span></span><br><span class="line">    cls_tokens = cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, mask, ids_restore</span><br></pre></td></tr></table></figure><h3 id="decoder-forward">1.7 decoder forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_decoder</span>(<span class="params">self, x, ids_restore</span>):</span><br><span class="line">    <span class="comment"># embed tokens</span></span><br><span class="line">    x = self.decoder_embed(x) <span class="comment"># 降维</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># append mask tokens to sequence</span></span><br><span class="line">    mask_tokens = self.mask_token.repeat(x.shape[<span class="number">0</span>], ids_restore.shape[<span class="number">1</span>] + <span class="number">1</span> - x.shape[<span class="number">1</span>], <span class="number">1</span>) <span class="comment"># repeat in batchsize</span></span><br><span class="line">    x_ = torch.cat([x[:, <span class="number">1</span>:, :], mask_tokens], dim=<span class="number">1</span>)  <span class="comment"># no cls token</span></span><br><span class="line">    x_ = torch.gather(x_, dim=<span class="number">1</span>, index=ids_restore.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, x.shape[<span class="number">2</span>]))  <span class="comment"># unshuffle</span></span><br><span class="line">    x = torch.cat([x[:, :<span class="number">1</span>, :], x_], dim=<span class="number">1</span>)  <span class="comment"># append cls token</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed</span></span><br><span class="line">    x = x + self.decoder_pos_embed</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.decoder_blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.decoder_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predictor projection</span></span><br><span class="line">    x = self.decoder_pred(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove cls token</span></span><br><span class="line">    x = x[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="loss-forward">1.8 loss forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_loss</span>(<span class="params">self, imgs, pred, mask</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: [N, 3, H, W]</span></span><br><span class="line"><span class="string">    pred: [N, L, p*p*3]</span></span><br><span class="line"><span class="string">    mask: [N, L], 0 is keep, 1 is remove, </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target = self.patchify(imgs)  <span class="comment"># N, L, patch_size**2 *3</span></span><br><span class="line">    <span class="keyword">if</span> self.norm_pix_loss:</span><br><span class="line">        mean = target.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = target.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        target = (target - mean) / (var + <span class="number">1.e-6</span>)**<span class="number">.5</span> <span class="comment"># 防止方差为0</span></span><br><span class="line"></span><br><span class="line">    loss = (pred - target) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean(dim=-<span class="number">1</span>)  <span class="comment"># [N, L], mean loss per patch</span></span><br><span class="line"></span><br><span class="line">    loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()  <span class="comment"># mean loss on removed patches</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="different-models">1.9 different models</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_base_patch16_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">16</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>, num_heads=<span class="number">12</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_large_patch16_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">16</span>, embed_dim=<span class="number">1024</span>, depth=<span class="number">24</span>, num_heads=<span class="number">16</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae_vit_huge_patch14_dec512d8b</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    model = MaskedAutoencoderViT(</span><br><span class="line">        patch_size=<span class="number">14</span>, embed_dim=<span class="number">1280</span>, depth=<span class="number">32</span>, num_heads=<span class="number">16</span>,</span><br><span class="line">        decoder_embed_dim=<span class="number">512</span>, decoder_depth=<span class="number">8</span>, decoder_num_heads=<span class="number">16</span>,</span><br><span class="line">        mlp_ratio=<span class="number">4</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks</span><br><span class="line">mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks</span><br><span class="line">mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks</span><br></pre></td></tr></table></figure><h2 id="main_pretrain">2 main_pretrain</h2><figure><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230420154128379.png" alt="image-20230420154128379" /><figcaption aria-hidden="true">image-20230420154128379</figcaption></figure><h3 id="设置参数的入口">2.1 设置参数的入口</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    args = get_args_parser()</span><br><span class="line">    args = args.parse_args()</span><br><span class="line">    <span class="keyword">if</span> args.output_dir:</span><br><span class="line">        Path(args.output_dir).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="main">2.2 main</h3><p>略。这个函数能够实现单机单卡，多机多卡，单机多卡，cpu等模式，通用性很强</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--accum_iter&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Accumulate gradient iterations (for increasing the effective batch size under memory constraints)&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>此内容可以用在低GPU内存，但是想要训练大batchsize的网络上(时间换空间)</li></ul><h3 id="misc.init_distributed_mode">2.3 misc.init_distributed_mode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_distributed_mode</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> args.dist_on_itp:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_RANK&#x27;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_SIZE&#x27;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;OMPI_COMM_WORLD_LOCAL_RANK&#x27;</span>])</span><br><span class="line">        args.dist_url = <span class="string">&quot;tcp://%s:%s&quot;</span> % (os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>], os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>])</span><br><span class="line">        os.environ[<span class="string">&#x27;LOCAL_RANK&#x27;</span>] = <span class="built_in">str</span>(args.gpu)</span><br><span class="line">        os.environ[<span class="string">&#x27;RANK&#x27;</span>] = <span class="built_in">str</span>(args.rank)</span><br><span class="line">        os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>] = <span class="built_in">str</span>(args.world_size)</span><br><span class="line">        <span class="comment"># [&quot;RANK&quot;, &quot;WORLD_SIZE&quot;, &quot;MASTER_ADDR&quot;, &quot;MASTER_PORT&quot;, &quot;LOCAL_RANK&quot;]</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;RANK&#x27;</span> <span class="keyword">in</span> os.environ <span class="keyword">and</span> <span class="string">&#x27;WORLD_SIZE&#x27;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;LOCAL_RANK&#x27;</span>])</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;SLURM_PROCID&#x27;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&#x27;SLURM_PROCID&#x27;</span>])</span><br><span class="line">        args.gpu = args.rank % torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Not using distributed mode&#x27;</span>)</span><br><span class="line">        setup_for_distributed(is_master=<span class="literal">True</span>)  <span class="comment"># hack</span></span><br><span class="line">        args.distributed = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    args.distributed = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    args.dist_backend = <span class="string">&#x27;nccl&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;| distributed init (rank &#123;&#125;): &#123;&#125;, gpu &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        args.rank, args.dist_url, args.gpu), flush=<span class="literal">True</span>)</span><br><span class="line">    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,</span><br><span class="line">                                         world_size=args.world_size, rank=args.rank)</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    setup_for_distributed(args.rank == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="prepare">2.4 prepare</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">misc.init_distributed_mode(args)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;job dir: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(os.path.dirname(os.path.realpath(__file__))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(args).replace(<span class="string">&#x27;, &#x27;</span>, <span class="string">&#x27;,\n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(args.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fix the seed for reproducibility</span></span><br><span class="line">seed = args.seed + misc.get_rank()</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="simple-augmentation">2.5 simple augmentation</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201555531.png" alt="image-20230420155514559" /><figcaption aria-hidden="true">image-20230420155514559</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># simple augmentation</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(args.input_size, scale=(<span class="number">0.2</span>, <span class="number">1.0</span>), interpolation=<span class="number">3</span>),  <span class="comment"># 3 is bicubic</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(), <span class="comment"># unint8-&gt;float</span></span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line">dataset_train = datasets.ImageFolder(os.path.join(args.data_path, <span class="string">&#x27;train&#x27;</span>), transform=transform_train)</span><br><span class="line"><span class="built_in">print</span>(dataset_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_train, sampler=sampler_train,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="define-model">2.6 define model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">model_without_ddp = model</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model = %s&quot;</span> % <span class="built_in">str</span>(model_without_ddp))</span><br><span class="line"></span><br><span class="line">eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.lr <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># only base_lr is specified</span></span><br><span class="line">    args.lr = args.blr * eff_batch_size / <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;base lr: %.2e&quot;</span> % (args.lr * <span class="number">256</span> / eff_batch_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;actual lr: %.2e&quot;</span> % args.lr)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accumulate grad iterations: %d&quot;</span> % args.accum_iter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;effective batch size: %d&quot;</span> % eff_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=<span class="literal">True</span>)</span><br><span class="line">    model_without_ddp = model.module</span><br><span class="line">    </span><br><span class="line">misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)</span><br></pre></td></tr></table></figure><h3 id="optimizer">2.7 optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># following timm: set wd as 0 for bias and norm layers</span></span><br><span class="line">param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)</span><br><span class="line">optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(<span class="number">0.9</span>, <span class="number">0.95</span>))</span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br><span class="line">loss_scaler = NativeScaler()</span><br></pre></td></tr></table></figure><h3 id="train">2.8 train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Start training for <span class="subst">&#123;args.epochs&#125;</span> epochs&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        data_loader_train.sampler.set_epoch(epoch)</span><br><span class="line">    train_stats = train_one_epoch(</span><br><span class="line">        model, data_loader_train,</span><br><span class="line">        optimizer, device, epoch, loss_scaler,</span><br><span class="line">        log_writer=log_writer,</span><br><span class="line">        args=args</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> (epoch % <span class="number">20</span> == <span class="number">0</span> <span class="keyword">or</span> epoch + <span class="number">1</span> == args.epochs):</span><br><span class="line">        misc.save_model(</span><br><span class="line">            args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,</span><br><span class="line">            loss_scaler=loss_scaler, epoch=epoch)</span><br><span class="line"></span><br><span class="line">    log_stats = &#123;**&#123;<span class="string">f&#x27;train_<span class="subst">&#123;k&#125;</span>&#x27;</span>: v <span class="keyword">for</span> k, v <span class="keyword">in</span> train_stats.items()&#125;,</span><br><span class="line">                    <span class="string">&#x27;epoch&#x27;</span>: epoch,&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">and</span> misc.is_main_process():</span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            log_writer.flush()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(args.output_dir, <span class="string">&quot;log.txt&quot;</span>), mode=<span class="string">&quot;a&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(json.dumps(log_stats) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">total_time = time.time() - start_time</span><br><span class="line">total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training time &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_time_str))</span><br></pre></td></tr></table></figure><p>其中核心代码，单个epoch的训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model: torch.nn.Module,</span></span><br><span class="line"><span class="params">                    data_loader: Iterable, optimizer: torch.optim.Optimizer,</span></span><br><span class="line"><span class="params">                    device: torch.device, epoch: <span class="built_in">int</span>, loss_scaler,</span></span><br><span class="line"><span class="params">                    log_writer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    args=<span class="literal">None</span></span>):</span><br><span class="line">    model.train(<span class="literal">True</span>)</span><br><span class="line">    metric_logger = misc.MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    metric_logger.add_meter(<span class="string">&#x27;lr&#x27;</span>, misc.SmoothedValue(window_size=<span class="number">1</span>, fmt=<span class="string">&#x27;&#123;value:.6f&#125;&#x27;</span>))</span><br><span class="line">    header = <span class="string">&#x27;Epoch: [&#123;&#125;]&#x27;</span>.<span class="built_in">format</span>(epoch)</span><br><span class="line">    print_freq = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    accum_iter = args.accum_iter</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;log_dir: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(log_writer.log_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data_iter_step, (samples, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(metric_logger.log_every(data_loader, print_freq, header)):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># we use a per iteration (instead of per epoch) lr scheduler</span></span><br><span class="line">        <span class="keyword">if</span> data_iter_step % accum_iter == <span class="number">0</span>:</span><br><span class="line">            lr_sched.adjust_learning_rate(optimizer, data_iter_step / <span class="built_in">len</span>(data_loader) + epoch, args)</span><br><span class="line"></span><br><span class="line">        samples = samples.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.amp.autocast(): <span class="comment"># 自动混合精度</span></span><br><span class="line">            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)</span><br><span class="line"></span><br><span class="line">        loss_value = loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> math.isfinite(loss_value):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Loss is &#123;&#125;, stopping training&quot;</span>.<span class="built_in">format</span>(loss_value))</span><br><span class="line">            sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        loss /= accum_iter</span><br><span class="line">        loss_scaler(loss, optimizer, parameters=model.parameters(),</span><br><span class="line">                    update_grad=(data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        torch.cuda.synchronize()</span><br><span class="line"></span><br><span class="line">        metric_logger.update(loss=loss_value)</span><br><span class="line"></span><br><span class="line">        lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">        metric_logger.update(lr=lr)</span><br><span class="line"></span><br><span class="line">        loss_value_reduce = misc.all_reduce_mean(loss_value)</span><br><span class="line">        <span class="keyword">if</span> log_writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (data_iter_step + <span class="number">1</span>) % accum_iter == <span class="number">0</span>:</span><br><span class="line">            <span class="string">&quot;&quot;&quot; We use epoch_1000x as the x-axis in tensorboard.</span></span><br><span class="line"><span class="string">            This calibrates different curves when batch size changes.</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            epoch_1000x = <span class="built_in">int</span>((data_iter_step / <span class="built_in">len</span>(data_loader) + epoch) * <span class="number">1000</span>)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, loss_value_reduce, epoch_1000x)</span><br><span class="line">            log_writer.add_scalar(<span class="string">&#x27;lr&#x27;</span>, lr, epoch_1000x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Averaged stats:&quot;</span>, metric_logger)</span><br><span class="line">    <span class="keyword">return</span> &#123;k: meter.global_avg <span class="keyword">for</span> k, meter <span class="keyword">in</span> metric_logger.meters.items()&#125;</span><br></pre></td></tr></table></figure><ul><li><p>如果因为timm报错，需要把 qk_scale=None注释掉</p></li><li><p>如果需要多卡训练 python -m torch.distributed.launch --nproc_per_node=2 main_prerain.py --batchsize=32 --world_size=2 --data_path="..."</p></li><li><p>dataset的目录结构</p></li></ul><p>/path/to/imagenet-1k/： train/ class1/ img1.jpeg class2/ img2.jpeg val/ class1/ img3.jpeg class2/ img4.jpeg</p><h2 id="main_fintune">3 main_fintune</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201725532.png" alt="image-20230420172505860" /><figcaption aria-hidden="true">image-20230420172505860</figcaption></figure><p>大体和main_pretrain相同，这个是对编码器进行微调，微调的目的是为了更好的完成下游任务</p><h3 id="datasetnew">3.1 dataset(new)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">is_train, args</span>):</span><br><span class="line">    transform = build_transform(is_train, args)</span><br><span class="line"></span><br><span class="line">    root = os.path.join(args.data_path, <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">    dataset = datasets.ImageFolder(root, transform=transform)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_transform</span>(<span class="params">is_train, args</span>):</span><br><span class="line">    mean = IMAGENET_DEFAULT_MEAN</span><br><span class="line">    std = IMAGENET_DEFAULT_STD</span><br><span class="line">    <span class="comment"># train transform</span></span><br><span class="line">    <span class="comment"># 强增广</span></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        <span class="comment"># this should always dispatch to transforms_imagenet_train</span></span><br><span class="line">        transform = create_transform(</span><br><span class="line">            input_size=args.input_size,</span><br><span class="line">            is_training=<span class="literal">True</span>,</span><br><span class="line">            color_jitter=args.color_jitter,</span><br><span class="line">            auto_augment=args.aa,</span><br><span class="line">            interpolation=<span class="string">&#x27;bicubic&#x27;</span>,</span><br><span class="line">            re_prob=args.reprob,</span><br><span class="line">            re_mode=args.remode,</span><br><span class="line">            re_count=args.recount,</span><br><span class="line">            mean=mean,</span><br><span class="line">            std=std,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> transform</span><br><span class="line"></span><br><span class="line">    <span class="comment"># eval transform</span></span><br><span class="line">    <span class="comment"># 基本没有增广</span></span><br><span class="line">    t = []</span><br><span class="line">    <span class="keyword">if</span> args.input_size &lt;= <span class="number">224</span>:</span><br><span class="line">        crop_pct = <span class="number">224</span> / <span class="number">256</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        crop_pct = <span class="number">1.0</span></span><br><span class="line">    size = <span class="built_in">int</span>(args.input_size / crop_pct)</span><br><span class="line">    t.append(</span><br><span class="line">        transforms.Resize(size, interpolation=PIL.Image.BICUBIC),  <span class="comment"># to maintain same ratio w.r.t. 224 images</span></span><br><span class="line">    )</span><br><span class="line">    t.append(transforms.CenterCrop(args.input_size))</span><br><span class="line"></span><br><span class="line">    t.append(transforms.ToTensor())</span><br><span class="line">    t.append(transforms.Normalize(mean, std))</span><br><span class="line">    <span class="keyword">return</span> transforms.Compose(t)</span><br></pre></td></tr></table></figure><h3 id="dataloader">3.2 dataloader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_train, sampler=sampler_train,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_val, sampler=sampler_val,</span><br><span class="line">    batch_size=args.batch_size,</span><br><span class="line">    num_workers=args.num_workers,</span><br><span class="line">    pin_memory=args.pin_mem,</span><br><span class="line">    drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="mixup增广">3.3 mixup增广</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mixup_fn = <span class="literal">None</span></span><br><span class="line">mixup_active = args.mixup &gt; <span class="number">0</span> <span class="keyword">or</span> args.cutmix &gt; <span class="number">0.</span> <span class="keyword">or</span> args.cutmix_minmax <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> mixup_active:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Mixup is activated!&quot;</span>)</span><br><span class="line">    mixup_fn = Mixup(</span><br><span class="line">        mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,</span><br><span class="line">        prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,</span><br><span class="line">        label_smoothing=args.smoothing, num_classes=args.nb_classes)</span><br></pre></td></tr></table></figure><h3 id="model定义">3.4 model定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = models_vit.__dict__[args.model](</span><br><span class="line">    num_classes=args.nb_classes,</span><br><span class="line">    drop_path_rate=args.drop_path,</span><br><span class="line">    global_pool=args.global_pool,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(timm.models.vision_transformer.VisionTransformer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer with support for global average pooling</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, global_pool=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self.global_pool = global_pool</span><br><span class="line">        <span class="keyword">if</span> self.global_pool:</span><br><span class="line">            norm_layer = kwargs[<span class="string">&#x27;norm_layer&#x27;</span>]</span><br><span class="line">            embed_dim = kwargs[<span class="string">&#x27;embed_dim&#x27;</span>]</span><br><span class="line">            self.fc_norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">del</span> self.norm  <span class="comment"># remove the original norm</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line"></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = blk(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.global_pool:</span><br><span class="line">            x = x[:, <span class="number">1</span>:, :].mean(dim=<span class="number">1</span>)  <span class="comment"># global pool without cls token</span></span><br><span class="line">            outcome = self.fc_norm(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">            outcome = x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outcome</span><br></pre></td></tr></table></figure><h3 id="预训练权重的导入">3.5 预训练权重的导入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.finetune <span class="keyword">and</span> <span class="keyword">not</span> args.<span class="built_in">eval</span>:</span><br><span class="line">    checkpoint = torch.load(args.finetune, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Load pre-trained checkpoint from: %s&quot;</span> % args.finetune)</span><br><span class="line">    checkpoint_model = checkpoint[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    state_dict = model.state_dict()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> checkpoint_model <span class="keyword">and</span> checkpoint_model[k].shape != state_dict[k].shape:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Removing key <span class="subst">&#123;k&#125;</span> from pretrained checkpoint&quot;</span>)</span><br><span class="line">            <span class="keyword">del</span> checkpoint_model[k]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># interpolate position embedding</span></span><br><span class="line">    interpolate_pos_embed(model, checkpoint_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load pre-trained model</span></span><br><span class="line">    msg = model.load_state_dict(checkpoint_model, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(msg)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.global_pool:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(msg.missing_keys) == &#123;<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>, <span class="string">&#x27;fc_norm.weight&#x27;</span>, <span class="string">&#x27;fc_norm.bias&#x27;</span>&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(msg.missing_keys) == &#123;<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># manually initialize fc layer</span></span><br><span class="line">    trunc_normal_(model.head.weight, std=<span class="number">2e-5</span>)</span><br></pre></td></tr></table></figure><ul><li>确保head部分的权重没有导入</li><li>线性插值：让微调阶段的position embedding仍然适用(如果模型变大)</li></ul><h2 id="linear-probing">4 linear probing</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304201725555.png" alt="image-20230420172530538" /><figcaption aria-hidden="true">image-20230420172530538</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for linear prob only</span></span><br><span class="line"><span class="comment"># hack: revise model&#x27;s head with BN</span></span><br><span class="line">model.head = torch.nn.Sequential(torch.nn.BatchNorm1d(model.head.in_features, affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>), model.head)</span><br><span class="line"><span class="comment"># freeze all but the head</span></span><br><span class="line"><span class="keyword">for</span> _, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> _, p <span class="keyword">in</span> model.head.named_parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;masked-autoencoders-are-scalable-vision-learners&quot;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.o</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门19-ResNet</title>
    <link href="https://wangtongyouwen.github.io/post/1c753934.html"/>
    <id>https://wangtongyouwen.github.io/post/1c753934.html</id>
    <published>2023-04-19T11:24:03.000Z</published>
    <updated>2023-04-19T15:02:39.751Z</updated>
    
    <content type="html"><![CDATA[<p>https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</p><h1 id="使用timm复现resnet">使用timm复现ResNet</h1><p>论文内容略，本文采用的python+pytorch 环境为:</p><p>torch 1.13.1+cu117</p><p>python 3.7</p><p>timm 0.4.12</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timm</span><br><span class="line">timm.create_model(<span class="string">&quot;resnet50&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br></pre></td><td class="code"><pre><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (act1): ReLU(inplace=True)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (4): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (5): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act1): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act2): ReLU(inplace=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (act3): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)</span><br><span class="line">  (fc): Linear(in_features=2048, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>lib/site-packages/timm/models/resnet.py</p><h2 id="resnet">1 ResNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stem</span></span><br><span class="line">self.conv1 = nn.Conv2d(in_chans, inplanes, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = norm_layer(inplanes)</span><br><span class="line">self.act1 = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">self.feature_info = [<span class="built_in">dict</span>(num_chs=inplanes, reduction=<span class="number">2</span>, module=<span class="string">&#x27;act1&#x27;</span>)]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Stem Pooling</span></span><br><span class="line"><span class="keyword">if</span> aa_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">self.maxpool = nn.Sequential(*[</span><br><span class="line">nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">aa_layer(channels=inplanes, stride=<span class="number">2</span>)])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>核心部分：feature map</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feature Blocks</span></span><br><span class="line">channels = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">stage_modules, stage_feature_info = make_blocks(</span><br><span class="line">block, channels, layers, inplanes, cardinality=cardinality, base_width=base_width,</span><br><span class="line">output_stride=output_stride, reduce_first=block_reduce_first, avg_down=avg_down,</span><br><span class="line">down_kernel_size=down_kernel_size, act_layer=act_layer, norm_layer=norm_layer, aa_layer=aa_layer,</span><br><span class="line">drop_block_rate=drop_block_rate, drop_path_rate=drop_path_rate, **block_args)</span><br><span class="line"><span class="keyword">for</span> stage <span class="keyword">in</span> stage_modules:</span><br><span class="line">self.add_module(*stage)  <span class="comment"># layer1, layer2, etc</span></span><br><span class="line">self.feature_info.extend(stage_feature_info)</span><br></pre></td></tr></table></figure><p>其中make_blocks是构建每个stage的具体实现，这里具体内容在第二部分查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.num_features = <span class="number">512</span> * block.expansion</span><br><span class="line">self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)</span><br></pre></td></tr></table></figure><p>为什么这里有block.expansion?</p><p>在ResNet50中有些stage之间的通道数是不相同的，如果直接使用残差连接，无法完成直接相加，需要对某些深度通道进行压缩，expansion能够保证某些通道的输出维度是匹配的</p><h2 id="make_blocks">2 make_blocks</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_blocks</span>(<span class="params"></span></span><br><span class="line"><span class="params">        block_fn, channels, block_repeats, inplanes, reduce_first=<span class="number">1</span>, output_stride=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">        down_kernel_size=<span class="number">1</span>, avg_down=<span class="literal">False</span>, drop_block_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.</span>, **kwargs</span>):</span><br><span class="line">    stages = []</span><br><span class="line">    feature_info = []</span><br><span class="line">    net_num_blocks = <span class="built_in">sum</span>(block_repeats)</span><br><span class="line">    net_block_idx = <span class="number">0</span></span><br><span class="line">    net_stride = <span class="number">4</span></span><br><span class="line">    dilation = prev_dilation = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> stage_idx, (planes, num_blocks, db) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(channels, block_repeats, drop_blocks(drop_block_rate))):</span><br><span class="line">        stage_name = <span class="string">f&#x27;layer<span class="subst">&#123;stage_idx + <span class="number">1</span>&#125;</span>&#x27;</span>  <span class="comment"># never liked this name, but weight compat requires it</span></span><br><span class="line">        stride = <span class="number">1</span> <span class="keyword">if</span> stage_idx == <span class="number">0</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> net_stride &gt;= output_stride:</span><br><span class="line">            dilation *= stride</span><br><span class="line">            stride = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            net_stride *= stride</span><br><span class="line"></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inplanes != planes * block_fn.expansion:</span><br><span class="line">            down_kwargs = <span class="built_in">dict</span>(</span><br><span class="line">                in_channels=inplanes, out_channels=planes * block_fn.expansion, kernel_size=down_kernel_size,</span><br><span class="line">                stride=stride, dilation=dilation, first_dilation=prev_dilation, norm_layer=kwargs.get(<span class="string">&#x27;norm_layer&#x27;</span>))</span><br><span class="line">            downsample = downsample_avg(**down_kwargs) <span class="keyword">if</span> avg_down <span class="keyword">else</span> downsample_conv(**down_kwargs)</span><br><span class="line"></span><br><span class="line">        block_kwargs = <span class="built_in">dict</span>(reduce_first=reduce_first, dilation=dilation, drop_block=db, **kwargs)</span><br><span class="line">        blocks = []</span><br><span class="line">        <span class="keyword">for</span> block_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks):</span><br><span class="line">            downsample = downsample <span class="keyword">if</span> block_idx == <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            stride = stride <span class="keyword">if</span> block_idx == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">            block_dpr = drop_path_rate * net_block_idx / (net_num_blocks - <span class="number">1</span>)  <span class="comment"># stochastic depth linear decay rule</span></span><br><span class="line">            blocks.append(block_fn(</span><br><span class="line">                inplanes, planes, stride, downsample, first_dilation=prev_dilation,</span><br><span class="line">                drop_path=DropPath(block_dpr) <span class="keyword">if</span> block_dpr &gt; <span class="number">0.</span> <span class="keyword">else</span> <span class="literal">None</span>, **block_kwargs))</span><br><span class="line">            prev_dilation = dilation</span><br><span class="line">            inplanes = planes * block_fn.expansion</span><br><span class="line">            net_block_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        stages.append((stage_name, nn.Sequential(*blocks)))</span><br><span class="line">        feature_info.append(<span class="built_in">dict</span>(num_chs=inplanes, reduction=net_stride, module=stage_name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stages, feature_info</span><br></pre></td></tr></table></figure><p>这段代码定义了一个名为<code>make_blocks</code>的函数，用于构建一系列残差块（residual blocks）并形成卷积神经网络（CNN）的多个阶段。这种结构在 ResNet 及其变体中很常见。<code>make_blocks</code>函数接收多个参数，用于控制网络的结构和行为。函数的主要目的是根据给定的参数创建网络的各个阶段并返回这些阶段及其相关的特征信息。</p><p>以下是函数参数的简要说明：</p><ul><li><code>block_fn</code>：残差块的函数，如 ResNet 中的基本块或瓶颈块。</li><li><code>channels</code>：每个阶段的输出通道数量。</li><li><code>block_repeats</code>：每个阶段的残差块重复次数。</li><li><code>inplanes</code>：输入通道数量。</li><li>其他参数用于控制步长（<code>output_stride</code>）、下采样方式（<code>avg_down</code>）、DropBlock（<code>drop_block_rate</code>）和 Stochastic Depth（<code>drop_path_rate</code>）等高级功能。</li></ul><p>代码的主要部分是一个循环，针对每个阶段执行以下操作：</p><ol type="1"><li>为每个阶段创建一个名为<code>stage_name</code>的变量，如<code>layer1</code>、<code>layer2</code>等。</li><li>计算步长（<code>stride</code>）和扩张率（<code>dilation</code>），这些值控制每个阶段的空间分辨率。</li><li>如果需要，创建下采样层（<code>downsample</code>），以使输入与输出具有兼容的尺寸。</li><li>根据给定的参数，创建一系列残差块。</li><li>将这些残差块组合成一个名为<code>nn.Sequential</code>的模块，将其添加到<code>stages</code>列表中。</li><li>将每个阶段的特征信息添加到<code>feature_info</code>列表中。</li></ol><p>最后，函数返回<code>stages</code>和<code>feature_info</code>，它们分别表示网络的各个阶段及其相关的特征信息。这些信息通常在之后的代码中用于构建完整的神经网络，并在训练过程中用于损失计算和评估等任务。</p><ul><li>downsample和block.expansion之间的联系：</li></ul><p><code>downsample</code> 和 <code>block.expansion</code> 在这段代码中具有不同的目的，但它们之间存在联系。首先，让我们解释它们各自的作用：</p><ol type="1"><li><code>downsample</code>：<code>downsample</code> 是一个可选的下采样层，用于在输入和输出之间调整维度。在构建残差网络时，<code>downsample</code> 的目的是使跳过连接（skip connection）中的输入与残差块输出具有相同的尺寸，以便将它们相加。<code>downsample</code> 层通常包含一个卷积层和一个批量归一化层，用于调整输入的通道数量和/或分辨率。这样，即使输入和输出具有不同的尺寸，也可以将它们相加。</li><li><code>block.expansion</code>：<code>block.expansion</code> 是残差块中的扩张因子，用于控制输入和输出通道的增长。在 ResNet 架构中，基本块的扩张因子为 1（即输入和输出通道数量相同），而瓶颈块的扩张因子为 4（即输出通道数量为输入通道数量的 4 倍）。</li></ol><p>这两者之间的联系是：<code>downsample</code> 层用于调整输入的通道数量，使其与残差块输出的通道数量匹配。而<code>block.expansion</code> 则决定了残差块输出的通道数量。因此，在计算下采样层时，通常会将输出通道数量设置为 <code>planes * block_fn.expansion</code>，其中 <code>planes</code> 是当前阶段的输出通道数量，<code>block_fn.expansion</code> 是残差块的扩张因子。</p><p>简而言之，<code>downsample</code> 和 <code>block.expansion</code> 分别负责调整输入尺寸以匹配输出尺寸，以及控制残差块中输入和输出通道的增长。尽管它们具有不同的目的，但它们之间存在联系，共同影响网络的整体结构。</p><h2 id="basicblock">3 BasicBlock</h2><p>这个类就是对ResNet进行的一个实现，这个网络中的block大小都是相同的，所以没有用到上述的downsample和block.expansion</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, cardinality=<span class="number">1</span>, base_width=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                 reduce_first=<span class="number">1</span>, dilation=<span class="number">1</span>, first_dilation=<span class="literal">None</span>, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,</span></span><br><span class="line"><span class="params">                 attn_layer=<span class="literal">None</span>, aa_layer=<span class="literal">None</span>, drop_block=<span class="literal">None</span>, drop_path=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> cardinality == <span class="number">1</span>, <span class="string">&#x27;BasicBlock only supports cardinality of 1&#x27;</span></span><br><span class="line">        <span class="keyword">assert</span> base_width == <span class="number">64</span>, <span class="string">&#x27;BasicBlock does not support changing base width&#x27;</span></span><br><span class="line">        first_planes = planes // reduce_first</span><br><span class="line">        outplanes = planes * self.expansion</span><br><span class="line">        first_dilation = first_dilation <span class="keyword">or</span> dilation</span><br><span class="line">        use_aa = aa_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (stride == <span class="number">2</span> <span class="keyword">or</span> first_dilation != dilation)</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(</span><br><span class="line">            inplanes, first_planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span> <span class="keyword">if</span> use_aa <span class="keyword">else</span> stride, padding=first_dilation,</span><br><span class="line">            dilation=first_dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = norm_layer(first_planes)</span><br><span class="line">        self.act1 = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.aa = aa_layer(channels=first_planes, stride=stride) <span class="keyword">if</span> use_aa <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(</span><br><span class="line">            first_planes, outplanes, kernel_size=<span class="number">3</span>, padding=dilation, dilation=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = norm_layer(outplanes)</span><br><span class="line"></span><br><span class="line">        self.se = create_attn(attn_layer, outplanes)</span><br><span class="line"></span><br><span class="line">        self.act2 = act_layer(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.dilation = dilation</span><br><span class="line">        self.drop_block = drop_block</span><br><span class="line">        self.drop_path = drop_path</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_init_last_bn</span>(<span class="params">self</span>):</span><br><span class="line">        nn.init.zeros_(self.bn2.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        shortcut = x</span><br><span class="line"></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_block <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.drop_block(x)</span><br><span class="line">        x = self.act1(x)</span><br><span class="line">        <span class="keyword">if</span> self.aa <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.aa(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_block <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.drop_block(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.se <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.se(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.drop_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.drop_path(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            shortcut = self.downsample(shortcut)</span><br><span class="line">        x += shortcut</span><br><span class="line">        x = self.act2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&lt;/p&gt;
&lt;h1 id=&quot;使用timm复现resnet&quot;&gt;使用timm复</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门18-ConvNext</title>
    <link href="https://wangtongyouwen.github.io/post/aa153511.html"/>
    <id>https://wangtongyouwen.github.io/post/aa153511.html</id>
    <published>2023-04-18T10:32:10.000Z</published>
    <updated>2023-04-19T15:02:39.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-convnet-for-the-2020s">A ConvNet for the 2020s</h1><p>https://arxiv.org/pdf/2201.03545.pdf</p><p>A vanilla <strong>ViT</strong>, on the other hand, faces <strong>difficulties</strong> when applied to general computer vision tasks such as object detection and semantic segmentation</p><p>It is the <strong>hierarchical</strong> Transformers (e.g., <strong>Swin Transformers</strong>) that reintroduced several <strong>ConvNet</strong> priors, making Transformers practically viable as a generic vision backbone and demonstrating <strong>remarkable performance on a wide variety of vision tasks</strong>.</p><h2 id="理解什么是resnet-50">1 理解什么是ResNet-50</h2><ul><li>由48层卷积+1层maxpool+1层avgpool构成，卷积每个block的配比为3:4:6:3</li><li>ResNet50 Architecture</li></ul><figure><img src="https://iq.opengenus.org/content/images/2020/03/Screenshot-from-2020-03-20-15-49-54.png" alt="Table 1" /><figcaption aria-hidden="true">Table 1</figcaption></figure><h2 id="convnext主要宗旨">2 ConVNeXt主要宗旨</h2><ul><li>本文主要是希望基于ReSNet-50结构，并参考Swin-T的思考来升级改造ResNet，最终得到ResNet结构，并实现了新的准确率，并进一步探索了它的可扩展性。</li></ul><h2 id="优化器参数">3 优化器参数</h2><ul><li>AdamW，300epochs</li><li>准确率直接从76.1%提升到了78.8%</li><li>预训练学习率为4e-3，weight_decay=0.05,batchsize=4096</li><li>微调学习率为5e-5,weight_decay=1e-8,batchsize=512,layer-wise Ir decay</li></ul><h2 id="宏观设计">4 宏观设计</h2><ul><li>将[3,4,6,3]的区块比例改成了[3,3,9,3]</li><li>将底层的卷积替换成了4*4 stride=4的卷积，类似于patch</li><li>引入depth-wise conv，并将channels从64提升到96</li><li>引入bottleneck结构{channels分别为96 384 96}，并增大 kernel size 到 7*7</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191428468.png" alt="image-20230419142812305" /><figcaption aria-hidden="true">image-20230419142812305</figcaption></figure><ul><li>至此，ImageNet-1k的准确率从78.8%提升到80.6%</li></ul><h2 id="微观设计">5 微观设计</h2><ul><li>将RELU替换成GELU，将BN替换为LN</li><li>引入更少激活函数和归一化层</li><li>采用2*2，stride=2的卷积进行下采样，并在底层、下采样之前和最后的平均池化之后加入LN层，使得训练更加稳定</li><li>至此，ImageNet-1k的准确率进一步提升到82.0%，击败Swin-T中的81.3%</li></ul><h2 id="可扩展性">6 可扩展性</h2><ul><li>ImageNet-1k训练<ul><li>随着参数数目和计算量的增大，准确率也在逐步提升至85.5%</li></ul></li><li>增加ImageNet-22k训练，在迁移至ImageNet-1k微调<ul><li>伴随预训练，同样的模型，效果涨幅约为2%</li><li>最终，ConvNeXt-XL效果达到87.8%</li></ul></li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191537241.png" alt="image-20230419144017393" /><figcaption aria-hidden="true">image-20230419144017393</figcaption></figure><h1 id="code">CODE</h1><p><a href="https://github.com/facebookresearch/ConvNeXt">facebookresearch/ConvNeXt: Code release for ConvNeXt model (github.com)</a></p><h2 id="block">1 block</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; ConvNeXt Block. There are two equivalent implementations:</span></span><br><span class="line"><span class="string">    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)</span></span><br><span class="line"><span class="string">    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back</span></span><br><span class="line"><span class="string">    We use (2) as we find it slightly faster in PyTorch</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        drop_path (float): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, drop_path=<span class="number">0.</span>, layer_scale_init_value=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dwconv = nn.Conv2d(dim, dim, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, groups=dim) <span class="comment"># depthwise conv</span></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.pwconv1 = nn.Linear(dim, <span class="number">4</span> * dim) <span class="comment"># pointwise/1x1 convs, implemented with linear layers</span></span><br><span class="line">        self.act = nn.GELU()</span><br><span class="line">        self.pwconv2 = nn.Linear(<span class="number">4</span> * dim, dim)</span><br><span class="line">        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), </span><br><span class="line">                                    requires_grad=<span class="literal">True</span>) <span class="keyword">if</span> layer_scale_init_value &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="built_in">input</span> = x</span><br><span class="line">        x = self.dwconv(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="comment"># (N, C, H, W) -&gt; (N, H, W, C)</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.pwconv1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.pwconv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.gamma <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.gamma * x</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (N, H, W, C) -&gt; (N, C, H, W)</span></span><br><span class="line"></span><br><span class="line">        x = <span class="built_in">input</span> + self.drop_path(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="connext">2 ConNeXt</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191505813.png" alt="image-20230419150530038" /><figcaption aria-hidden="true">image-20230419150530038</figcaption></figure><ul><li>stem<ul><li>conv2d: 3-&gt;96 kernel_size =4,stride=4</li><li>layernarm: 96</li></ul></li><li>downsampler_layer<ul><li>layernrom: 96, 192, 384</li><li>conv2d: 96, 192, 384 -&gt; 192, 384, 768 kernel_size = 2,stride = 2 (patch merging)</li></ul></li><li>stage(4个阶段)<ul><li>block数量:3,3,9,3</li><li>block input_channel: 96, 192, 384, 768</li><li>cur记录总的深度：每个深度的dropout是不同的，随着深度增大，dropout比例越大</li></ul></li><li>layernorm：768</li><li>head(Linear): 768 -&gt; num_classes</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, </span></span><br><span class="line"><span class="params">             depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>], dims=[<span class="number">96</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>], drop_path_rate=<span class="number">0.</span>, </span></span><br><span class="line"><span class="params">             layer_scale_init_value=<span class="number">1e-6</span>, head_init_scale=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">             </span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    self.downsample_layers = nn.ModuleList() <span class="comment"># stem and 3 intermediate downsampling conv layers</span></span><br><span class="line">    stem = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_chans, dims[<span class="number">0</span>], kernel_size=<span class="number">4</span>, stride=<span class="number">4</span>),</span><br><span class="line">        LayerNorm(dims[<span class="number">0</span>], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    self.downsample_layers.append(stem)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        downsample_layer = nn.Sequential(</span><br><span class="line">                LayerNorm(dims[i], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>),</span><br><span class="line">                nn.Conv2d(dims[i], dims[i+<span class="number">1</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.downsample_layers.append(downsample_layer)</span><br><span class="line"></span><br><span class="line">    self.stages = nn.ModuleList() <span class="comment"># 4 feature resolution stages, each consisting of multiple residual blocks</span></span><br><span class="line">    dp_rates=[x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))] </span><br><span class="line">    cur = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        stage = nn.Sequential(</span><br><span class="line">            *[Block(dim=dims[i], drop_path=dp_rates[cur + j], </span><br><span class="line">            layer_scale_init_value=layer_scale_init_value) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(depths[i])]</span><br><span class="line">        )</span><br><span class="line">        self.stages.append(stage)</span><br><span class="line">        cur += depths[i]</span><br><span class="line"></span><br><span class="line">    self.norm = nn.LayerNorm(dims[-<span class="number">1</span>], eps=<span class="number">1e-6</span>) <span class="comment"># final norm layer</span></span><br><span class="line">    self.head = nn.Linear(dims[-<span class="number">1</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    self.apply(self._init_weights)</span><br><span class="line">    self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">    self.head.bias.data.mul_(head_init_scale)</span><br></pre></td></tr></table></figure><h1 id="isotropic-convnext-各向同性">Isotropic ConvNeXt 各向同性</h1><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304191537246.png" alt="image-20230419153724437" /><figcaption aria-hidden="true">image-20230419153724437</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ef __init__(self, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, </span><br><span class="line">                 depth=<span class="number">18</span>, dim=<span class="number">384</span>, drop_path_rate=<span class="number">0.</span>, </span><br><span class="line">                 layer_scale_init_value=<span class="number">0</span>, head_init_scale=<span class="number">1.</span>,</span><br><span class="line">                 ):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.stem = nn.Conv2d(in_chans, dim, kernel_size=<span class="number">16</span>, stride=<span class="number">16</span>)</span><br><span class="line">        dp_rates=[x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)] </span><br><span class="line">        self.blocks = nn.Sequential(*[Block(dim=dim, drop_path=dp_rates[i], </span><br><span class="line">                                    layer_scale_init_value=layer_scale_init_value)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>) <span class="comment"># final norm layer</span></span><br><span class="line">        self.head = nn.Linear(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line">        self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">        self.head.bias.data.mul_(head_init_scale)</span><br></pre></td></tr></table></figure><ul><li>不需要step，各向同性，通道数目在每个阶段都是相同的</li></ul><h1 id="训练">训练</h1><ul><li>data_path</li><li>data_set {CIFAR,IMNET,image_foler}</li></ul><p>https://timm.fast.ai/这里有大量的cv网络的实现</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;a-convnet-for-the-2020s&quot;&gt;A ConvNet for the 2020s&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/pdf/2201.03545.pdf&lt;/p&gt;
&lt;p&gt;A vanilla &lt;strong&gt;ViT&lt;/strong</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门17-word embedding</title>
    <link href="https://wangtongyouwen.github.io/post/70a386a7.html"/>
    <id>https://wangtongyouwen.github.io/post/70a386a7.html</id>
    <published>2023-04-17T14:27:03.000Z</published>
    <updated>2023-04-18T09:58:52.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言建模">语言建模</h2><ul><li>基于已有的人类组织的文本语料，基于无监督学习如何组织一句话并还能得到单词的语义表征</li><li>统计模型：n-gram</li><li>无监督学习：NNLM</li><li>大规模无监督学习：word2vec,BERT</li></ul><h3 id="n-gram">1 n-gram</h3><ul><li>特点：统计性、简单、泛化能力差、无法得到单词的语义信息</li><li>定义：n个相邻字符构成的序列<ul><li>unigram</li><li>bigram</li><li>trigram</li></ul></li><li>用途：基于n-gram的频数分析文本，如垃圾邮件分类</li><li>对于word n-gram，特征维度随着语料词汇增大和n增大而指数增大(curse of dimensionality 维度灾难)</li><li>对于character n-gram，特征维度只随着n增大而增大</li></ul><h3 id="单词的语义表征">2 单词的语义表征</h3><ul><li>稀疏式<ul><li>one-hot encoding 只能反应出单词在单词表中的位置信息，不能得出任何语义上的信息</li></ul></li><li>分布式<ul><li>类似于word embedding 固定长度，每个位置上都是浮点型，这种语义表征是隐式的，是训练获得的(通过向量点积能得到相似度)</li></ul></li><li>应用场景<ul><li>word/character/phrase/sentence/paragraph embedding</li><li>speaker/user/item embedding</li></ul></li></ul><h3 id="基于神经网络的语言模型nnlm">3 基于神经网络的语言模型(NNLM)</h3><ul><li>NNLM包括：<ul><li>输入层(one-hot)</li><li>投影层</li><li>隐含层</li><li>输出层</li></ul></li><li>word embeddings为副产物，隐含的语义表征</li><li>主要复杂度： N*D*H+H*V</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/dQiaQ6INiazLoDx1sOWTQSTiaoLahdlZvZ9gBdtWVSS6gsqz8PLgHAPUesz0mqVCyo2MwjO6yssqnBOPO5BJ8z1lg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="Image" style="zoom:67%;" /></p><ul><li>如何降低复杂度？如何训练大规模数据？</li></ul><h3 id="word2vec">4 word2vec</h3><h4 id="改进1抛弃了隐含层并提出cbow和skip-gram">改进1：抛弃了隐含层，并提出CBOW和Skip-gram</h4><ul><li>continuous Bag-of-Words<ul><li>不同于NNLM，CBOW考虑到了前后上下文</li><li>使用周围单词预测中间单词</li><li>输入：前n个单词和后n个单词</li><li>目标：基于H-softmax预测中间单词</li></ul></li></ul><p><span class="math display">\[J_{\theta}=\frac{1}{T}\sum^T_{t=1}log P(w_t|w_{t-n},\dots,w_{t-1},w_{t+1},\dots,w_{t+n})\]</span></p><ul><li>Skip-gram<ul><li>与CBOW相反，使用中间单词预测周围单词</li><li>输入：中间单词</li><li>目标：基于H-softmax预测前n个单词和后n个单词</li></ul></li></ul><p><span class="math display">\[J_{\theta} = \frac{1}{T}\sum^T_{t=1} \sum_{-n\le j\le n,n\ne 0}log\ \ p(w_{t+j}|w_t)\]</span></p><figure><img src="https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Screenshot-2021-03-05-at-11.29.31-1024x616-1.png" alt="Screenshot-2021-03-05-at-11.29.31" /><figcaption aria-hidden="true">Screenshot-2021-03-05-at-11.29.31</figcaption></figure><h4 id="改进2优化softmax">改进2：优化softmax</h4><ul><li>softmax<ul><li>计算量跟单词表数目K呈线性关系</li></ul></li></ul><p><span class="math display">\[\sigma(\vec z)_i = \frac{e^{z_i}}{\sum^K_{j=1}e^{z_j}}\]</span></p><ul><li>hierarchical softmax(huffman树)</li></ul><ol type="1"><li>将{w1,w2,...,wn}看成是由n棵树的森林(每棵树仅有一个节点)</li><li>在森林中选出两个树节点的权值最小的树合并，作为一棵新树的左右子树，且新树的根节点权值为其左、右子树节点权值之和</li><li>从森林中删除选取的两棵树，并将新树加入森林</li><li>重复2,3步，知道森林中只剩一棵树为止，该树就为所求的Huffman树</li></ol><p><img src="https://www.ruder.io/content/images/2016/05/hierarchical_softmax.png" alt="img" /> <span class="math display">\[p(right|n,c)=\sigma(h^Tv&#39;_n)\]</span></p><p><span class="math display">\[p(left|n,c) = 1-\sigma(h^Tv&#39;_n)\]</span></p><h4 id="改进3引入负采样">改进3：引入负采样</h4><ul><li>continuous bag ofwords<ul><li>输入：前n个单词和后n个单词</li><li>目标：使得预测中间单词的概率最大，负样本单词的概率最小</li></ul></li></ul><p><span class="math display">\[g(w) = \prod_{u\in {w} \bigcup NEG(w)}p(u|Context(w))\]</span></p><ul><li>Skip-gram<ul><li>入：中间单词</li><li>目标：使得上下文单词概率最大，负样本单词的概率最小</li></ul></li></ul><p><span class="math display">\[g(w) = \prod_{\tilde w\in {Context(w)}} \prod_{u\in {w} \bigcup  NEG^{\tilde w}(w)} p(u|\tilde w)\]</span></p><h2 id="word-embeddings-in-pytorch">word embeddings in pytorch</h2><p>https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;语言建模&quot;&gt;语言建模&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;基于已有的人类组织的文本语料，基于无监督学习如何组织一句话并还能得到单词的语义表征&lt;/li&gt;
&lt;li&gt;统计模型：n-gram&lt;/li&gt;
&lt;li&gt;无监督学习：NNLM&lt;/li&gt;
&lt;li&gt;大规模无监督学习：w</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实用工具2-编写技巧</title>
    <link href="https://wangtongyouwen.github.io/post/898bb30b.html"/>
    <id>https://wangtongyouwen.github.io/post/898bb30b.html</id>
    <published>2023-04-17T11:36:18.000Z</published>
    <updated>2023-04-17T14:20:00.293Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304172006805.png" alt="编写技巧" /><figcaption aria-hidden="true">编写技巧</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange,reduce,repeat</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>) <span class="comment"># 4D tensor bs*ic*h*w</span></span><br></pre></td></tr></table></figure><h2 id="rearrange">1 rearrange</h2><h3 id="转置">1.1 转置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转置</span></span><br><span class="line">out1 = x.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">out2 = rearrange(x,<span class="string">&#x27;b i h w -&gt; b h i w&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="变形">1.2 变形</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变形</span></span><br><span class="line">out1 = rearrange(x,<span class="string">&#x27;b i h w -&gt; (b i) h w&#x27;</span>)</span><br><span class="line">out2 = x.reshape(<span class="number">6</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">out3 = rearrange(out2,<span class="string">&#x27;(b i) h w -&gt; b i h w&#x27;</span>,b=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out3,x))</span><br></pre></td></tr></table></figure><h3 id="image2patch">1.3 image2patch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image2patch</span></span><br><span class="line">bs, ic, h, w = x.shape</span><br><span class="line">p = <span class="number">2</span></span><br><span class="line">out1 = rearrange(x,<span class="string">&#x27;b i (h1 p1) (w1 p2) -&gt; b (h1 w1) (i p1 p2)&#x27;</span>,p1=<span class="number">2</span>,p2=<span class="number">2</span>) <span class="comment"># p是patch的边长 [batchsize,num_patch,patch_depth]</span></span><br><span class="line">out2 = F.unfold(x, kernel_size=(p, p),stride=(p, p)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)  <span class="comment"># [bs,num_patch,patch_depth]</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="堆叠">1.4 堆叠</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 堆叠</span></span><br><span class="line"><span class="built_in">print</span>(bs,i,h,w) <span class="comment"># (2,2,4,4)</span></span><br><span class="line">tensor_list = [x,x,x] <span class="comment"># only in the form of einops</span></span><br><span class="line">out1 = rearrange(tensor_list,<span class="string">&#x27;n b i h w -&gt; n b i h w&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(out1.shape)</span><br><span class="line"></span><br><span class="line">y = torch.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">out2 = y.repeat(bs,i,h,w).reshape(<span class="number">3</span>,bs,i,h,w)</span><br><span class="line"><span class="built_in">print</span>(out2.shape)</span><br></pre></td></tr></table></figure><h2 id="reduce">2 reduce</h2><h3 id="求平均池化">2.1 求平均池化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求平均池化</span></span><br><span class="line">out1 = reduce(x,<span class="string">&#x27;b i h w -&gt; b i h&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># mean, min, max, sum, prod</span></span><br><span class="line">out2 = torch.mean(x,(-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="求和">2.2 求和</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求和</span></span><br><span class="line">out1 = reduce(x,<span class="string">&#x27;b i h w -&gt; b i h 1&#x27;</span>,<span class="string">&#x27;sum&#x27;</span>) <span class="comment"># keep dimension</span></span><br><span class="line">out2 = torch.<span class="built_in">sum</span>(x,(-<span class="number">1</span>)).unsqueeze(-<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h3 id="多个维度操作">2.3 多个维度操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多个维度操作</span></span><br><span class="line">b, i, h, w = x.shape</span><br><span class="line">out1 = reduce(x,<span class="string">&#x27;b i h w -&gt; b i&#x27;</span>,<span class="string">&#x27;max&#x27;</span>)</span><br><span class="line">out2 = torch.max_pool2d(x,(h,w)).reshape(b,i)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(out1,out2))</span><br></pre></td></tr></table></figure><h2 id="repeat">3 repeat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复制</span></span><br><span class="line">out1 = rearrange(x,<span class="string">&#x27;b i h w -&gt; b i h w 1&#x27;</span>)</span><br><span class="line">out2 = repeat(out1,<span class="string">&#x27;b i h w 1 -&gt; b i (2 h) w 2&#x27;</span>)</span><br><span class="line"></span><br><span class="line">out3 = torch.tile(out1,(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,))</span><br><span class="line"><span class="built_in">print</span>(out2.shape)</span><br><span class="line"><span class="built_in">print</span>(out3.shape)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304172006805.png&quot; alt=&quot;编写技巧&quot; /&gt;&lt;figcaption aria-hidden</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="tools" scheme="https://wangtongyouwen.github.io/categories/pytorch/tools/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="tools" scheme="https://wangtongyouwen.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门16-seq2seq2</title>
    <link href="https://wangtongyouwen.github.io/post/86ae38c6.html"/>
    <id>https://wangtongyouwen.github.io/post/86ae38c6.html</id>
    <published>2023-04-17T04:40:39.000Z</published>
    <updated>2023-04-17T12:58:01.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">1 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</h1><p>https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171444845.png" alt="image-20230417144427318" /><figcaption aria-hidden="true">image-20230417144427318</figcaption></figure><h1 id="effective-approaches-to-attention-based-neural-machine-translation">2 Effective Approaches to Attention-based Neural Machine Translation</h1><p>https://arxiv.org/pdf/1508.04025)</p><h2 id="global-attention">2.1 global attention</h2><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171447590.png" alt="image-20230417144755015" /><figcaption aria-hidden="true">image-20230417144755015</figcaption></figure><p>The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the <strong>context vector</strong> <span class="math inline">\(c_t\)</span> . In this model type, a <strong>variable-length alignment vector</strong> <span class="math inline">\(a_t\)</span> , whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state ht with each <strong>source hidden state</strong> <span class="math inline">\(\bar h _s\)</span>: <span class="math display">\[\begin{aligned}\boldsymbol{a}_{t}(s) &amp; =\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \\&amp; =\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s^{\prime}}\right)\right)}\end{aligned}\]</span> score is referred as a <strong>content-based function</strong> for which we consider three different alternatives <span class="math display">\[\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{ll}\boldsymbol{h}_{t}^{\top} \overline{\boldsymbol{h}}_{s} &amp; \text { dot } \\\boldsymbol{h}_{t}^{\top} \boldsymbol{W}_{\boldsymbol{a}} \overline{\boldsymbol{h}}_{s} &amp; \text { general } \\\boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{a}}\left[\boldsymbol{h}_{t} ; \overline{\boldsymbol{h}}_{s}\right]\right) &amp; \text { concat }\end{array}\right.\]</span> dot: transformer Q K V 并行计算</p><p>general: 乘法注意力机制</p><p>concat: 加法注意力机制</p><p>基于位置的: location-based function in which the <strong>alignment scores</strong> are computed from solely the target hidden state <span class="math inline">\(h_t\)</span> <span class="math display">\[a_t = softmax(W_ah_t)  \     \ location\]</span></p><p>全局注意力计算量大，有些任务也不需要进行全局注意力计算。</p><h2 id="local-attention">2.2 local attention</h2><p>This model takes inspiration from the tradeoff between the <strong>soft and hard</strong> attentional models</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171454431.png" alt="image-20230417145444733" /><figcaption aria-hidden="true">image-20230417145444733</figcaption></figure><ul><li>generate an aligned position <span class="math inline">\(p_t\)</span> for each target word at time <span class="math inline">\(t\)</span></li><li>the context vector <span class="math inline">\(c_t\)</span> is then derived as a weighted average over the set of source hidden states within the window <span class="math inline">\([p_t-D,p_t+D]\)</span> <span class="math inline">\(D\)</span> is empirically selected</li><li><span class="math inline">\(a_t\)</span> is fixed-dimensional, i.e. <span class="math inline">\(\in \R^{2D+1}\)</span></li><li>Monotonic alignment (<strong>local-m</strong>) <span class="math inline">\(p_t = t\)</span> assuming that source and target sequences are roughly monotonically aligned 编码器的中心位置是经验性的，不是训练出来的。hard 方法</li><li>Predictive alignment (<strong>local-p</strong>) 模型能够计算出编码器的中心位置，这是训练得到的 soft 方法</li></ul><p><span class="math display">\[p_{t}=S \cdot \operatorname{sigmoid}\left(\boldsymbol{v}_{p}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{p}} \boldsymbol{h}_{t}\right)\right)\]</span></p><ul><li><ul><li><span class="math inline">\(W_p,v_p\)</span>: the model parameters which will be learned to predict positions</li><li><span class="math inline">\(S\)</span>: the source sentence length</li></ul></li><li>alignment weights -&gt; Gaussian distribution</li></ul><p><span class="math display">\[\boldsymbol{a}_{t}(s)=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \exp \left(-\frac{\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right)  where \ \sigma = \frac{D}{2}\]</span></p><h1 id="代码实现">3 代码实现</h1><h2 id="encoder">3.1 encoder</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;以离散符号(索引的符号)的分类任务为例，实现基于注意力机制的seq2seq模型&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Seq2SeqEncoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;实现基于LSTM的编码器，也可以是其他类型的，如CNN，TransformerEncoder&quot;&quot;&quot;</span><br><span class="line">    &quot;&quot;&quot;对原序列进行上下文相关的表征&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, embedding_dim, hidden_size, source_vocab_size):</span><br><span class="line">        super(Seq2SeqEncoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)</span><br><span class="line">        # num_embeddings (int): size of the dictionary of embeddings</span><br><span class="line">        # embedding_dim (int): the size of each embedding vector</span><br><span class="line">        self.embedding_table = nn.Embedding(source_vocab_size, embedding_dim)  # 将token转换成 token_embedding # 可训练</span><br><span class="line"></span><br><span class="line">    def forward(self, input_ids):</span><br><span class="line">        # 原序列不是流式的，所以能够一次性拿到全部序列</span><br><span class="line">        input_sequence = self.embedding_table(input_ids)  # [bs,source_length,embedding_dim]</span><br><span class="line">        output_states, (final_h, final_c) = self.lstm_layer(input_sequence)</span><br><span class="line"></span><br><span class="line">        return output_states, final_h</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="attentionmechanism">3.2 AttentionMechanism</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Seq2SeqAttentionMechanism(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;实现dot-product的attention&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Seq2SeqAttentionMechanism, self).__init__()</span><br><span class="line"></span><br><span class="line">    def forward(self, decoder_state_t, encoder_states):</span><br><span class="line">        # encoder_states: 完整的编码序列</span><br><span class="line">        # decoder_State_t: t 时刻的decoder序列</span><br><span class="line">        bs, source_length, hidden_size = encoder_states.shape</span><br><span class="line"></span><br><span class="line">        decoder_state_t = decoder_state_t.unsqueeze(1)  # [bs,1,hidden_size]</span><br><span class="line">        decoder_state_t = torch.tile(decoder_state_t, dims=(1, source_length, 1))  # 复制为 [bs,source_length,hidden_size]</span><br><span class="line"></span><br><span class="line">        score = torch.sum(decoder_state_t * encoder_states, dim=-1)  # [bs,source_length]</span><br><span class="line"></span><br><span class="line">        attn_prob = F.softmax(score, dim=-1)  # 得到序列中每个具体索引在整个序列中的权重值,总和为1 [bs,source_length]</span><br><span class="line"></span><br><span class="line">        # 加权求和</span><br><span class="line">        # 使用了广播机制 attn_prob: [bs,source_length], decoder_state_t: [bs,source_length,hidden_size]</span><br><span class="line">        context = torch.sum(attn_prob.unsqueeze(-1) * encoder_states,1) # [bs,hidden_size] -&gt; 第t时刻解码器需要的上下文向量</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn_prob, context</span><br></pre></td></tr></table></figure><h2 id="decoder">3.3 Decoder</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">class Seq2SeqDecoder(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, embedding_dim, hidden_size, num_classes, target_vocab_size, start_id, end_id):</span><br><span class="line">        super(Seq2SeqDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.lstm_cell = nn.LSTMCell(embedding_dim, hidden_size) # cell 是单步完成，[bs,embedding_dim]</span><br><span class="line">        self.proj_layer = nn.Linear(hidden_size * 2, num_classes) # [context_vector+decode_state_t,num_classes]</span><br><span class="line">        self.attention_mechanism = Seq2SeqAttentionMechanism()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.embedding_table = nn.Embedding(target_vocab_size, embedding_dim)</span><br><span class="line">        self.start_id = start_id # 会传入一个真实的target作为输入，可以进行第一个位置的预测</span><br><span class="line">        self.end_id = end_id # 能够判断序列的截止位置</span><br><span class="line"></span><br><span class="line">    def forward(self, shifted_target_ids, encoder_states):</span><br><span class="line">        # 训练阶段调用，teacher-force mode</span><br><span class="line">        # shifted_target_ids 真实的序列id</span><br><span class="line">        shifted_target = self.embedding_table(shifted_target_ids)</span><br><span class="line"></span><br><span class="line">        bs, target_length, embedding_dim = shifted_target.shape</span><br><span class="line">        bs, source_length, hidden_size = encoder_states.shape</span><br><span class="line"></span><br><span class="line">        logits = torch.zeros(bs, target_length, self.num_classes)</span><br><span class="line">        probs = torch.zeros(bs, target_length, source_length)</span><br><span class="line"></span><br><span class="line">        for t in range(target_length):</span><br><span class="line">            decoder_input_t = shifted_target[:, t, :]  # [bs,embedding_dim]</span><br><span class="line">            if t == 0:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t)</span><br><span class="line">            else:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t, (h_t, c_t))</span><br><span class="line"></span><br><span class="line">            attn_prob, context = self.attention_mechanism(h_t, encoder_states) # [decoder_state_t, encoder_states]</span><br><span class="line">            # context: [bs,hidden_size] h_t: [bs,hidden_size]</span><br><span class="line">            decoder_output = torch.cat((context, h_t), -1)</span><br><span class="line">            logits[:, t, :] = self.proj_layer(decoder_output)</span><br><span class="line">            probs[:, t, :] = attn_prob # [bs,source_length]</span><br><span class="line"></span><br><span class="line">        return probs, logits</span><br><span class="line"></span><br><span class="line">    def inference(self, encoder_states):</span><br><span class="line">        # 推理阶段使用</span><br><span class="line">        target_id = self.start_id</span><br><span class="line">        h_t = None</span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        while True:</span><br><span class="line">            decoder_input_t = self.embedding_table(target_id)</span><br><span class="line">            if h_t is None:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t)</span><br><span class="line">            else:</span><br><span class="line">                h_t, c_t = self.lstm_cell(decoder_input_t, (h_t, c_t))</span><br><span class="line"></span><br><span class="line">            attn_prob, context = self.attention_mechanism(h_t, encoder_states)</span><br><span class="line"></span><br><span class="line">            decoder_output = torch.cat((context, h_t), -1)</span><br><span class="line">            logits = self.proj_layer(decoder_output)</span><br><span class="line"></span><br><span class="line">            target_id = torch.argmax(logits, -1)</span><br><span class="line">            result.append(target_id)</span><br><span class="line"></span><br><span class="line">            if torch.any(target_id == self.end_id): # 解码终止条件,在构造训练数据的时候需要在每个句子后面添加一个字符用做end id</span><br><span class="line">                print(&quot;start coding!&quot;)</span><br><span class="line">                break</span><br><span class="line">        predict_ids = torch.stack(result, dim=0)</span><br><span class="line"></span><br><span class="line">        return predict_ids</span><br></pre></td></tr></table></figure><h2 id="model">3.4 Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_size, num_classes, source_vocab_size, target_vocab_size, start_id, end_id</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = Seq2SeqEncoder(embedding_dim, hidden_size, source_vocab_size)</span><br><span class="line">        self.decoder = Seq2SeqDecoder(embedding_dim, hidden_size, num_classes, target_vocab_size, start_id, end_id)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_sequence_ids, shifted_target_ids</span>):</span><br><span class="line">        <span class="comment"># 训练阶段</span></span><br><span class="line">        encoder_states, final_h = self.encoder(input_sequence_ids)</span><br><span class="line">        probs, logits = self.decoder(shifted_target_ids, encoder_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> probs, logits</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inference</span>(<span class="params">self,predict_ids,input_sequence_ids</span>):</span><br><span class="line">        <span class="comment"># 推理阶段</span></span><br><span class="line">        encoder_states, final_h = self.encoder(input_sequence_ids)</span><br><span class="line">        probs, logits = self.decoder(predict_ids, encoder_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> probs, logits</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304171935294.png" alt="seq2seq代码编写技巧" /><figcaption aria-hidden="true">seq2seq代码编写技巧</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;neural-machine-translation-by-jointly-learning-to-align-and-translate&quot;&gt;1 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门15-SwimTransformer</title>
    <link href="https://wangtongyouwen.github.io/post/daaec8c8.html"/>
    <id>https://wangtongyouwen.github.io/post/daaec8c8.html</id>
    <published>2023-04-15T14:34:26.000Z</published>
    <updated>2023-04-17T03:39:02.629Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152245800.png" alt="image-20230415224534491" /><figcaption aria-hidden="true">image-20230415224534491</figcaption></figure><p>将transformer直接使用到CV领域遇到的问题：</p><ul><li>尺度问题：同一张图中代表不用语义信息的block尺度差距很大</li><li>处理像素问题的计算成本大(形成的序列很长)</li></ul><p>本文提出一种hierarchical transformer,主要使用了shifted windows</p><ul><li>自注意力机制是在这个窗口中计算的，序列长度大大降低</li><li>通过shifting这个操作能够让相邻的两个窗口之间产生交互，上下层之间有了cross-window-connection</li></ul><p>image classification/object detection/semantic segmentation</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152255423.png" alt="image-20230415225520894" /><figcaption aria-hidden="true">image-20230415225520894</figcaption></figure><h1 id="如何基于图片生成-patch-embedding">1.如何基于图片生成 patch embedding?</h1><h2 id="方法一">方法一</h2><ul><li>基于 pytorch unfold 的API将图片进行分块，也就是模仿卷积的思路，设置kernel_size=patch_size，得到分块后的图片</li><li>得到格式为[bs,num_patch,patch_depth]的张量</li><li>将张量与形状为[patch_depth,model_dim_C]的权重矩阵进行乘法操作，即可得到形状为[bs,num_patch,model_dim_C]的path_embedding ## 方法二</li><li>patch_depth等于input_channel*patch_size*patch_size</li><li>model_dim_C相当于二维卷积的输出通道数目</li><li>将形状为[patch_depth,model_dim_C]的权重矩阵转换为[model_dim_C,input_channel,patch_size,path_size]的卷积核</li><li>调用pytorch中的conv2d API得到卷积的输出张量，形状为[bs,output_channel,height,width]</li><li>转换为[bs,num_patch,model_dim_C]的格式，即为patch embedding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_naive</span>(<span class="params">image,patch_size,weight</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;直观方法实现patch_embedding&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># image shape: bs*channel*h*w</span></span><br><span class="line">    patch = F.unfold(image,kernel_size=(patch_size,patch_size),</span><br><span class="line">                    stride=(patch_size,patch_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    patch_embedding = patch @ weight</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_conv</span>(<span class="params">image,kernel,stride</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于二维卷积来实现patch_embedding，embedding的维度就是卷积的输出通道数&quot;&quot;&quot;</span></span><br><span class="line">    conv_output = F.conv2d(image,kernel,stride=stride) <span class="comment">#[bs,oc,oh.ow]</span></span><br><span class="line">    bs, oc, oh, ow = conv_output.shape</span><br><span class="line">    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">    </span><br></pre></td></tr></table></figure><h1 id="如何构建mhsa并计算其复杂度">2.如何构建MHSA并计算其复杂度？</h1><ul><li>基于输入x进行三个映射分别得到q,k,v<ul><li>此步复杂度为<span class="math inline">\(3LC^2\)</span>,其中<span class="math inline">\(L\)</span>为序列长度，<span class="math inline">\(C\)</span>为特征大小<br /></li><li><span class="math inline">\(q,k,v\)</span>维度:<span class="math inline">\([L,C]\)</span></li></ul></li><li>将q,k,v拆分成多头的形式，注意这里的多头各自计算不受影响，所以可以与bs维度进行统一看待(c-&gt;c/n，把embedding看成一个个小的embedding)</li><li>计算<span class="math inline">\(qk^T\)</span>,并考虑可能的掩码，即让无效的两两位置之间的能量为负无穷，掩码在shift window MHSA中会需要，而在window MHSA中暂不需要<ul><li><span class="math inline">\(attn\_prob=\frac{q\times k^T}{\sqrt{d}}\)</span></li><li>此步复杂度为<span class="math inline">\(L^2C\)</span></li></ul></li><li>计算概率值与<span class="math inline">\(v\)</span>的乘积<ul><li>概率维度:<span class="math inline">\([L,L]\)</span>;<span class="math inline">\(v\)</span>维度:<span class="math inline">\([L,C]\)</span></li><li>此步复杂度为<span class="math inline">\(L^2C\)</span></li></ul></li><li>对输出进行再次映射<ul><li>映射矩阵:<span class="math inline">\([C,C]\)</span>;输出矩阵:<span class="math inline">\([L,C]\)</span></li><li>此步复杂度为<span class="math inline">\(LC^2\)</span></li></ul></li><li>总体复杂度为<span class="math inline">\(4LC^2+2L^2C\)</span></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadSelfAttention</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model_dim,num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadSelfAttention,self).__init__()</span><br><span class="line">        self.num_head = num_head</span><br><span class="line">        </span><br><span class="line">        self.proj_linear_layer = nn.Linear(model_dim,<span class="number">3</span>*model_dim)</span><br><span class="line">        self.final_linear_layer = nn.Linear(model_dim,model_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span>,additive_mask = <span class="literal">None</span></span>):</span><br><span class="line">        bs, seqlen, model_dim = <span class="built_in">input</span>.shape</span><br><span class="line">        num_head = self.num_head</span><br><span class="line">        head_dim = model_dim // num_head</span><br><span class="line">        </span><br><span class="line">        proj_output = self.proj_linear_layer(<span class="built_in">input</span>)</span><br><span class="line">        q,k,v = proj_output.chunk(<span class="number">3</span>,dim=-<span class="number">1</span>) <span class="comment"># [bs,seqlen,seqlen,model_dim]</span></span><br><span class="line">        </span><br><span class="line">        q = q.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        q = q.reshape(bs*num_head,seqlen,head_dim)</span><br><span class="line">        </span><br><span class="line">        k = k.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        k = k.reshape(bs*num_head,seqlen,head_dim)</span><br><span class="line">        </span><br><span class="line">        v = v.reshape(bs,seqlen,num_head,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,num_head,seqlen,head_dim]</span></span><br><span class="line">        v = v.reshape(bs*num_head,seqlen,head_dim) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> additive_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attn_prob = F.softmax(torch.bmm(q,k.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/math.sqrt(head_dim),dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            additive_mask = additive_mask.tile((num_head,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">            attn_prob = F.softmax(bs,num_head,seqlen,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs*num_head,seqlen,seqlen]</span></span><br><span class="line">            </span><br><span class="line">        output = torch.bmm(attn_prob,v) <span class="comment"># [bs*num_head,seqlen,head_dim]</span></span><br><span class="line">        output = output.reshape(bs,num_head,seqlen,head_dim).transpose(<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># [bs,seqlen,num_head,head_dim]</span></span><br><span class="line">        output = output.reshape(bs,seqlen,model_dim)</span><br><span class="line">        </span><br><span class="line">        output = self.final_linear_layer(output)</span><br><span class="line">        <span class="keyword">return</span> attn_prob,ouput</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="如何构建window-mhsa并计算其复杂度">3.如何构建Window MHSA并计算其复杂度？</h1><ul><li><p>将patch组成的图片进一步划分成一个个更大的window</p><ul><li>首先需要将三维的patch embedding转换成图片格式 [bs,channel,h,w] num_patch = h*w</li><li>使用unfold来将patch换分成window</li></ul></li><li><p>在每个window内部计算MHSA</p><ul><li><p>window数目其实可以跟batchsize进行统一对待，因为window与window之间没有交互计算</p></li><li><p>关于计算复杂度</p><ul><li><p>假设窗的边长为<span class="math inline">\(W\)</span>，起那么计算每个窗的总体复杂度是<span class="math inline">\(4W^2C^2+2W^4C\)</span></p></li><li><p>假设patch的总数目为<span class="math inline">\(L\)</span>,那么窗的数目为<span class="math inline">\(\frac{L}{W^2}\)</span></p></li><li><p>因此，W-HMSA的总体复杂度为<span class="math inline">\((4W^2C^2+2W^4C)\times\frac{L}{W^2} = 4LC^2+2LW^2C\)</span></p></li></ul></li><li><p>此处不需要mask</p></li><li><p>将计算结果转换成window的四维张量形式</p></li></ul></li><li><p>复杂度对比</p><ul><li>MHSA:<span class="math inline">\(4LC^2+2L^2C\)</span></li><li>W-MHSA:<span class="math inline">\(4LC^2+2LW^2C\)</span></li></ul><p>​</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_multi_head_self_attention</span>(<span class="params">patch_embedding,mhsa,window_size=<span class="number">4</span>,num_head=<span class="number">2</span></span>):</span><br><span class="line">    num_patch_in_window = widow_size * widow_size</span><br><span class="line">    bs, num_patch, patch_depth = patch_embedding.shape</span><br><span class="line">    image_height = image_width = <span class="built_in">int</span>(math.sqrt(num_patch))</span><br><span class="line">    </span><br><span class="line">    patch_embedding = patch_embedding,transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    patch = patch_embedding.reshape(bs,patch_depth,image_height,image_width)</span><br><span class="line">    window = F.unfold(patch,kernel_size=(widow_size,widow_size),</span><br><span class="line">                     stride=(window_size,window_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># [bs,num_window,window_depth]</span></span><br><span class="line">    bs,num_window,patch_depth_times_num_patch_in_window = window.shape</span><br><span class="line">    window = window.reshape(bs*num_window,patch_depth,num_patch_in_window).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    attn_prob, output = mhsa(window) <span class="comment"># [bs*num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    output = output.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h1 id="如何构建shift-window-mhsa及其mask">4.如何构建Shift Window MHSA及其Mask？</h1><ul><li>将上一步的W-MHSA的结果转换成图片格式</li><li>假设已经做了新的window划分，这一步叫做shift-window</li><li>为了保持window数目不变从而有高效的计算，需要将图片的patch往左和往上各自滑动半个窗口大小的步长，保持patch所属window类型不变</li><li>将图片patch还原成window的数据格式</li><li>由于cycle shift后，每个window虽然形状规整，但部分window中存在原本不属于同一个窗口的patch，所以需要生成mask</li><li>如何生成mask？<ul><li>首先构建一个shift-window的patch所属的window类别矩阵</li><li>对该矩阵进行同样的往左往上各自滑动半个窗口大小的步长的操作</li><li>通过unfold操作得到[bs,num_window,num_patch_in_window]形状的类别矩阵</li><li>对该矩阵进行扩维成[bs,num_window,num_patch_in_window,1]</li><li>将该矩阵与其转置矩阵进行作差，得到同类关系矩阵(为0的位置上的patch属于同类，否则属于不同类)</li><li>对同类关系矩阵中非零的位置用负无穷数进行填充，对于零的位置用0去填充，这样就构建好了MHSA所需要的mask</li><li>此mask的形状为[bs,num_window,num_patch_in_window,num_patch_in_window]</li></ul></li><li>将window转换成三维的格式，[bs*num_window,num_patch_in_window,patch_depth]</li><li>将三维格式的特征连同mask一起送入MHSA中计算得到注意力输出</li><li>将注意力输出转换成图片patch格式，[bs,num_window,num_patch_in_window,patch_depth]</li><li>为了恢复位置，需要将图片的patch往右和往下各自滑动半个窗口大小的步长，至此，SW-MHSA计算完毕</li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/note/other/image-20230416152318837.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个辅助函数，window2image，也就是将transformer block的结构转换成图片的形式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window2image</span>(<span class="params">msa_output</span>):</span><br><span class="line">    bs,num_window,num_patch_in_window,patch_depth = msa_output.shape</span><br><span class="line">    window_size = <span class="built_in">int</span>(math.sqrt(num_patch_in_window))</span><br><span class="line">    image_height = <span class="built_in">int</span>(math.sqrt(num_window)) * window_size</span><br><span class="line">    image_width = image_height</span><br><span class="line">    </span><br><span class="line">    msa_output = msa_output.reshape(bs,<span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                       <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                       window_size,</span><br><span class="line">                                       window_size,</span><br><span class="line">                                       patch_depth)</span><br><span class="line">    msa_output = msa_output.transpose(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    image = msa_output.reshape(bs,image_height*image_width,patch_depth)</span><br><span class="line">    image = image.transpose(-<span class="number">1</span>,-<span class="number">2</span>).reshape(bs,patch_depth,image_height,image_width)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义辅助函数 shift_window，即高效计算sw-msa</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_window</span>(<span class="params">w_msa_output,window_size,shift_size,generate_mask=<span class="literal">False</span></span>): <span class="comment"># 只有在正向的时候才会 shift</span></span><br><span class="line">    bs,num_window,num_patch_in_window,patch_depth = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    w_msa_output = window2image(w_msa_output) <span class="comment"># [bs,depth,h,w]</span></span><br><span class="line">    bs,patch_depth,image_height,image_width = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    rolled_w_msa_output = torch.roll(w_msa_output,shifts=(shift_size,shift_size),dim=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = rolled_w_msa_output.reshape(bs,patch_depth,</span><br><span class="line">                                                     <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                                     window_size,</span><br><span class="line">                                                     <span class="built_in">int</span>(math.sqrt(num_window)),</span><br><span class="line">                                                     window_size)</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.transpose(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.reshape(bs,patch_depth,num_window*num_patch_in_window)</span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># [bs,num_window*num_patch_in_window,patch_depth]</span></span><br><span class="line">    shifted_window = shifted_w_msa_input.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> generate_mask:</span><br><span class="line">        additive_mask = build_mask_for_shifted_wmsa(bs,image_height,image_width,window_size) <span class="comment"># [bs,num_window,num_patch_in_windows,num_patch_in_windows]</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        additive_mask = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> shifted_window, additive_mask</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建shift window multi-head attention mask</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_mask_for_shifted_wmsa</span>(<span class="params">batch_size,image_height,image_width,window_size</span>):</span><br><span class="line">    index_metrix = torch.zeros(image_height,image_width)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(image_height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(image_width):</span><br><span class="line">            row_times = (i + window_size // <span class="number">2</span>) // window_size</span><br><span class="line">            col_times = (j + window_size // <span class="number">2</span>) // window_size</span><br><span class="line">            index_metrix[i,j] = row_times * (image_height/window_size) + col_times + <span class="number">1</span></span><br><span class="line">    rolled_index_matrix = torch.roll(index_metrix,shifts=(-window_size//<span class="number">2</span>,-window_size//<span class="number">2</span>),dims=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    rolled_index_matrix = rolled_index_matrix.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>) <span class="comment"># [bs,ch,h,w]</span></span><br><span class="line">    </span><br><span class="line">    c = F.unfold(rolled_index_matrix,kernel_size=(window_size,window_size),</span><br><span class="line">                stride=(window_size,window_size)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    c = c.tile(batch_size,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># [bs.num_window,num_patch_in_window]</span></span><br><span class="line">    </span><br><span class="line">    bs, num_window,num_patch_in_window = c.shape</span><br><span class="line">    </span><br><span class="line">    c1 = c.unsqueeze(-<span class="number">1</span>) <span class="comment"># [bs,num_window,num_patch_in_windows,1]</span></span><br><span class="line">    c2 = (c1-c1.transpose(-<span class="number">1</span>,-<span class="number">2</span>) == -<span class="number">0</span>) <span class="comment"># [bs,num_window,num_patch_in_windows,num_patch_in_windows]</span></span><br><span class="line">    valid_matrix = c2.to(torch.float32)</span><br><span class="line">    additive_mask = (<span class="number">1</span> - valid_matrix) * (-<span class="number">1e-9</span>) </span><br><span class="line">    </span><br><span class="line">    additive_mask = additive_mask.reshape(bs*num_window,num_patch_in_window,num_patch_in_window)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> additive_mask</span><br><span class="line">            </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_window_multi_head_self_attention</span>(<span class="params">w_msa_output,mhsa,window_size=<span class="number">4</span>,num_head=<span class="number">2</span></span>):</span><br><span class="line">    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape</span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input, additive_mask = shift_window(w_msa_output,window_size,</span><br><span class="line">                                                      shift_size=(-window_size//<span class="number">2</span>),</span><br><span class="line">                                                      generate_mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(shifted_w_msa_input.shape) <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    <span class="built_in">print</span>(additive_mask.shape) <span class="comment"># [bs*num_window,num_patch_in_window,num_patch_in_window]</span></span><br><span class="line">    </span><br><span class="line">    shifted_w_msa_input = shifted_w_msa_input.reshape(bs*num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    attn_prob,output = mhsa(shifted_w_msa_input,additive_mask=additive_mask)</span><br><span class="line">    </span><br><span class="line">    output = output.reshape(bs,num_window,num_patch_in_window,patch_depth)</span><br><span class="line">    </span><br><span class="line">    output, _ = shift_window(output,window_size,shift_size=window_size//<span class="number">2</span>,generate_mask=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape) <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h1 id="如何构建patch-merging">5.如何构建Patch Merging？</h1><ul><li>将window格式的特征转换成图片patch格式</li><li>利用unfold操作，按照merge_size*merge_size大小得到新的patch,形状为[bs,num_patch_new,merge_size*merge_size*patch_depth_old]</li><li>使用一个全连接层对depth进行降维成0.5倍，也就是从merge_size*merge_size*patch_depth_old映射到0.5*merge_size*merge_size*patch_depth_old</li><li>输出的是patch embedding的形状格式,[bs,num_patch,patch_depth]</li><li>举例说明：以merge_size=2为例，经过PatchMerging后，patch数目减少为之前的<span class="math inline">\(\frac{1}{4}\)</span>，但是depth增大为原来的2倍，而不是4倍</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_dim, merge_size,output_depth_scale=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PatchMerging, self).__init__()</span><br><span class="line">        self.merge_size = merge_size</span><br><span class="line">        self.model_dim = model_dim</span><br><span class="line">        self.proj_layer = nn.Linear(</span><br><span class="line">            model_dim * merge_size * merge_size,</span><br><span class="line">            <span class="built_in">int</span>(model_dim * merge_size * merge_size * output_depth_scale))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        bs, num_window, num_patch_in_window, patch_depth = <span class="built_in">input</span>.shape</span><br><span class="line">        window_size = <span class="built_in">int</span>(math.sqrt(num_patch_in_window))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">input</span> = window2image(<span class="built_in">input</span>)  <span class="comment"># [bs,patch_depth,image_h,image_w]</span></span><br><span class="line"></span><br><span class="line">        merged_window = F.unfold(<span class="built_in">input</span>, kernel_size=(self.merge_size, self.merge_size),</span><br><span class="line">                                 stride=(self.merge_size, self.merge_size)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line">        merged_window = self.proj_layer(merged_window)  <span class="comment"># [bs,num_patch,new_patch_depth]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> merged_window</span><br></pre></td></tr></table></figure><h1 id="如何构建swin-transformerblock">6.如何构建Swin TransformerBlock?</h1><ul><li>每个block包含LayerNorm，W-MHSA，MLP，SW-MHSA，残差连接等模块</li><li>输入是patch embedding格式</li><li>每个MLP包括两层，分别是4*mode_dim和mode_dim大小</li><li>输出的是window的数据格式，[bs,num_window,num_patch_in_window,patch_depth]</li><li>需要注意残差连接对数据形状的要求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model_dim,window_size,num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(SwinTransformerBlock,self).__init__()</span><br><span class="line">        self.layer_norm1 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm2 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm3 = nn.LayerNorm(model_dim)</span><br><span class="line">        self.layer_norm4 = nn.LayerNorm(model_dim)</span><br><span class="line">        </span><br><span class="line">        self.wsma_mlp1 = nn.Linear(model_dim,<span class="number">4</span>*model_dim)</span><br><span class="line">        self.wsma_mlp2 = nn.Linear(<span class="number">4</span>*model_dim,model_dim)</span><br><span class="line">        self.swsma_mlp1 = nn.Linear(model_dim,<span class="number">4</span>*model_dim)</span><br><span class="line">        self.swsma_mlp2 = nn.Linear(<span class="number">4</span>*model_dim,model_dim)</span><br><span class="line">        </span><br><span class="line">        self.mhsa1 = MultiHeadSelfAttention(model_dim,num_head)</span><br><span class="line">        self.mhsa2 = MultiHeadSelfAttention(model_dim,num_head)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        </span><br><span class="line">        bs,num_patch,patch_depth = <span class="built_in">input</span>.shape</span><br><span class="line">        </span><br><span class="line">        input1 = self.layer_norm1(<span class="built_in">input</span>)</span><br><span class="line">        w_msa_output = window_multi_head_self_attention(<span class="built_in">input</span>,self.mhsa1,window_size=<span class="number">4</span>,num_head=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#         bs, num_head, num_patch_in_window, patch_depth = w_msa_output.shape </span></span><br><span class="line">        w_msa_output = <span class="built_in">input</span> + w_msa_output.reshape(bs,num_patch,patch_depth)</span><br><span class="line">        output1 = self.wsma_mlp2(self.wsma_mlp1(self.layer_norm2(w_msa_output)))</span><br><span class="line">        output1 += w_msa_output</span><br><span class="line">        </span><br><span class="line">        input2 = self.layer_norm3(input1)</span><br><span class="line">        input2 = input2.reshape(bs,num_patch,num_patch_in_window,patch_depth)</span><br><span class="line">        sw_msa_output = shift_window_multi_head_self_attention(input2,self.mhsa2,window_size=<span class="number">4</span>,num_head=<span class="number">2</span>)</span><br><span class="line">        sw_msa_output = output1 + sw_msa_output.reshape(bs,num_patch,patch_depth)</span><br><span class="line">        output2 = self.swsma_mlp2(self.swsma_mlp1(self.layer_norm4(sw_msa_output)))</span><br><span class="line">        output2 += sw_msa_output</span><br><span class="line">        </span><br><span class="line">        output2 = output2.reshape(bs,num_patch,num_patch_in_window.patch_depth)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output2</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="如何构建swintransformermodel">7.如何构建SwinTransformerModel？</h1><ul><li>输入是图片</li><li>首先对图片进行分块并得到Patch embedding</li><li>经过第一个stage</li><li>进行patch merging,在进行第二个stage</li><li>以此类推...</li><li>对最后一个block的输出转换成patch embedding的格式[bs,num_patch_depth]</li><li>对patch embedding在时间维度进行平均池化，并映射到分类层得到分类的logits</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_image_channel=<span class="number">3</span>, patch_size=<span class="number">4</span>, model_dim_C=<span class="number">8</span>, num_classes=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">4</span>, num_head=<span class="number">2</span>, merge_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SwinTransformerModel, self).__init__()</span><br><span class="line">        patch_depth = patch_size * patch_size * input_image_channel</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.model_dim_C = model_dim_C</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.patch_embedding_weight = nn.Parameter(torch.randn(patch_depth, model_dim_C))  <span class="comment"># 模型可训练的一部分参数</span></span><br><span class="line"></span><br><span class="line">        self.block1 = SwinTransformerBlock(model_dim_C, window_size, num_head)</span><br><span class="line">        self.block2 = SwinTransformerBlock(model_dim_C * <span class="number">2</span>, window_size, num_head)</span><br><span class="line">        self.block3 = SwinTransformerBlock(model_dim_C * <span class="number">4</span>, window_size, num_head)</span><br><span class="line">        self.block4 = SwinTransformerBlock(model_dim_C * <span class="number">8</span>, window_size, num_head)</span><br><span class="line"></span><br><span class="line">        self.patch_merging1 = PatchMerging(model_dim_C , merge_size)</span><br><span class="line">        self.patch_merging2 = PatchMerging(model_dim_C * <span class="number">2</span>, merge_size)</span><br><span class="line">        self.patch_merging3 = PatchMerging(model_dim_C * <span class="number">4</span>, merge_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类</span></span><br><span class="line">        self.final_layer = nn.Linear(model_dim_C * <span class="number">8</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image</span>):</span><br><span class="line">        patch_embedding_naive = image2emb_naive(image, self.patch_size, self.patch_embedding_weight)</span><br><span class="line">        <span class="built_in">print</span>(patch_embedding_naive.shape)</span><br><span class="line"></span><br><span class="line">        kernel = self.patch_embedding_weight.transpose(<span class="number">0</span>, <span class="number">1</span>).reshape((-<span class="number">1</span>, ic, patch_size, patch_size))  <span class="comment"># oc*ic*kh*kw</span></span><br><span class="line">        patch_embedding_conv = image2emb_conv(image, kernel, self.patch_size)  <span class="comment"># 二维卷积的方法得到embedding</span></span><br><span class="line">        <span class="built_in">print</span>(patch_embedding_conv.shape)</span><br><span class="line">        <span class="comment"># block1</span></span><br><span class="line">        patch_embedding = patch_embedding_naive</span><br><span class="line">        <span class="built_in">print</span>(patch_embedding.shape)</span><br><span class="line">        sw_msa_output = self.block1(patch_embedding)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block1_output&quot;</span>, sw_msa_output.shape)  <span class="comment"># [bs,num_window,num_patch_in_window,patch_depth]</span></span><br><span class="line"></span><br><span class="line">        merged_patch1 = self.patch_merging1(sw_msa_output)</span><br><span class="line">        sw_msa_output_1 = self.block2(merged_patch1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block2_output&quot;</span>, sw_msa_output_1.shape)</span><br><span class="line"></span><br><span class="line">        merged_patch2 = self.patch_merging2(sw_msa_output_1)</span><br><span class="line">        sw_msa_output_2 = self.block3(merged_patch2)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block3_output&quot;</span>, sw_msa_output_2.shape)</span><br><span class="line"></span><br><span class="line">        merged_patch3 = self.patch_merging3(sw_msa_output_2)</span><br><span class="line">        sw_msa_output_3 = self.block4(merged_patch3)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;block4_output&quot;</span>, sw_msa_output_3.shape)</span><br><span class="line"></span><br><span class="line">        bs, num_window, num_patch_in_window, patch_depth = sw_msa_output_3.shape</span><br><span class="line">        sw_msa_output_3 = sw_msa_output_3.reshape(bs, -<span class="number">1</span>, patch_depth)</span><br><span class="line"></span><br><span class="line">        pool_output = torch.mean(sw_msa_output_3, dim=<span class="number">1</span>)</span><br><span class="line">        logits = self.final_layer(pool_output)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;logits&quot;</span>,logits.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="模型测试代码">8.模型测试代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get parameters amount in the network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_n_params</span>(<span class="params">model</span>):</span><br><span class="line">    pp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">list</span>(model.parameters()):</span><br><span class="line">        nn = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">list</span>(p.size()):</span><br><span class="line">            nn = nn * s</span><br><span class="line">        pp += nn</span><br><span class="line">    <span class="keyword">return</span> pp</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    bs, ic, image_h, image_w = <span class="number">4</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">    patch_size = <span class="number">4</span></span><br><span class="line">    model_dim_C = <span class="number">8</span>  <span class="comment"># 一开始的patch embedding大小</span></span><br><span class="line">    max_num_token = <span class="number">16</span></span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    window_size = <span class="number">4</span></span><br><span class="line">    num_head = <span class="number">2</span></span><br><span class="line">    merge_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    patch_depth = patch_size * patch_size * ic</span><br><span class="line">    image = torch.randn(bs, ic, image_h, image_w)</span><br><span class="line"></span><br><span class="line">    model = SwinTransformerModel(ic, patch_size, model_dim_C, num_classes, window_size, num_head, merge_size)</span><br><span class="line"></span><br><span class="line">    model(image)</span><br><span class="line">    pp = get_n_params(model)</span><br><span class="line">    <span class="built_in">print</span>(pp)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152245800.png&quot; alt=&quot;image-20230415224534491&quot; /&gt;&lt;fig</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目2-基于GCN(DNN)的文本分类模型(GPU)</title>
    <link href="https://wangtongyouwen.github.io/post/ceb60e40.html"/>
    <id>https://wangtongyouwen.github.io/post/ceb60e40.html</id>
    <published>2023-04-15T11:19:44.000Z</published>
    <updated>2023-04-17T12:56:58.717Z</updated>
    
    <content type="html"><![CDATA[<p>https://pytorch.org/tutorials/distributed/home.html</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152009298.png" alt="image-20230415200908110" /><figcaption aria-hidden="true">image-20230415200908110</figcaption></figure><h2 id="单机单卡">1 单机单卡</h2><p>在 main 函数前加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is available!&quot;</span>)</span><br><span class="line">    os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is not available! Exit!&quot;</span>)</span><br></pre></td></tr></table></figure><p>在train函数中，第一次调用模型的地方加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.cuda() <span class="comment"># 拷贝到GPU上，模型拷贝</span></span><br></pre></td></tr></table></figure><p>在train函数中，每次使用到数据的地方加入(train and eval)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">token_index = token_index.cuda()  <span class="comment"># tensor cuda拷贝方法,数据拷贝</span></span><br><span class="line">target = target.cuda() <span class="comment"># 数据拷贝</span></span><br><span class="line">eval_target = eval_target.cuda()  <span class="comment"># tensor cuda拷贝方法,数据拷贝</span></span><br><span class="line">eval_token_index = eval_token_index.cuda()  <span class="comment"># 数据拷贝</span></span><br></pre></td></tr></table></figure><h2 id="单机多卡">2 单机多卡</h2><p>在 main 函数前加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is available!&quot;</span>)</span><br><span class="line">    os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">        logging.warning(<span class="string">f&quot;find <span class="subst">&#123;torch.cuda.device_count()&#125;</span> GPUs!&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logging.warning(<span class="string">&quot;Too few GPU!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">&quot;Cuda is not available! Exit!&quot;</span>)</span><br></pre></td></tr></table></figure><p>在train函数中，第一次调用模型的地方加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model.cuda(),device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 拷贝到GPU上，模型拷贝,放入DataParallel</span></span><br></pre></td></tr></table></figure><p>在模型的保存中，修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % save_step_interval == <span class="number">0</span>:</span><br><span class="line">    os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    save_file = os.path.join(save_path, <span class="string">f&quot;step_<span class="subst">&#123;step&#125;</span>.pt&quot;</span>)</span><br><span class="line">    torch.save(&#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">&quot;model_state_dict&quot;</span>: model.module.state_dict(),</span><br><span class="line">...</span><br><span class="line">    &#125;, save_file)</span><br><span class="line">    logging.warning(<span class="string">f&quot;checkpoint has been saved in <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这种方式更加合理：</p><figure><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230415211135334.png" alt="image-20230415211135334" /><figcaption aria-hidden="true">image-20230415211135334</figcaption></figure><h3 id="init_process_group">2.1 init_process_group</h3><p>This sets up the communication backend for distributed training (such as NCCL, Gloo, etc.) and initializes the process group.</p><h4 id="nccl">2.1.1 nccl</h4><p>https://developer.nvidia.com/nccl</p><h4 id="world_size">2.1.2 world_size</h4><p>当前节点上有多少张GPU</p><h4 id="local_rank">2.1.3 local_rank</h4><p>当前进程在某张确定的GPU卡上(因为这里采用了多线程，每个线程表示一张GPU)</p><h3 id="torch.cuda.set_deviceargs.local_rank">2.2 torch.cuda.set_device(args.local_rank)</h3><p>设定某张卡进行训练</p><h3 id="对模型进行包裹">2.3 对模型进行包裹</h3><p>类似DataParallel中的操作</p><h3 id="train_sampler">2.4 train_sampler</h3><p>https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler</p><p>把dataset中的样本分配当不同的GPU上面，这是随机分配的</p><p>It is especially useful in <strong>conjunction</strong> with class:torch.nn.parallel.<strong>DistributedDataParallel</strong></p><p>每张卡上面的参数的总数量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.num_samples = math.ceil(<span class="built_in">len</span>(self.dataset) / self.num_replicas)  <span class="comment"># type: ignore[arg-<span class="built_in">type</span>]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span><br><span class="line">        <span class="keyword">if</span> self.shuffle:</span><br><span class="line">            <span class="comment"># deterministically shuffle based on epoch and seed</span></span><br><span class="line">            g = torch.Generator()</span><br><span class="line">            g.manual_seed(self.seed + self.epoch)  <span class="comment"># 不改变种子和epoch，顺序是相同的(显然不合理)</span></span><br><span class="line">            indices = torch.randperm(<span class="built_in">len</span>(self.dataset), generator=g).tolist()  <span class="comment"># type: </span></span><br></pre></td></tr></table></figure><p>显然在需要在每个训练之前对此进行修改，调用sampler.set_epoch方法，把epoch传入进来。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304152228839.png" alt="image-20230415222851173" /><figcaption aria-hidden="true">image-20230415222851173</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://pytorch.org/tutorials/distributed/home.html&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/bl</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>pytorch项目1-基于GCN+DNN的文本分类模型</title>
    <link href="https://wangtongyouwen.github.io/post/27f58942.html"/>
    <id>https://wangtongyouwen.github.io/post/27f58942.html</id>
    <published>2023-04-15T07:04:41.000Z</published>
    <updated>2023-04-17T12:55:13.613Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目介绍">1 项目介绍</h2><p>IMDB dataset having 50K movie reviews for natural language processing or Text analytics.</p><p>This is a dataset for <strong>binary</strong> sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of <strong>25,000</strong> highly polar movie reviews for training and <strong>25,000</strong> for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.</p><p>代码构成：</p><ul><li>GCNN模型</li><li>简单版embeddingbag+DNN模型</li><li>yield_tokens: 对源 comment 进行分词处理</li><li>collate_fn： 对DataLoader所生成的mini-batch进行后处理</li><li>train</li><li>main函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> IMDB</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.imdb <span class="keyword">import</span> NUM_LINES</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.WARN, stream=sys.stdout,</span><br><span class="line">                    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s (%(module)s:%(lineno)d) %(levelname)s:%(message)s&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = <span class="number">15000</span> <span class="comment"># 这里的统计是根据数据集进行处理后得到的</span></span><br><span class="line">train_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;train&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&quot;basic_english&quot;</span>) </span><br><span class="line">vocab = build_vocab_from_iterator(yield_tokens(train_data_iter, tokenizer), min_freq=<span class="number">20</span>, specials=[<span class="string">&quot;&lt;unk&gt;&quot;</span>]) <span class="comment"># 只把出现频率高于20个的词语取出来，其他的单词都变成&lt;unk&gt; # 构建词表</span></span><br><span class="line">vocab.set_default_index(<span class="number">0</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;单词表大小：<span class="subst">&#123;<span class="built_in">len</span>(vocab)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果长时间下载失败，建议直接在以下链接进行下载：</p><p>http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</p><p>之后把下载好的文件放在当前目录下的<code>.data</code>中</p><h2 id="gcnn模型">2 GCNN模型</h2><h3 id="介绍">2.1 介绍</h3><p>https://arxiv.org/pdf/1612.08083v3.pdf</p><p>门控卷积网络是一种将卷积网络与门控机制相结合的语言模型。使用零填充以确保未来的语境无法被观察。门控卷积层可以层次化地叠加在其他层之上。通过自适应softmax层来获取模型预测结果。</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304151758805.png" alt="image-20230415175810671" style="zoom:67%;" /></p><h3 id="代码">2.2 代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocal_size=VOCAB_SIZE, embedding_dim=<span class="number">64</span>, num_class=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GCNN, self).__init__()  <span class="comment"># 对父类进行初始化</span></span><br><span class="line"></span><br><span class="line">        self.embedding_table = nn.Embedding(vocal_size, embedding_dim)</span><br><span class="line">        nn.init.xavier_uniform_(self.embedding_table.weight)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;xavier_uniform的出现是为了训练过程中前后的方差稳定问题，正确的初始化有利于训练的稳定；</span></span><br><span class="line"><span class="string">        Xavier初始化表明，对于每⼀层，输出的⽅差不受输⼊数量的影响，任何梯度的⽅差不受输出数量的影响。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.conv_A_1 = nn.Conv1d(embedding_dim, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)  <span class="comment"># input_dim,output_dim,kernel_Size</span></span><br><span class="line">        self.conv_B_1 = nn.Conv1d(embedding_dim, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        self.conv_A_2 = nn.Conv1d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line">        self.conv_B_2 = nn.Conv1d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">15</span>, stride=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        self.output_linear1 = nn.Linear(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.output_linear2 = nn.Linear(<span class="number">128</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, word_index</span>):</span><br><span class="line">        <span class="comment"># 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1.通过word_index得到word_embedding</span></span><br><span class="line">        <span class="comment"># word_index shape: [bs,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        word_embedding = self.embedding_table(word_index)  <span class="comment"># [bs,max_seq_len,embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2.编写第一层ID门卷积模块</span></span><br><span class="line">        word_embedding = word_embedding.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [bs,embedding_dim,max_seq_len]</span></span><br><span class="line">        A = self.conv_A_1(word_embedding)</span><br><span class="line">        B = self.conv_B_1(word_embedding)</span><br><span class="line">        H = A * torch.sigmoid(B)  <span class="comment"># [bs,64,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        A = self.conv_A_2(H)</span><br><span class="line">        B = self.conv_B_2(H)</span><br><span class="line">        H = A * torch.sigmoid(B)  <span class="comment"># [bs,64,max_seq_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.池化并进过全连接层</span></span><br><span class="line">        pool_output = torch.mean(H, dim=-<span class="number">1</span>)  <span class="comment"># 平均池化 得到 [bs,64]</span></span><br><span class="line">        linear1_output = self.output_linear1(pool_output)</span><br><span class="line">        logits = self.output_linear2(linear1_output)  <span class="comment"># [bs,2]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h2 id="简单版embeddingbagdnn模型">3 简单版embeddingbag+DNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextClassificationModel</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=VOCAB_SIZE, embed_dim=<span class="number">64</span>, num_class=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TextClassificationModel, self).__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token_index</span>):</span><br><span class="line">        embedded = self.embedding(token_index)  <span class="comment"># shape: [bs,embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure><p>通过embeddingbag可以省略平均池化层操作，变得更加简单</p><figure><img src="https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=1024" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="yield_tokens">4 yield_tokens</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">train_data_iter, tokenizer</span>):</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_iter):</span><br><span class="line">        label, comment = sample</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(comment)  <span class="comment"># 把一句话转换成一个个token的列表</span></span><br></pre></td></tr></table></figure><h2 id="collate_fn">5 collate_fn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>): </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;mini-batch 中最重要的就是对同一个批次内的数据进行统一处理，比如某些句子很短，需要padding，这样才能进行batch操作&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对dataloader所生成的mini-batch进行后处理&quot;&quot;&quot;</span></span><br><span class="line">    target = []</span><br><span class="line">    token_index = []</span><br><span class="line">    max_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (label, comment) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch):</span><br><span class="line">        tokens = tokenizer(comment)</span><br><span class="line">        token_index.append(vocab(tokens))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens) &gt; max_length:</span><br><span class="line">            max_length = <span class="built_in">len</span>(tokens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> label == <span class="string">&quot;pos&quot;</span>:</span><br><span class="line">            target.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    token_index = [index + [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(index)) <span class="keyword">for</span> index <span class="keyword">in</span> token_index]</span><br><span class="line">    <span class="keyword">return</span> (torch.tensor(target).to(torch.int64), torch.tensor((token_index)).to(torch.int32))  <span class="comment"># target：pos/neg token_index</span></span><br></pre></td></tr></table></figure><h2 id="train">6 train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_data_loader, eval_data_loader, model, optimizer, num_epoch, log_step_interval, save_step_interval,</span></span><br><span class="line"><span class="params">          eval_step_interval, save_path, resume=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;此处的data_loader是map-style dataset&quot;&quot;&quot;</span></span><br><span class="line">    start_epoch = <span class="number">0</span></span><br><span class="line">    start_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> resume != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="comment"># 加载之前训练过的模型的参数文件</span></span><br><span class="line">        logging.warning(<span class="string">f&quot;loading from <span class="subst">&#123;resume&#125;</span>&quot;</span>)</span><br><span class="line">        checkpoint = torch.load(resume)</span><br><span class="line">        model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">        optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">        start_epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">        start_step = checkpoint[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch_index <span class="keyword">in</span> <span class="built_in">range</span>(start_epoch, num_epoch):</span><br><span class="line">        ema_loss = <span class="number">0.</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;https://www.investopedia.com/terms/e/ema.asp&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        num_bathes = <span class="built_in">len</span>(train_data_loader)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch_index, (target, token_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_loader):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            step = num_bathes * epoch_index + batch_index + <span class="number">1</span></span><br><span class="line">            logits = model(token_index)</span><br><span class="line">            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits),</span><br><span class="line">                                              F.one_hot(target, num_classes=<span class="number">2</span>).to(torch.float32))  <span class="comment"># 维度需要相同，把target转换</span></span><br><span class="line">            ema_loss = <span class="number">0.9</span> * ema_loss + <span class="number">0.1</span> * bce_loss  <span class="comment"># 指数平均loss</span></span><br><span class="line">            bce_loss.backward()  <span class="comment"># 梯度回传</span></span><br><span class="line">            nn.utils.clip_grad_norm(model.parameters(), <span class="number">0.1</span>)</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;https://blog.csdn.net/Mikeyboi/article/details/119522689&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % log_step_interval == <span class="number">0</span>:</span><br><span class="line">                logging.warning(</span><br><span class="line">                    <span class="string">f&quot;epoch_index:<span class="subst">&#123;epoch_index&#125;</span>,batch_index:<span class="subst">&#123;batch_index&#125;</span>,ema_loss:<span class="subst">&#123;ema_loss.item()&#125;</span>&quot;</span>)  <span class="comment"># 避免使用张量的形式打印，而是使用python格式，防止印象性能</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % save_step_interval == <span class="number">0</span>:</span><br><span class="line">                os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">                save_file = os.path.join(save_path, <span class="string">f&quot;step_<span class="subst">&#123;step&#125;</span>.pt&quot;</span>)</span><br><span class="line">                torch.save(&#123;</span><br><span class="line">                    <span class="string">&quot;epoch&quot;</span>: epoch_index,</span><br><span class="line">                    <span class="string">&quot;step&quot;</span>: step,</span><br><span class="line">                    <span class="string">&quot;model_state_dict&quot;</span>: model.state_dict(),</span><br><span class="line">                    <span class="string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: bce_loss</span><br><span class="line">                &#125;, save_file)</span><br><span class="line">                logging.warning(<span class="string">f&quot;checkpoint has been saved in <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % eval_step_interval == <span class="number">0</span>:</span><br><span class="line">                logging.warning(<span class="string">&quot;start to do evaluation...&quot;</span>)</span><br><span class="line">                model.<span class="built_in">eval</span>()  <span class="comment"># evaluation ,only forward calculation</span></span><br><span class="line">                eval_ema_loss = <span class="number">0</span></span><br><span class="line">                total_acc_account = <span class="number">0</span></span><br><span class="line">                total_account = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> eval_batch_index, (eval_target, eval_token_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_data_loader):</span><br><span class="line">                    total_account += eval_target.shape[<span class="number">0</span>]</span><br><span class="line">                    eval_logits = model(eval_token_index)</span><br><span class="line">                    total_acc_account += (torch.argmax(eval_logits, dim=-<span class="number">1</span>) == eval_target).<span class="built_in">sum</span>().item()</span><br><span class="line">                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(logits),</span><br><span class="line">                                                           F.one_hot(target, num_classes=<span class="number">2</span>).to(torch.float32))</span><br><span class="line">                    eval_ema_loss = <span class="number">0.9</span> * eval_ema_loss + <span class="number">0.1</span> * eval_bce_loss  <span class="comment"># 指数平均loss</span></span><br><span class="line">                    acc = total_acc_account / total_account  <span class="comment"># 精确度：一样的次数/总次数</span></span><br><span class="line">                logging.warning(</span><br><span class="line">                    <span class="string">f&quot;eval_ema_loss:<span class="subst">&#123;eval_ema_loss.item()&#125;</span>,eval_acc:<span class="subst">&#123;acc.item()&#125;</span>&quot;</span>)</span><br><span class="line">                model.train()</span><br></pre></td></tr></table></figure><h2 id="main">7 main</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = GCNN()</span><br><span class="line">    <span class="comment"># model = TextClassificationModel()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型总参数&quot;</span>, <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()))</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    train_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;train&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">    train_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(train_data_iter), batch_size=BATCH_SIZE,</span><br><span class="line">                                                    collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">    eval_data_iter = IMDB(root=<span class="string">&quot;.data&quot;</span>, split=<span class="string">&quot;test&quot;</span>)  <span class="comment"># Dataset类型的对象</span></span><br><span class="line">    eval_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(eval_data_iter), batch_size=<span class="number">8</span>,</span><br><span class="line">                                                   collate_fn=collate_fn) <span class="comment"># 变成map_style </span></span><br><span class="line">    resume = <span class="string">&#x27;F:\study\code\pytorch\logs_imdb_text_classification\step_500.pt&#x27;</span></span><br><span class="line">    train(train_data_loader, eval_data_loader, model, optimizer, num_epoch=<span class="number">10</span>, log_step_interval=<span class="number">20</span>,</span><br><span class="line">          save_step_interval=<span class="number">500</span>,</span><br><span class="line">          eval_step_interval=<span class="number">300</span>, save_path=<span class="string">&#x27;./logs_imdb_text_classification&#x27;</span>, resume=resume)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;项目介绍&quot;&gt;1 项目介绍&lt;/h2&gt;
&lt;p&gt;IMDB dataset having 50K movie reviews for natural language processing or Text analytics.&lt;/p&gt;
&lt;p&gt;This is a dat</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/categories/pytorch/project/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
    <category term="project" scheme="https://wangtongyouwen.github.io/tags/project/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门14-GRU</title>
    <link href="https://wangtongyouwen.github.io/post/f22b56ba.html"/>
    <id>https://wangtongyouwen.github.io/post/f22b56ba.html</id>
    <published>2023-04-14T09:47:29.000Z</published>
    <updated>2023-04-17T03:39:02.632Z</updated>
    
    <content type="html"><![CDATA[<p>torch.nn.GRU(<strong>args<em>, </em></strong>kwargs*)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU <span class="math display">\[\begin{aligned}r_{t} &amp; =\sigma\left(W_{i r} x_{t}+b_{i r}+W_{h r} h_{(t-1)}+b_{h r}\right) \\z_{t} &amp; =\sigma\left(W_{i z} x_{t}+b_{i z}+W_{h z} h_{(t-1)}+b_{h z}\right) \\n_{t} &amp; =\tanh \left(W_{i n} x_{t}+b_{i n}+r_{t} *\left(W_{h n} h_{(t-1)}+b_{h n}\right)\right) \\h_{t} &amp; =\left(1-z_{t}\right) * n_{t}+z_{t} * h_{(t-1)}\end{aligned}\]</span> 同等<code>hidden size</code>的参数量，GRU是LSTM的<span class="math inline">\(\frac{3}{4}\)</span></p><p>何时使用GRU，何时使用LSTM？</p><p>https://arxiv.org/pdf/1412.3555.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304141848732.png" alt="image-20230414180541344" /><figcaption aria-hidden="true">image-20230414180541344</figcaption></figure><h2 id="api">1 API</h2><h3 id="parameters">1.1 Parameters</h3><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional GRU. Default: <code>False</code></li></ul><h2 id="实现">2 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru_forward</span>(<span class="params"><span class="built_in">input</span>,initial_states,w_ih,w_hh,b_ih,b_hh</span>):</span><br><span class="line">    prev_h = initial_states</span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对权重扩维，复制成batch_size倍</span></span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    output = torch.zeros(bs,T,h_size) <span class="comment"># GRU网络的输出状态序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># t时刻的GRU cell的输入特征向量 [bs,i_size]</span></span><br><span class="line">        w_time_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment">#[bs,3*i_size,1]</span></span><br><span class="line">        w_time_x = w_time_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_time_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment">#[bs,3*i_size,1]</span></span><br><span class="line">        w_time_h_prev = w_time_h_prev.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        r_t = torch.sigmoid(w_time_x[:,:h_size] + w_time_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size]) <span class="comment"># 重置门</span></span><br><span class="line">        z_t = torch.sigmoid(w_time_x[:,h_size:<span class="number">2</span>*h_size] + w_time_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size]) <span class="comment"># 更新门</span></span><br><span class="line">        </span><br><span class="line">        n_t = torch.tanh(w_time_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size]+b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size]+</span><br><span class="line">                         r_t*(w_time_h_prev[:,<span class="number">2</span>*h_size: <span class="number">3</span>*h_size]+b_hh[<span class="number">2</span>*h_size: <span class="number">3</span>*h_size]))   <span class="comment"># 候选状态</span></span><br><span class="line">        </span><br><span class="line">        prev_h = (<span class="number">1</span>-z_t)*n_t + z_t*prev_h <span class="comment"># 增量更新得到当前时刻最新隐含状态</span></span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,prev_h</span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size)</span><br><span class="line">h0 = torch.randn(bs,h_size)</span><br><span class="line"></span><br><span class="line">gru_layer = nn.GRU(i_size,h_size,batch_first = <span class="literal">True</span>)</span><br><span class="line">output,h_final = gru_layer(<span class="built_in">input</span>,h0.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">output_custom,h_final_custom = gru_forward(<span class="built_in">input</span>,h0,gru_layer.weight_ih_l0,gru_layer.weight_hh_l0,gru_layer.bias_ih_l0,gru_layer.bias_hh_l0)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,h_final_custom))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(output,output_custom))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;torch.nn.GRU(&lt;strong&gt;args&lt;em&gt;, &lt;/em&gt;&lt;/strong&gt;kwargs*)&lt;/p&gt;
&lt;p&gt;https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#t</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门13-LSTM</title>
    <link href="https://wangtongyouwen.github.io/post/23f44fab.html"/>
    <id>https://wangtongyouwen.github.io/post/23f44fab.html</id>
    <published>2023-04-13T10:13:13.000Z</published>
    <updated>2023-04-13T12:26:22.728Z</updated>
    
    <content type="html"><![CDATA[<p>https://colah.github.io/posts/2015-08-Understanding-LSTMs/</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131816130.png" alt="image-20230413181558550" /><figcaption aria-hidden="true">image-20230413181558550</figcaption></figure><h2 id="整体介绍">1 整体介绍</h2><p><span class="math display">\[\begin{array}{l}i_{t}=\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \\f_{t}=\sigma\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \\g_{t}=\tanh \left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \\o_{t}=\sigma\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\right) \\c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot g_{t} \\h_{t}=o_{t} \odot \tanh \left(c_{t}\right)\end{array}\]</span></p><ul><li><p><span class="math inline">\(h_t\)</span>: hidden state at time <span class="math inline">\(t\)</span></p></li><li><p><span class="math inline">\(c_t\)</span>: cell state at time <span class="math inline">\(t\)</span></p></li><li><p><span class="math inline">\(x_t\)</span>: input at time <span class="math inline">\(t\)</span></p></li><li><p><span class="math inline">\(h_{t-1}\)</span>: hidden state of the layer at time <span class="math inline">\(t-1\)</span> or the initial hidden state at time o</p></li><li><p><span class="math inline">\(i_t\)</span>: input</p></li><li><p><span class="math inline">\(f_t\)</span>: forget</p></li><li><p><span class="math inline">\(g_t\)</span>: cell</p></li><li><p><span class="math inline">\(o_t\)</span>: output gates</p></li><li><p><span class="math inline">\(\sigma\)</span>: sigmoid function</p></li><li><p><span class="math inline">\(\odot\)</span>: Hadamard product</p></li><li><p><span class="math inline">\(N\)</span> = batch size</p></li><li><p><span class="math inline">\(L\)</span> = sequence length</p></li><li><p><span class="math inline">\(D\)</span> = 2 if bidirectional = True otherwise 1</p></li><li><p><span class="math inline">\(H_{in}\)</span> = input_size</p></li><li><p><span class="math inline">\(H_{cell}\)</span> = hidden_size</p></li><li><p><span class="math inline">\(H_{out}\)</span> = pro_size if pro_size &gt; 0 otherwise hidden_size</p></li></ul><h3 id="parameters">1.1 Parameters</h3><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional LSTM. Default: <code>False</code></li><li><strong>proj_size</strong> – If <code>&gt; 0</code>, will use LSTM with projections of corresponding size. Default: 0 减少LTSM的参数和计算量</li></ul><h3 id="inputs-input-h_0-c_0">1.2 Inputs: input, (h_0, c_0)</h3><ul><li>input：<ul><li><span class="math inline">\((L,H_{in})\)</span>-&gt; unbatched input</li><li><span class="math inline">\((L,N,H_{in})\)</span>-&gt;<code>batch_first=False</code></li><li><span class="math inline">\((N,L,H_{in})\)</span>-&gt;<code>batch_first=True</code></li></ul></li><li>h_0: Defaults to zeros if (h_0, c_0) is not provided<ul><li><span class="math inline">\((D*num\_layers,H_{out})\)</span> for unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{out})\)</span> containing the initial hidden state for each element in the input sequence.</li></ul></li><li>c_0: Defaults to zeros if (h_0, c_0) is not provided<ul><li><span class="math inline">\((D*num\_layers,H_{cell})\)</span> for unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{cell})\)</span> containing the initial cell state for each element in the input sequence.</li></ul></li></ul><h3 id="outputs-output-h_n-c_n">1.3 Outputs: output, (h_n, c_n)</h3><ul><li>output:<ul><li><span class="math inline">\((L,D*H_{out})\)</span>-&gt;unbatched input</li><li><span class="math inline">\((L,N,D*H_{out})\)</span>-&gt;<code>batch_first=False</code> containing the output features (h_t) from the last layer of the LSTM,for each t</li><li><span class="math inline">\((N,L,D*H_{out})\)</span>-&gt;<code>batch_first=True</code> containing the output features (h_t) from the last layer of the LSTM,for each t</li><li>If a <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> has been given as the input, the output will also be a packed sequence</li><li>When <code>bidirectional=True</code>, output will contain a concatenation of the forward and reverse hidden states at each time step in the sequence</li></ul></li><li>h_n:<ul><li><span class="math inline">\((D*num\_layers,H_{out})\)</span>-&gt; unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{out})\)</span>-&gt;containing the final hidden state for each element in the sequence.</li><li>When <code>bidirectional=True</code>, h_n will contain a concatenation of the final forward and reverse hidden states, respectively.</li></ul></li><li>c_n:<ul><li><span class="math inline">\((D*num\_layers,H_{cell})\)</span>-&gt; unbatched input</li><li><span class="math inline">\((D*num\_layers,N,H_{cell})\)</span>-&gt;containing the final hidden state for each element in the sequence.</li><li>When <code>bidirectional=True</code>, h_n will contain a concatenation of the final forward and reverse hidden states, respectively.</li></ul></li></ul><h3 id="variables">1.4 Variables</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131856525.png" alt="image-20230413185629132" /><figcaption aria-hidden="true">image-20230413185629132</figcaption></figure><h2 id="调用api">2 调用API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line"><span class="comment"># proj_size = </span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size) <span class="comment"># 输入序列</span></span><br><span class="line">c0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line">h0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用官方LSTM API</span></span><br><span class="line">lstm_layer = nn.LSTM(i_size,h_size,batch_first=<span class="literal">True</span>)</span><br><span class="line">output,(h_final,c_final) = lstm_layer(<span class="built_in">input</span>,(h0.unsqueeze(<span class="number">0</span>),c0.unsqueeze(<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k,v.shape)</span><br></pre></td></tr></table></figure><h2 id="lstm-without-proj_size">3 LSTM without proj_size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自己写一个LSTM模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_forward</span>(<span class="params"><span class="built_in">input</span>,inittal_states,w_ih,w_hh,b_ih,b_hh</span>):</span><br><span class="line">    h0,c0 = inittal_states <span class="comment"># 初始状态</span></span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = c0</span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,i_size)</span></span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,h_size)</span></span><br><span class="line">    </span><br><span class="line">    output_size = h_size</span><br><span class="line">    output = torch.zeros(bs,T,output_size) <span class="comment"># 输出序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># 当前时刻的输入向量 (bs,i_size)</span></span><br><span class="line">        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_x = w_times_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_h_prev = w_times_h_prev.squeeze(-<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分別计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)</span></span><br><span class="line">        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])</span><br><span class="line">        f_t = torch.sigmoid(w_times_x[:,h_size:<span class="number">2</span>*h_size] + w_times_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size])</span><br><span class="line">        g_t = torch.tanh(w_times_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + w_times_h_prev[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_hh[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size])</span><br><span class="line">        o_t = torch.sigmoid(w_times_x[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + w_times_h_prev[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_ih[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_hh[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size])</span><br><span class="line"></span><br><span class="line">        prev_c = f_t * prev_c + i_t * g_t</span><br><span class="line">        prev_h = o_t * torch.tanh(prev_c)</span><br><span class="line">        </span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,(prev_h,prev_c)</span><br><span class="line">    </span><br><span class="line">custom_output,(custom_h_final,custom_c_final) = lstm_forward(<span class="built_in">input</span>,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)    </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_output,output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,custom_h_final))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(c_final,custom_c_final))</span><br></pre></td></tr></table></figure><h2 id="lstm-with-proj_size">4 LSTM with proj_size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">bs,T,i_size,h_size = <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">proj_size = <span class="number">3</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs,T,i_size) <span class="comment"># 输入序列</span></span><br><span class="line">c0 = torch.randn(bs,h_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line">h0 = torch.randn(bs,proj_size) <span class="comment"># 初始值，不需要训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用官方LSTM API</span></span><br><span class="line">lstm_layer = nn.LSTM(i_size,h_size,batch_first=<span class="literal">True</span>,proj_size=proj_size)</span><br><span class="line">output,(h_final,c_final) = lstm_layer(<span class="built_in">input</span>,(h0.unsqueeze(<span class="number">0</span>),c0.unsqueeze(<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k,v)</span><br></pre></td></tr></table></figure><p>eight_ih_l0 torch.Size([20, 4]) h_size<em>4,i_size weight_hh_l0 torch.Size([20, 3]) h_size</em>4,proj_size bias_ih_l0 torch.Size([20]) h_size<em>4 bias_hh_l0 torch.Size([20]) h_size</em>4 weight_hr_l0 torch.Size([3, 5]) proj_size,h_size 只会对 h_state 进行改变，不会对 c_state 进行改变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_forward</span>(<span class="params"><span class="built_in">input</span>,inittal_states,w_ih,w_hh,b_ih,b_hh,w_hr=<span class="literal">None</span></span>):</span><br><span class="line">    h0,c0 = inittal_states <span class="comment"># 初始状态</span></span><br><span class="line">    bs,T,i_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_size = w_ih.shape[<span class="number">0</span>] // <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = c0</span><br><span class="line">    batch_w_ih = w_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,i_size)</span></span><br><span class="line">    batch_w_hh = w_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,4*h_size,h_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> w_hr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_size = w_hr.shape[<span class="number">0</span>]</span><br><span class="line">        output_size = p_size</span><br><span class="line">        batch_w_hr = w_hr.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (bs,proj_size,h_size)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output_size = h_size</span><br><span class="line">    output = torch.zeros(bs,T,output_size) <span class="comment"># 输出序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:] <span class="comment"># 当前时刻的输入向量 (bs,i_size)</span></span><br><span class="line">        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_x = w_times_x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># [bs,4*h_size,1]</span></span><br><span class="line">        w_times_h_prev = w_times_h_prev.squeeze(-<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分別计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)</span></span><br><span class="line">        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])</span><br><span class="line">        f_t = torch.sigmoid(w_times_x[:,h_size:<span class="number">2</span>*h_size] + w_times_h_prev[:,h_size:<span class="number">2</span>*h_size] + b_ih[h_size:<span class="number">2</span>*h_size] + b_hh[h_size:<span class="number">2</span>*h_size])</span><br><span class="line">        g_t = torch.tanh(w_times_x[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + w_times_h_prev[:,<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_ih[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size] + b_hh[<span class="number">2</span>*h_size:<span class="number">3</span>*h_size])</span><br><span class="line">        o_t = torch.sigmoid(w_times_x[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + w_times_h_prev[:,<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_ih[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size] + b_hh[<span class="number">3</span>*h_size:<span class="number">4</span>*h_size])</span><br><span class="line"></span><br><span class="line">        prev_c = f_t * prev_c + i_t * g_t</span><br><span class="line">        prev_h = o_t * torch.tanh(prev_c)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> w_hr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 做projection</span></span><br><span class="line">            prev_h = torch.bmm(batch_w_hr,prev_h.unsqueeze(-<span class="number">1</span>)) <span class="comment"># bs,proj_size,1</span></span><br><span class="line">            prev_h = prev_h.squeeze(-<span class="number">1</span>) <span class="comment"># bs,proj_size</span></span><br><span class="line">            </span><br><span class="line">        output[:,t,:] = prev_h</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> output,(prev_h,prev_c)</span><br><span class="line">    </span><br><span class="line">custom_output,(custom_h_final,custom_c_final) = lstm_forward(<span class="built_in">input</span>,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,</span><br><span class="line">                                                             lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0,lstm_layer.weight_hr_l0)    </span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_output,output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(h_final,custom_h_final))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(c_final,custom_c_final))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门12-RNN</title>
    <link href="https://wangtongyouwen.github.io/post/b945630.html"/>
    <id>https://wangtongyouwen.github.io/post/b945630.html</id>
    <published>2023-04-13T06:29:13.000Z</published>
    <updated>2023-04-13T09:31:09.288Z</updated>
    
    <content type="html"><![CDATA[<p>https://www.cs.toronto.edu/~graves/preprint.pdf</p><p>Supervised Sequence Labelling with Recurrent Neural Networks</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131437010.png" alt="image-20230413143700100" style="zoom:67%;" /></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131437818.png" alt="image-20230413143710020" style="zoom:67%;" /></p><p>delay: 能看到前几帧的信息，牺牲一定的时间，预测的更加准确。</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131448069.png" alt="image-20230413144847329" /><figcaption aria-hidden="true">image-20230413144847329</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131502377.png" alt="image-20230413150234647" /><figcaption aria-hidden="true">image-20230413150234647</figcaption></figure><h2 id="api">1 API</h2><p>torch.nn.RNN(<strong>args<em>, </em></strong>kwargs*)</p><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN</p><p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.</p><p>For each element in the input sequence, each layer computes the following function: <span class="math display">\[h_t = tanh(x_tW_{ih}^T+b_{ih}+h_{t-1}W_{hh}^T+b_{hh})\]</span> where <span class="math inline">\(h_t\)</span> is the hidden state at time t, <span class="math inline">\(x_t\)</span> is the input at time t, and <span class="math inline">\(h_{t-1}\)</span> is the hidden state of the previous layer at time t-1 or the inital hidden state at time 0. If nonlinearity is "relu", then ReLU is used instead of tanh.</p><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</li><li><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code>'tanh'</code> or <code>'relu'</code>. Default: <code>'tanh'</code></li><li><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></li><li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: <code>False</code></li><li><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li><li><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional RNN. Default: <code>False</code></li></ul><p>Inputs: input, h_0</p><ul><li><p>inputs: tensor of shape <span class="math inline">\((L,H_{in})\)</span> for unbatched input, <span class="math inline">\((L,N,H_{in})\)</span> when <code>batch_first=False</code> or <span class="math inline">\((N,L,H_{in})\)</span> when <code>batch_first=True</code></p></li><li><p>h_0: tensor of shape <span class="math inline">\((D*num\_layer,H_{out})\)</span> for unbatched input or$ (D*num_layers,N,H_{out})$containing the initial hidden state for the input sequence batch. Defaults to zeros if not provided.</p></li></ul><p><span class="math display">\[N=batch\ size \\L = sequence\ length \\D = 2\ if\ bidirectional = True\ otherwise\ 1 \\H_{in} = input \_size \\H_{out} = hidden\_size\]</span></p><p>Outputs: output, h_n</p><h2 id="代码实现">2 代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 单向，单层RNN</span></span><br><span class="line">single_rnn = nn.RNN(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,batch_first=<span class="literal">True</span>) <span class="comment"># feature_size * hidden_size * layer_num</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>) <span class="comment"># batch_size*sequence_length*feature_size</span></span><br><span class="line">output,h_n = single_rnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output) <span class="comment"># batch_size*sequence_length*hidden_size</span></span><br><span class="line"><span class="built_in">print</span>(h_n) <span class="comment"># layer_num*batch_size*hidden_size</span></span><br><span class="line"><span class="comment"># 双向，单层RNN</span></span><br><span class="line">bidirectional_rnn = nn.RNN(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,batch_first=<span class="literal">True</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line">bi_output,bi_h_n = bidirectional_rnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(bi_output) <span class="comment"># batch_size*sequence_length*(2*hidden_size)</span></span><br><span class="line"><span class="built_in">print</span>(bi_h_n) <span class="comment"># (2*layer_num)*batch_size*hidden_size</span></span><br></pre></td></tr></table></figure><h2 id="实现单向单层rnn">3 实现单向单层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step2: 手写一个rnn_forward函数,实现单向rnn的计算原理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_forward</span>(<span class="params"><span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev</span>):</span><br><span class="line">    bs,T,input_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_dim = weight_ih.shape[<span class="number">0</span>]</span><br><span class="line">    h_out = torch.zeros(bs,T,h_dim) <span class="comment"># 初始化一个输出(状态)矩阵</span></span><br><span class="line">    <span class="comment"># h_prev: bs*hidden_size</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = <span class="built_in">input</span>[:,t,:].unsqueeze(<span class="number">2</span>) <span class="comment">#获取当前时刻输入 # bs*input_size*1</span></span><br><span class="line">        w_ih_batch = weight_ih.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># bs*h_dim*input_size</span></span><br><span class="line">        w_hh_batch = weight_hh.unsqueeze(<span class="number">0</span>).tile(bs,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># bs*h_dim*h_dim</span></span><br><span class="line">        </span><br><span class="line">        w_time_x = torch.bmm(w_ih_batch,x).squeeze(-<span class="number">1</span>) <span class="comment"># bs*h_dim</span></span><br><span class="line">        w_time_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(<span class="number">2</span>)).squeeze(-<span class="number">1</span>) <span class="comment">#bs*h_dim</span></span><br><span class="line">        h_prev = torch.tanh(w_time_x + bias_ih + w_time_h + bias_hh) <span class="comment"># t时刻的输出</span></span><br><span class="line">        </span><br><span class="line">        h_out[:,t,:] = h_prev</span><br><span class="line">    <span class="keyword">return</span> h_out,h_prev.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 验证一下rnn_forward的正确性</span></span><br><span class="line"><span class="comment"># for k,v in rnn.named_parameters():</span></span><br><span class="line"><span class="comment">#     print(k,v)</span></span><br><span class="line">custom_rnn_output,custom_state_final = run_forward(<span class="built_in">input</span>,rnn.weight_ih_l0,rnn.weight_hh_l0,rnn.bias_ih_l0,rnn.bias_hh_l0,h_prev)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_rnn_output,rnn_output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(custom_state_final,state_final))</span><br></pre></td></tr></table></figure><h2 id="实现双向单层rnn">4 实现双向单层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3: 手写一个bidirectional_rnn_forward函数，实现双向RNN的计算原理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bidirectional_run_forward</span>(<span class="params"><span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev,</span></span><br><span class="line"><span class="params">                              weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse</span>):</span><br><span class="line">    bs,T,input_size = <span class="built_in">input</span>.shape</span><br><span class="line">    h_dim = weight_ih.shape[<span class="number">0</span>]</span><br><span class="line">    h_out = torch.zeros(bs,T,h_dim*<span class="number">2</span>) <span class="comment"># 初始化一个输出(状态)矩阵</span></span><br><span class="line">    </span><br><span class="line">    forward_output = run_forward(<span class="built_in">input</span>,weight_ih,weight_hh,bias_ih,bias_hh,h_prev)[<span class="number">0</span>] <span class="comment"># forward layer</span></span><br><span class="line">    backward_output = run_forward(torch.flip(<span class="built_in">input</span>,[<span class="number">1</span>]),</span><br><span class="line">                      weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse)[<span class="number">0</span>]  <span class="comment"># flip 对dim进行翻转, backward layer</span></span><br><span class="line">    </span><br><span class="line">    h_out[:,:,:h_dim] = forward_output</span><br><span class="line">    h_out[:,:,h_dim:] = torch.flip(backward_output,[<span class="number">1</span>]) <span class="comment"># 反向的结果需要在T维度上再次反向，才能与api结果相同</span></span><br><span class="line">    </span><br><span class="line">    h_n = torch.zeros(<span class="number">2</span>,bs,h_dim)</span><br><span class="line">    h_n[<span class="number">0</span>,:,:] = forward_output[:,-<span class="number">1</span>,:]</span><br><span class="line">    h_n[<span class="number">1</span>,:,:] = backward_output[:,-<span class="number">1</span>,:]</span><br><span class="line">        <span class="comment"># h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)</span></span><br><span class="line">    <span class="keyword">return</span> h_out, h_n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证一下 bidirectional_rnn_forward正确性</span></span><br><span class="line">bi_rnn = nn.RNN(input_size,hidden_size,batch_first=<span class="literal">True</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line">h_prev =torch.zeros(<span class="number">2</span>,bs,hidden_size)</span><br><span class="line">bi_rnn_output,bi_state_final = bi_rnn(<span class="built_in">input</span>,h_prev)</span><br><span class="line"></span><br><span class="line">custom_bi_rnn_output,custom_bi_state_final = bidirectional_run_forward(<span class="built_in">input</span>,</span><br><span class="line">                                                                       bi_rnn.weight_ih_l0,</span><br><span class="line">                                                                       bi_rnn.weight_hh_l0,</span><br><span class="line">                                                                       bi_rnn.bias_ih_l0,</span><br><span class="line">                                                                       bi_rnn.bias_hh_l0,</span><br><span class="line">                                                                       h_prev[<span class="number">0</span>],</span><br><span class="line">                                                                       bi_rnn.weight_ih_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.weight_hh_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.bias_ih_l0_reverse,</span><br><span class="line">                                                                       bi_rnn.bias_hh_l0_reverse,</span><br><span class="line">                                                                       h_prev[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(bi_rnn_output,custom_bi_rnn_output))</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(bi_state_final,custom_bi_state_final))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://www.cs.toronto.edu/~graves/preprint.pdf&lt;/p&gt;
&lt;p&gt;Supervised Sequence Labelling with Recurrent Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;ht</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识11-ViT</title>
    <link href="https://wangtongyouwen.github.io/post/1229d3b9.html"/>
    <id>https://wangtongyouwen.github.io/post/1229d3b9.html</id>
    <published>2023-04-12T15:59:34.000Z</published>
    <updated>2023-04-13T06:33:31.369Z</updated>
    
    <content type="html"><![CDATA[<p>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p><p>https://arxiv.org/pdf/2010.11929.pdf</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131331748.png" alt="image-20230413132929498"  /></p><p>We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017).</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304131433347.png" alt="image-20230413143320971" /><figcaption aria-hidden="true">image-20230413143320971</figcaption></figure><h2 id="image2emb">1 image2emb</h2><h3 id="分块">1.1 分块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_naive</span>(<span class="params">image,patch_size,weight</span>):</span><br><span class="line">    <span class="comment"># image size: bs*channel*h*w</span></span><br><span class="line">    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-<span class="number">1</span>,-<span class="number">2</span>) <span class="comment"># bs*patch_num*patch_pixel_num</span></span><br><span class="line">    patch_embedding = patch @ weight</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line">        </span><br><span class="line"><span class="comment"># test code for image2emb</span></span><br><span class="line">bs,ic,image_h,image_w = <span class="number">1</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">8</span></span><br><span class="line">patch_size = <span class="number">4</span></span><br><span class="line">model_dim = <span class="number">8</span> </span><br><span class="line">patch_depth = patch_size*patch_size*ic <span class="comment"># 每个patch中的元素个数</span></span><br><span class="line">image = torch.randn(bs,ic,image_h,image_w)</span><br><span class="line">weight = torch.randn(patch_depth,model_dim) <span class="comment"># model_dim 是输出通道数目，patch_depth是卷积核的面积乘以输入通道数</span></span><br><span class="line">patch_embedding_naive = image2emb_naive(image,patch_size,weight) <span class="comment"># 分块得到embedding</span></span><br><span class="line"><span class="built_in">print</span>(patch_embedding_naive.shape)</span><br></pre></td></tr></table></figure><h3 id="conv2d">1.2 conv2d</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">image2emb_conv</span>(<span class="params">image,kernel,stride</span>):</span><br><span class="line">    conv_output = F.conv2d(image,kernel,stride=stride) <span class="comment"># bs*oc*oh*ow</span></span><br><span class="line">    bs,oc,oh,ow = conv_output.shape</span><br><span class="line">    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-<span class="number">1</span>,-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> patch_embedding</span><br><span class="line"></span><br><span class="line">kernel = weight.transpose(<span class="number">0</span>,<span class="number">1</span>).reshape((-<span class="number">1</span>,ic,patch_size,patch_size)) <span class="comment"># oc*ic*kh*kw</span></span><br><span class="line">patch_embedding_conv = image2emb_conv(image,kernel,stride=patch_size) <span class="comment"># 二维卷积的方法得到embedding</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(patch_embedding_conv,patch_embedding_naive))</span><br></pre></td></tr></table></figure><h2 id="classification-token">2 classification token</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cls_token_embedding = torch.randn(bs,<span class="number">1</span>,model_dim,requires_grad=<span class="literal">True</span>)</span><br><span class="line">token_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="add-position-embedding">3 add position embedding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">position_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=<span class="literal">True</span>)</span><br><span class="line">seq_len = token_embedding.shape[<span class="number">1</span>]</span><br><span class="line">position_embedding = torch.tile(position_embedding_table[:seq_len],[token_embedding.shape[<span class="number">0</span>],<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">token_embedding += position_embedding</span><br></pre></td></tr></table></figure><h2 id="pass-embedding-to-transformer-encoder">4 pass embedding to Transformer Encoder</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">encoder_output = transformer_encoder(token_embedding)</span><br></pre></td></tr></table></figure><h2 id="do-classification">5 do classification</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cls_token_output = encoder_output[:,<span class="number">0</span>,:]</span><br><span class="line">liner_layer = nn.Linear(model_dim,num_classes)</span><br><span class="line">logits = liner_layer(cls_token_output)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss = loss_fn(logits,label)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2010.11929.pdf&lt;/p&gt;
&lt;p&gt;&lt;img src=</summary>
      
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识10-卷积网络</title>
    <link href="https://wangtongyouwen.github.io/post/b0d26e2a.html"/>
    <id>https://wangtongyouwen.github.io/post/b0d26e2a.html</id>
    <published>2023-04-10T09:16:59.000Z</published>
    <updated>2023-04-12T15:56:33.111Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101930110.png" alt="image-20230410193006020" style="zoom:80%;" /></p><h2 id="卷积的api">1 卷积的API</h2><h3 id="conv2d">1.1 CONV2D</h3><p>torch.nn.Conv2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode='zeros'</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101932268.png" alt="image-20230410193201892" style="zoom:67%;" /></p><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels in the input image</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – Size of the convolving kernel</li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – Padding added to all four sides of the input. Default: 0</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></li></ul><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304111953529.png" alt="image-20230411195325050" /><figcaption aria-hidden="true">image-20230411195325050</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">in_channels = <span class="number">1</span></span><br><span class="line">out_channels = <span class="number">1</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">bias = <span class="literal">False</span></span><br><span class="line">input_size = [in_channels,<span class="number">4</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">conv_layer = nn.Conv2d(in_channels,out_channels,kernel_size,bias=bias)</span><br><span class="line">input_feature_map = torch.randn(input_size)</span><br><span class="line">output_feature_map = conv_layer(input_feature_map) <span class="comment"># 直接调用卷积这个方法</span></span><br><span class="line"><span class="built_in">print</span>(input_feature_map,<span class="string">&#x27;\n&#x27;</span>,output_feature_map)</span><br><span class="line"><span class="built_in">print</span>(conv_layer.weight) <span class="comment"># 1*1*3*3 out_channels*in_channels*height*width</span></span><br></pre></td></tr></table></figure><h3 id="functional.conv2d">1.2 FUNCTIONAL.CONV2D</h3><p>torch.nn.functional.conv2d(<em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>) → <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112015956.png" alt="image-20230411201534888" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_feature_map1 = F.conv2d(input_feature_map,conv_layer.weight) <span class="comment"># functional,需要传入卷积的weight</span></span><br><span class="line"><span class="built_in">print</span>(output_feature_map)</span><br><span class="line"><span class="built_in">print</span>(output_feature_map1)</span><br></pre></td></tr></table></figure><h2 id="padding-and-stride">2 padding and stride</h2><p>https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html</p><h3 id="padding">2.1 padding</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112028509.png" alt="image-20230411202806742" /><figcaption aria-hidden="true">image-20230411202806742</figcaption></figure><p>In general, if we add a total of <span class="math inline">\(p_h\)</span> rows of padding (roughly half on top and half on bottom) and a total of <span class="math inline">\(p_w\)</span> columns of padding (roughly half on the left and half on the right), the output shape will be <span class="math display">\[(n_k - k_h + p_h + 1)\times (n_w - k_w + p_w + 1)\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We define a helper function to calculate convolutions. It initializes the</span></span><br><span class="line"><span class="comment"># convolutional layer weights and performs corresponding dimensionality</span></span><br><span class="line"><span class="comment"># elevations and reductions on the input and output</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># (1, 1) indicates that batch size and the number of channels are both 1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># Strip the first two dimensions: examples and channels</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 row and column is padded on either side, so a total of 2 rows or columns</span></span><br><span class="line"><span class="comment"># are added</span></span><br><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We use a convolution kernel with height 5 and width 3. The padding on either</span></span><br><span class="line"><span class="comment"># side of the height and width are 2 and 1, respectively</span></span><br><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><h3 id="stride">2.2 stride</h3><p>In general, when the stride for the height is <span class="math inline">\(s_h\)</span> and the stride for the width is <span class="math inline">\(s_w\)</span>, the output shape is <span class="math display">\[[(n_h-k_h+p_h+s_h)/s_h] \times [((n_w-k_w+p_w+s_w)/s_w)]\]</span> f we set<span class="math inline">\(p_h =k_h -1\)</span> and <span class="math inline">\(p_w = k_w -1\)</span>, then the output shape can be simplified to <span class="math inline">\([(n_h+s_h-1)/s_h\times (n_w+s_w-1)/s_w]\)</span>. Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be <span class="math inline">\((n_h/s_h)\times (n_w/s_w)\)</span></p><p>note: the [] means floor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.LazyConv2d(<span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><h3 id="demo">2.3 demo</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># With square kernels and equal stride</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding and dilation</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>), dilation=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><h2 id="multiple-input-and-multiple-output-channels">3 Multiple Input and Multiple Output Channels</h2><h3 id="multiple-input-channels">3.1 Multiple Input Channels</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304112057860.png" alt="image-20230411205730182" /><figcaption aria-hidden="true">image-20230411205730182</figcaption></figure><h3 id="multiple-output-channels">3.2 Multiple Output Channels</h3><p>we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater <em>channel depth</em>. Intuitively, you could think of each channel as responding to a different set of features.</p><p>把每个输出通道都看做一个单独的操作，最后stack起来得到结果</p><h2 id="矩阵运算实现卷积操作">4 矩阵运算实现卷积操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 卷积的输入特征图</span></span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># 卷积核</span></span><br><span class="line">bias = torch.randn(<span class="number">1</span>) <span class="comment"># 卷积偏置，默认输出通道数目等于1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 用原始的矩阵运算来实现二维卷积,先不考虑 batchsize 维度和 channel 维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d</span>(<span class="params"><span class="built_in">input</span>,kernel,bias = <span class="number">0</span>,stride = <span class="number">1</span>,padding = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding))</span><br><span class="line">        </span><br><span class="line">    input_h,input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros((output_h,output_w))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">            region = <span class="built_in">input</span>[i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">            output[<span class="built_in">int</span>(i/stride)][<span class="built_in">int</span>(j/stride)] = torch.<span class="built_in">sum</span>(region * kernel) + bias <span class="comment"># 点乘，并赋值给输出位置的元素</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">mat_mul_conv_output = matrix_multiplication_for_conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(mat_mul_conv_output)</span><br><span class="line">pytorch_api_conv_output = F.conv2d(<span class="built_in">input</span>.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="built_in">input</span>.shape[<span class="number">0</span>],<span class="built_in">input</span>.shape[<span class="number">1</span>]),kernel.reshape(<span class="number">1</span>,<span class="number">1</span>,kernel.shape[<span class="number">0</span>],kernel.shape[<span class="number">1</span>]),bias=bias,padding=<span class="number">1</span>).squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(pytorch_api_conv_output)</span><br></pre></td></tr></table></figure><h2 id="向量内积实现卷积操作">5 向量内积实现卷积操作</h2><h3 id="flatten">5.1 flatten</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step2 用原始的矩阵运算来实现二维卷积,先不考虑 batchsize 维度和 channel 维度, flatten 版本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_flatten</span>(<span class="params"><span class="built_in">input</span>,kernel,bias = <span class="number">0</span>,stride = <span class="number">1</span>,padding = <span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding))</span><br><span class="line">        </span><br><span class="line">    input_h,input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros((output_h,output_w))</span><br><span class="line">    region_matrix = torch.zeros(output.numel(),kernel.numel()) <span class="comment"># 存储所有的拉平后的特征区域</span></span><br><span class="line">    kernel_matrix = kernel.reshape((kernel.numel(),<span class="number">1</span>)) <span class="comment"># 变为列向量,kernel的列向量形式</span></span><br><span class="line">    row_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">            region = <span class="built_in">input</span>[i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">            region_vetor = torch.flatten(region) </span><br><span class="line">            region_matrix[row_index] = region_vetor</span><br><span class="line">            row_index += <span class="number">1</span></span><br><span class="line">    output_matrix = region_matrix @ kernel_matrix</span><br><span class="line">    output = output_matrix.reshape((output_h,output_w)) + bias</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># flatten input</span></span><br><span class="line">pytorch_api_conv_output = F.conv2d(<span class="built_in">input</span>.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="built_in">input</span>.shape[<span class="number">0</span>],<span class="built_in">input</span>.shape[<span class="number">1</span>]),</span><br><span class="line">                                   kernel.reshape(<span class="number">1</span>,<span class="number">1</span>,kernel.shape[<span class="number">0</span>],kernel.shape[<span class="number">1</span>]),</span><br><span class="line">                                   bias=bias,padding=<span class="number">1</span>).squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line">mat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_api_conv_output,mat_mul_conv_output_flatten))</span><br></pre></td></tr></table></figure><p>这里可以使用torch.unfold实现flatten操作</p><p>torch.nn.Unfold(<em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html</p><h3 id="考虑batchsize维度和channel维度">5.2 考虑batchsize维度和channel维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 用原始的矩阵运算来实现二维卷积，考虑batchsize维度和channel维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_full</span>(<span class="params"><span class="built_in">input</span>,kernel,bias=<span class="number">0</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span></span>):</span><br><span class="line">    <span class="comment"># input和kernel 都是4维张量 </span></span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)) <span class="comment"># w,h,input_channel,batchsize</span></span><br><span class="line">        </span><br><span class="line">    bs,in_channel,input_h,input_w = <span class="built_in">input</span>.shape <span class="comment"># batchsize,in_channel,input_h,input_w</span></span><br><span class="line">    out_channel,in_channel,kernel_h,kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        bias = torch.zeros(out_channel)</span><br><span class="line">    </span><br><span class="line">    output_h = math.floor((input_h - kernel_h)/stride) + <span class="number">1</span> <span class="comment"># 输出高度</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w)/stride) + <span class="number">1</span> <span class="comment"># 输出宽度</span></span><br><span class="line">    output = torch.zeros(bs,out_channel,output_h,output_w)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(bs):</span><br><span class="line">        <span class="keyword">for</span> oc <span class="keyword">in</span> <span class="built_in">range</span>(out_channel):</span><br><span class="line">            <span class="keyword">for</span> ic <span class="keyword">in</span> <span class="built_in">range</span>(in_channel):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h - kernel_h + <span class="number">1</span>,stride):  <span class="comment"># 对高度遍历</span></span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w - kernel_w + <span class="number">1</span>,stride): <span class="comment"># 对宽度遍历</span></span><br><span class="line">                        region = <span class="built_in">input</span>[ind,ic,i:i+kernel_h,j:j+kernel_w] <span class="comment"># 取出被核滑动到的区域</span></span><br><span class="line">                        output[ind,oc,<span class="built_in">int</span>(i/stride),<span class="built_in">int</span>(j/stride)] += torch.<span class="built_in">sum</span>(region * kernel[oc,ic]) <span class="comment"># 点乘，并赋值给输出位置的元素</span></span><br><span class="line">            output[ind,oc] += bias[oc]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 卷积的输入特征图(batchsize,in_channel,in_h,in_w)</span></span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># 卷积核(out_channel,in_channel,kernel_h,kernel_w)</span></span><br><span class="line">bias = torch.randn(<span class="number">3</span>) <span class="comment"># 卷积偏置，默认输出通道数目等于1</span></span><br><span class="line">pytorch_conv2d_api_output = F.conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line">mm_conv2d_full_output = matrix_multiplication_for_conv2d_full(<span class="built_in">input</span>,kernel,bias=bias,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_conv2d_api_output,mm_conv2d_full_output))</span><br></pre></td></tr></table></figure><h2 id="转置卷积">6 转置卷积</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积</span></span><br><span class="line"><span class="comment"># 把input 和 kernel 都resize 列向量(把kernel空缺的位置用0填充) 不考虑padding,假设stride=1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_kernel_matrix</span>(<span class="params">kernel,input_size,stride=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠后的矩阵&quot;&quot;&quot;</span></span><br><span class="line">    kernel_h, kernel_w = kernel.shape</span><br><span class="line">    input_h,input_w = input_size</span><br><span class="line">    num_out_feature_map = (math.floor((input_h - kernel_h)/stride) + <span class="number">1</span>) * (math.floor((input_w - kernel_w)/stride) + <span class="number">1</span>)</span><br><span class="line">    result = torch.zeros((num_out_feature_map,input_h*input_w)) <span class="comment"># 初始化结果矩阵，输出特征图元素个数*输入特征图元素个数</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h-kernel_h+<span class="number">1</span>,stride):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w-kernel_w+<span class="number">1</span>,stride):</span><br><span class="line">            padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j),) <span class="comment"># 上下左右</span></span><br><span class="line">            result[count] = padded_kernel.flatten()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">kernel = torch.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">kernel_matrix = get_kernel_matrix(kernel,<span class="built_in">input</span>.shape) <span class="comment"># 4*16</span></span><br><span class="line"><span class="built_in">print</span>(kernel)</span><br><span class="line"><span class="built_in">print</span>(kernel_matrix)</span><br></pre></td></tr></table></figure><h3 id="验证二维卷积">6.1 验证二维卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试1：验证二维卷积</span></span><br><span class="line">pytorch_conv2d_output = F.conv2d(<span class="built_in">input</span>.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),kernel.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)) <span class="comment"># output 2*2</span></span><br><span class="line"><span class="comment"># 因为计算得到的 mm_conv2d_output 是列向量，所以需要reshape为大小一致的矩阵，再进行比较</span></span><br><span class="line">mm_conv2d_output = (kernel_matrix @ <span class="built_in">input</span>.reshape((-<span class="number">1</span>,<span class="number">1</span>))).reshape(pytorch_conv2d_output.shape).transpose(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 通过矩阵乘积来计算卷积</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(mm_conv2d_output,pytorch_conv2d_output))</span><br></pre></td></tr></table></figure><h3 id="验证二维转置卷积">6.2 验证二维转置卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试2：验证二维转置卷积</span></span><br><span class="line"><span class="comment"># 2*2 -&gt; 4*4 一般用于上采样过程 output 的 feature map 恢复到输入的 feature map 的 size</span></span><br><span class="line"><span class="comment"># 卷积的梯度,后项传播实现 √</span></span><br><span class="line"><span class="comment"># 使用填充的方式实现 ×</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>torch.nn.ConvTranspose2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em>, <em>padding_mode='zeros'</em>, <em>device=None</em>, <em>dtype=None</em>)</p><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d</p><p>torch.nn.functional.conv_transpose2d(<em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em>)</p><p>https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html</p><h2 id="group">7 group</h2><ul><li><strong>groups</strong> (int,optional) – Number of blocked connections from input channels to output channels. Default: 1</li></ul><h2 id="dilation">8 dilation</h2><ul><li><strong>dilation</strong> (int or tuple,optional) – Spacing between kernel elements. Default: 1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">7</span>,<span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">3</span>])     <span class="comment"># dilation=1</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">5</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># dilation=2</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">7</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">7</span>:<span class="number">3</span>]) <span class="comment"># dilation=3</span></span><br></pre></td></tr></table></figure><h2 id="最终版本">9 最终版本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiplication_for_conv2d_final</span>(<span class="params"><span class="built_in">input</span>,kernel,bias=<span class="literal">None</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>,dilation=<span class="number">1</span>,groups=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>,(padding,padding,padding,padding,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    bs, in_channel, input_h, input_w = <span class="built_in">input</span>.shape</span><br><span class="line">    out_channel,_, kernel_h, kernel_w = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> out_channel % groups == <span class="number">0</span> <span class="keyword">and</span> in_channel % groups == <span class="number">0</span>, <span class="string">&quot;group必须要同时被输入通道数和输出通道数整除！&quot;</span></span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.reshape((bs, groups, in_channel//groups, input_h, input_w))</span><br><span class="line">    kernel = kernel.reshape((groups, out_channel//groups, in_channel//groups, kernel_h, kernel_w))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># dilation (往原来的kernel中加入空洞)</span></span><br><span class="line">    kernel_h = (kernel_h - <span class="number">1</span>) * (dilation - <span class="number">1</span>) + kernel_h</span><br><span class="line">    kernel_w = (kernel_w - <span class="number">1</span>) * (dilation - <span class="number">1</span>) + kernel_w    </span><br><span class="line">    <span class="comment"># 输出结果的 feature map</span></span><br><span class="line">    output_h = math.floor((input_h - kernel_h) / stride) + <span class="number">1</span></span><br><span class="line">    output_w = math.floor((input_w - kernel_w) / stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    output_shape = (bs,groups,out_channel//groups,output_h,output_w)</span><br><span class="line">    output = torch.zeros(output_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        bias = torch.zeros(out_channel)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(bs):                                                                           <span class="comment"># 对 batchsize进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> <span class="built_in">range</span>(groups):                                                                     <span class="comment"># 对群组进行遍历</span></span><br><span class="line">            <span class="keyword">for</span> oc <span class="keyword">in</span> <span class="built_in">range</span>(out_channel // groups):                                                 <span class="comment"># 对分组后的输出通道进行遍历</span></span><br><span class="line">                <span class="keyword">for</span> ic <span class="keyword">in</span> <span class="built_in">range</span>(in_channel // groups):                                              <span class="comment"># 对分组厚的输入通道进行遍历</span></span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_h-kernel_h+<span class="number">1</span>,stride):                                    <span class="comment"># 对kernel高度遍历</span></span><br><span class="line">                        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,input_w-kernel_w+<span class="number">1</span>,stride):                                <span class="comment"># 对kernel宽度遍历</span></span><br><span class="line">                            region = <span class="built_in">input</span>[ind, g, ic, i:i+kernel_h:dilation,j:j+kernel_w:dilation]   <span class="comment"># 特征区域</span></span><br><span class="line">                            output[ind,g,oc,<span class="built_in">int</span>(i/stride),<span class="built_in">int</span>(j/stride)] += torch.<span class="built_in">sum</span>(region * kernel[g,oc,ic])</span><br><span class="line">                output[ind,g,oc] += bias[g*(out_channel//groups)+oc]                                <span class="comment"># 考虑偏置</span></span><br><span class="line">    output = output.reshape((bs,out_channel,output_h,output_w))                                     <span class="comment"># 还原成4维</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">bs, in_channel, input_h, input_w = <span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span></span><br><span class="line">out_channel = <span class="number">4</span></span><br><span class="line">groups, dilation, stride, padding = <span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(bs, in_channel, input_h, input_w)</span><br><span class="line">kernel = torch.randn(out_channel,in_channel//groups,kernel_size,kernel_size)</span><br><span class="line">bias = torch.randn(out_channel)</span><br><span class="line"></span><br><span class="line">pytorch_conv2d_api_output = F.conv2d(<span class="built_in">input</span>,kernel,bias=bias,padding=padding,stride=stride,dilation=dilation,groups=groups)</span><br><span class="line">mm_conv2d_final_output = matrix_multiplication_for_conv2d_final(<span class="built_in">input</span>,kernel,bias=bias,stride=stride,padding=padding,dilation=dilation,groups=groups)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_conv2d_api_output,mm_conv2d_final_output))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101930110.png&quot; alt=&quot;image-20230410193006020&quot; style=&quot;zoom:8</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门9-transformer</title>
    <link href="https://wangtongyouwen.github.io/post/c9ccea0.html"/>
    <id>https://wangtongyouwen.github.io/post/c9ccea0.html</id>
    <published>2023-04-08T12:33:48.000Z</published>
    <updated>2023-04-17T12:55:31.323Z</updated>
    
    <content type="html"><![CDATA[<p>https://arxiv.org/pdf/1706.03762.pdf</p><p>https://nlp.seas.harvard.edu/2018/04/03/attention.html</p><h3 id="attention-is-all-you-need-----transformer">attention is all you need ---&gt; transformer</h3><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725197.png" alt="image-20230408205122663" style="zoom: 80%;" /></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101725367.png" alt="image-20230408205258220" style="zoom:50%;" /></p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082110329.jpg" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082209301.png" alt="image-20230408220929314" style="zoom:80%;" /></p><h3 id="pytorch-源码">1 pytorch 源码</h3><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span><br><span class="line">        &gt;&gt;&gt; src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">             num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">             activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">             custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">             device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">    <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.encoder = custom_encoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.decoder = custom_decoder</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                **factory_kwargs)</span><br><span class="line">        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">    self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.nhead = nhead</span><br><span class="line"></span><br><span class="line">    self.batch_first = batch_first</span><br></pre></td></tr></table></figure><p>其中初始化部分最为重要的四部分：TransformerEncoderLayer（通过encoder_layer连接），TransformerDecoderLayer（通过decoder_layer连接）</p><h4 id="transformerencoderlayer">1.1 TransformerEncoderLayer</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Args:</span><br><span class="line">    d_model: the number of expected features <span class="keyword">in</span> the input (required).</span><br><span class="line">    nhead: the number of heads <span class="keyword">in</span> the multiheadattention models (required).</span><br><span class="line">    dim_feedforward: the dimension of the feedforward network model (default=2048).</span><br><span class="line">    dropout: the dropout value (default=0.1).</span><br><span class="line">    activation: the activation <span class="keyword">function</span> of the intermediate layer, can be a string</span><br><span class="line">        (<span class="string">&quot;relu&quot;</span> or <span class="string">&quot;gelu&quot;</span>) or a unary callable. Default: relu</span><br><span class="line">    layer_norm_eps: the eps value <span class="keyword">in</span> layer normalization components (default=1e-5).</span><br><span class="line">    batch_first: If ``True``, <span class="keyword">then</span> the input and output tensors are provided</span><br><span class="line">        as (batch, <span class="built_in">seq</span>, feature). Default: ``False`` (<span class="built_in">seq</span>, batch, feature).</span><br><span class="line">    norm_first: <span class="keyword">if</span> ``True``, layer norm is <span class="keyword">done</span> prior to attention and feedforward</span><br><span class="line">        operations, respectively. Otherwise it<span class="string">&#x27;s done after. Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Examples::</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; out = encoder_layer(src)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">              activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">              layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">              device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">     factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">     <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">     self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                         **factory_kwargs)</span><br><span class="line">     <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">     self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">     self.dropout = Dropout(dropout)</span><br><span class="line">     self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">     self.norm_first = norm_first</span><br><span class="line">     self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">     self.dropout1 = Dropout(dropout)</span><br><span class="line">     self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">         activation = _get_activation_fn(activation)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># We can&#x27;t test self.activation in forward() in TorchScript,</span></span><br><span class="line">     <span class="comment"># so stash some information about it instead.</span></span><br><span class="line">     <span class="keyword">if</span> activation <span class="keyword">is</span> F.relu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.ReLU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">1</span></span><br><span class="line">     <span class="keyword">elif</span> activation <span class="keyword">is</span> F.gelu <span class="keyword">or</span> <span class="built_in">isinstance</span>(activation, torch.nn.GELU):</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">2</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         self.activation_relu_or_gelu = <span class="number">0</span></span><br><span class="line">     self.activation = activation</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">x = src</span><br><span class="line">     <span class="keyword">if</span> self.norm_first:</span><br><span class="line">         x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">         x = x + self._ff_block(self.norm2(x))</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">         x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> x</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h4 id="transformerencoder">1.2 TransformerEncoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers. Users can build the</span></span><br><span class="line"><span class="string">    BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        encoder_layer: an instance of the TransformerEncoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-encoder-layers in the encoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string">        enable_nested_tensor: if True, input will automatically convert to nested tensor</span></span><br><span class="line"><span class="string">            (and convert back on output). This will improve the overall performance of</span></span><br><span class="line"><span class="string">            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span>, enable_nested_tensor=<span class="literal">True</span>, mask_check=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">        self.enable_nested_tensor = enable_nested_tensor</span><br><span class="line">        self.mask_check = mask_check</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="transformerdecoderlayer">1.3 TransformerDecoderLayer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of the intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, layer norm is done prior to self attention, multihead</span></span><br><span class="line"><span class="string">            attention and feedforward operations, respectively. Otherwise it&#x27;s done after.</span></span><br><span class="line"><span class="string">            Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>, norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">        memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">        tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">        memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">        tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">        memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        see the docs in Transformer class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">    x = tgt</span><br><span class="line">    <span class="keyword">if</span> self.norm_first:</span><br><span class="line">        x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">        x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">        x = x + self._ff_block(self.norm3(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">        x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">        x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="transformerdecoder">1.4 TransformerDecoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        decoder_layer: an instance of the TransformerDecoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-decoder-layers in the decoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><em>Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.</em></p><figure><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption></figure><p>在进行解码过程中，第一个词的预测只与第一个词有关，因此最后的的attention机制是个上三角的形式，如上图所示。</p><h4 id="attention">1.5 Attention</h4><figure><img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption></figure><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span>, and values of dimension <span class="math inline">\(d_v\)</span>. We compute the dot products of the query with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax function to obtain the weights on the values.</p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We compute the matrix of outputs as: <span class="math display">\[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304100940797.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="encoder-细节">2 Encoder 细节</h3><h4 id="word-embedding">2.1 word embedding</h4><p>考虑 source sentence 和 target sentence 构建序列，序列的字符以其词表中的索引的形式表示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source sentence 和 target sentence 的初始长度</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 单词表大小</span></span><br><span class="line">max_num_src_words = <span class="number">8</span></span><br><span class="line">max_num_tgt_words = <span class="number">8</span></span><br><span class="line">model_dim = <span class="number">8</span> <span class="comment"># 特征大小，原文是512</span></span><br><span class="line"><span class="comment"># 序列最大长度</span></span><br><span class="line">max_src_seq_len = <span class="number">5</span></span><br><span class="line">max_tgt_seq_len = <span class="number">5</span></span><br><span class="line">max_position_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># src_len = torch.randint(2,5,(batch_size,))</span></span><br><span class="line"><span class="comment"># tgt_len = torch.randint(2,5,(batch_size,)) </span></span><br><span class="line">src_len = torch.Tensor([<span class="number">2</span>,<span class="number">4</span>]).to(torch.int32)  <span class="comment"># 句子长度（2个句子）</span></span><br><span class="line">tgt_len = torch.Tensor([<span class="number">4</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line"><span class="built_in">print</span>(src_len,tgt_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 单词索引构成的源句子和目标句子,pad为最大序列长度,unsqueeze 变为2维张量，然后使用cat拼接起来,padding 默认值为0,构建batch</span></span><br><span class="line">src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_src_words,(L,)),(<span class="number">0</span>,max_src_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]) </span><br><span class="line">tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>,max_num_tgt_words,(L,)),(<span class="number">0</span>,max_tgt_seq_len - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_seq,<span class="string">&quot;\n&quot;</span>,tgt_seq,end=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2 构造 word embedding</span></span><br><span class="line"><span class="comment"># 第0行是默认padding的0，第1-9行是每个单词的embedding结果</span></span><br><span class="line">src_embedding_table = nn.Embedding(max_num_src_words + <span class="number">1</span>, model_dim)</span><br><span class="line">tgt_embedding_table = nn.Embedding(max_num_tgt_words + <span class="number">1</span>, model_dim)</span><br><span class="line">src_embedding = src_embedding_table(src_seq) <span class="comment"># embedding 的 forward 方法</span></span><br><span class="line">stgt_embedding = tgt_embedding_table(tgt_seq) </span><br><span class="line"><span class="built_in">print</span>(src_embedding_table.weight)</span><br><span class="line"><span class="built_in">print</span>(src_seq)</span><br><span class="line"><span class="built_in">print</span>(src_embedding)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="position-embedding">2.2 position embedding</h4><p><span class="math display">\[\begin{aligned}P E_{(p o s, 2 i)} &amp; =\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\P E_{(p o s, 2 i+1)} &amp; =\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)\end{aligned}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step3 构建 position embedding</span></span><br><span class="line">pos_mat = torch.arange(max_position_len).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">i_mat = torch.<span class="built_in">pow</span>(<span class="number">10000</span>,torch.arange(<span class="number">0</span>,model_dim,<span class="number">2</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)/model_dim)</span><br><span class="line">pe_embedding_table = torch.zeros(max_position_len,model_dim)</span><br><span class="line"><span class="comment"># element point</span></span><br><span class="line">pe_embedding_table[:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos_mat/i_mat)  <span class="comment"># 偶数行</span></span><br><span class="line">pe_embedding_table[:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos_mat/i_mat)  <span class="comment"># 奇数行</span></span><br><span class="line"><span class="built_in">print</span>(pos_mat,<span class="string">&#x27;\n&#x27;</span>,i_mat,<span class="string">&#x27;\n&#x27;</span>,pe_embedding_table)</span><br><span class="line"></span><br><span class="line">src_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(src_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> src_len]).to(torch.int32)</span><br><span class="line">tgt_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(tgt_len)),<span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> tgt_len]).to(torch.int32)</span><br><span class="line"></span><br><span class="line">src_pe_embedding = pe_embedding(src_pos)</span><br><span class="line">tgt_pe_embedding = pe_embedding(tgt_pos)</span><br><span class="line"><span class="built_in">print</span>(src_pe_embedding)</span><br><span class="line"><span class="built_in">print</span>(tgt_pe_embedding)</span><br></pre></td></tr></table></figure><p><span class="math inline">\(10000^{2 i / d_{\mathrm{model}}}\)</span> 表示为<span class="math inline">\(\omega_k\)</span>，pos表示为<span class="math inline">\(t\)</span></p><p>解决 out of demain: 如果是超出序列长度，可以通过之前序列长度的线性组合来表示</p><p>For every sine-cosine pair corresponding to frequency <span class="math inline">\(\omega_k\)</span>, there is a linear transformation $ M ^{2 } $(independent of t) where the following equation holds:</p>$$ M <span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega _k \cdot (t+\phi))\\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}\]</span><p>$$ proof:</p>Let <span class="math inline">\(M\)</span> be a <span class="math inline">\(2\times2\)</span> matrix, we want to find <span class="math inline">\(u_1,v_1,u_2\)</span> and <span class="math inline">\(v_2\)</span> so that: $$<span class="math display">\[\begin{bmatrix} u_1 &amp;v_1 \\ u_2 &amp; v_2\end{bmatrix}\]</span><span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega _k \cdot (t+\phi))\\cos(\omega _k \cdot (t+\phi)) \end{bmatrix}\]</span><span class="math display">\[By applying the addition theorem, we can expand the right hand side as follows:\]</span><span class="math display">\[\begin{bmatrix} u_1 &amp;v_1 \\ u_2 &amp; v_2\end{bmatrix}\]</span><span class="math display">\[\begin{bmatrix}    sin(\omega _k \cdot t)  \\    cos(\omega _k \cdot t)  \\     \end{bmatrix}\]</span>=<span class="math display">\[\begin{bmatrix} sin(\omega_k \cdot t)cos(\omega_k \cdot \phi) + cos(\omega_k \cdot t)sin(\omega_k \cdot \phi)         \\cos(\omega_k \cdot t)cos(\omega_k \cdot \phi)  - sin(\omega_k \cdot t)sin(\omega_k \cdot \phi)\end{bmatrix}\]</span><span class="math display">\[Which result in the following two equations:\]</span><span class="math display">\[\begin{array}{l}u_{1} \sin \left(\omega_{k} \cdot t\right)+v_{1} \cos \left(\omega_{k} \cdot t\right)=\cos \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\sin \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right) \\u_{2} \sin \left(\omega_{k} \cdot t\right)+v_{2} \cos \left(\omega_{k} \cdot t\right)=-\sin \left(\omega_{k} \cdot \phi\right) \sin \left(\omega_{k} \cdot t\right)+\cos \left(\omega_{k} \cdot \phi\right) \cos \left(\omega_{k} \cdot t\right)\end{array}\]</span><span class="math display">\[By solving above equations, we get:\]</span><span class="math display">\[\begin{aligned}u_{1}=\cos \left(\omega_{k} \cdot \phi\right) ， v_{1}=\sin \left(\omega_{k} \cdot \phi\right) \\u_{2}=-\sin \left(\omega_{k} \cdot \phi\right) ， v_{2}=\cos \left(\omega_{k} \cdot \phi\right)\end{aligned}\]</span><span class="math display">\[So the final transformation matrix M is:\]</span> M_{,k}=<span class="math display">\[\begin{bmatrix} cos(\omega_k,\phi) &amp; sin(\omega_k,\phi) \\- sin(\omega_k,\phi) &amp; cos(\omega_k,\phi)\end{bmatrix}\]</span><p>$$</p><h4 id="构建encoder的self-attention-mask">2.3 构建encoder的self-attention mask</h4><p>mask的shape：[batch_size,max_src_len,max_tgt_len] 值为1或-inf</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step4 构建encoder的self-attention mask</span></span><br><span class="line">valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(src_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]),<span class="number">2</span>) <span class="comment"># 有效长度</span></span><br><span class="line">valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效矩阵</span></span><br><span class="line">invalid_encoder_pos_matrix = <span class="number">1</span> - valid_encoder_pos_matrix</span><br><span class="line">mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.<span class="built_in">bool</span>) <span class="comment"># 变为bool</span></span><br><span class="line"><span class="built_in">print</span>(valid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,invalid_encoder_pos_matrix,<span class="string">&#x27;\n&#x27;</span>,mask_encoder_self_attention) <span class="comment">#(batchsize,maxlen after padding,_)</span></span><br><span class="line"><span class="comment"># true 需要 mask</span></span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(src_len),<span class="built_in">max</span>(src_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_score = score.masked_fill(mask_encoder_self_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(src_len)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="built_in">print</span>(masked_score)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br><span class="line"><span class="comment"># 无需因果的遮掩</span></span><br></pre></td></tr></table></figure><h4 id="scaled-的重要性">2.4 scaled 的重要性</h4><p><span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]</span></p><p>这里的softmax中为什么要除以<span class="math inline">\(\sqrt{d_k}\)</span>?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># when the varience of prob is too big</span></span><br><span class="line">alpha1 = <span class="number">0.1</span></span><br><span class="line">alpha2 = <span class="number">10</span></span><br><span class="line">score = torch.randn(<span class="number">5</span>)</span><br><span class="line">prob1 = F.softmax(score*alpha1,-<span class="number">1</span>)</span><br><span class="line">prob2 = F.softmax(score*alpha2,-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_func</span>(<span class="params">score</span>):</span><br><span class="line">    <span class="keyword">return</span> F.softmax(score)</span><br><span class="line">jaco_mat1 = torch.autograd.functional.jacobian(softmax_func,score*alpha1)</span><br><span class="line">jaco_mat2 = torch.autograd.functional.jacobian(softmax_func,score*alpha2)</span><br><span class="line"><span class="comment"># jaco matrix is close to zero when the varience is too big</span></span><br><span class="line"><span class="comment"># print(score,prob1,prob2)</span></span><br><span class="line"><span class="built_in">print</span>(jaco_mat1,<span class="string">&#x27;\n&#x27;</span>,jaco_mat2)</span><br></pre></td></tr></table></figure><h3 id="decoder-细节">3 decoder 细节</h3><h4 id="intra-attention-的-mask">3.1 intra-attention 的 mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step5 构造intra-attention的mask</span></span><br><span class="line"><span class="comment"># Q @ k^T shape:(batch_size,tgt_seq_len,src_seq_len)</span></span><br><span class="line">valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len]),<span class="number">2</span>)</span><br><span class="line">valid_cross_pos_matrix = torch.bmm(valid_decoder_pos,valid_encoder_pos.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 有效位置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;源序列有效位置张量：&quot;</span>,valid_encoder_pos,<span class="string">&quot;\n 目标序列有效位置张量：&quot;</span>,valid_decoder_pos,<span class="string">&quot;\n 目标序列对源头序列有效位置张量：&quot;</span>,valid_cross_pos)</span><br><span class="line"></span><br><span class="line">invalid_cross_pos_matrix = <span class="number">1</span> - valid_cross_pos_matrix</span><br><span class="line">mask_cross_attention = invalid_cross_pos_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(mask_cross_attention)</span><br><span class="line"><span class="comment"># print(valid_cross_pos)</span></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line"><span class="comment"># print(score.shape,mask_encoder_self_attention.shape)</span></span><br><span class="line"><span class="comment"># masked </span></span><br><span class="line">masked_cross_score = score.masked_fill(mask_cross_attention,-<span class="number">1e9</span>)</span><br><span class="line">prob_cross = F.softmax(masked_cross_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(prob_cross)</span><br></pre></td></tr></table></figure><h4 id="decoder-self-attention">3.2 decoder self-attention</h4><p>下三角形的mask：防止因果</p><p>要把答案遮住，如果预测第四个位置，就要把第四个位置以后的所有内容都遮住。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step6 构造 decoder self-attention 的 mask</span></span><br><span class="line">valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L,<span class="number">0</span>,<span class="built_in">max</span>(tgt_len) - L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">invalid_decoder_tri_matrix = <span class="number">1</span> - valid_decoder_tri_matrix</span><br><span class="line">invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(valid_decoder_tri_matrix,invalid_decoder_tri_matrix)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size,<span class="built_in">max</span>(tgt_len),<span class="built_in">max</span>(tgt_len))</span><br><span class="line">masked_score = score.masked_fill(invalid_decoder_tri_matrix,-<span class="number">1e9</span>)</span><br><span class="line">prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(tgt_len)</span><br><span class="line"><span class="built_in">print</span>(prob)</span><br></pre></td></tr></table></figure><p>流式预测的时候，特别需要这个掩码。</p><h4 id="scaled-self-attention">3.3 scaled self-attention</h4><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101603265.png" alt="image-20230410160332412" /><figcaption aria-hidden="true">image-20230410160332412</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q,K,V,attn_mask</span>):</span><br><span class="line">    <span class="comment"># shape pf Q,k,V: (batch_size * num_head,seq_len,model_dim/num_head)</span></span><br><span class="line">    score = torch.bmn(Q,K.transpose(-<span class="number">2</span>,-<span class="number">1</span>))/torch.sqrt(model_dim)</span><br><span class="line">    masked_score = score.masked_fill(attn_mask,-<span class="number">1e9</span>)</span><br><span class="line">    prob = F.softmax(masked_score,-<span class="number">1</span>)</span><br><span class="line">    context = torch.bmn(prov,V)</span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>源码：D:\0_python-packages.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multi_head_attention_forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    query: Tensor,</span></span><br><span class="line"><span class="params">    key: Tensor,</span></span><br><span class="line"><span class="params">    value: Tensor,</span></span><br><span class="line"><span class="params">    embed_dim_to_check: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    in_proj_weight: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    in_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_k: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    bias_v: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    add_zero_attn: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">    dropout_p: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    out_proj_weight: Tensor,</span></span><br><span class="line"><span class="params">    out_proj_bias: <span class="type">Optional</span>[Tensor],</span></span><br><span class="line"><span class="params">    training: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    use_separate_proj_weight: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    q_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    k_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    v_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_k: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    static_v: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    average_attn_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span><br></pre></td></tr></table></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101653337.png" alt="image-20230410165300158" /><figcaption aria-hidden="true">image-20230410165300158</figcaption></figure><h3 id="loss-function">4 Loss function</h3><p>https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304101703643.png" alt="image-20230410170322549" /><figcaption aria-hidden="true">image-20230410170322549</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bath_size = <span class="number">2</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">vocab_size = <span class="number">4</span></span><br><span class="line">logits = torch.randn(bath_size,seq_len,vocab_size)      <span class="comment"># bath_size = 2, seq_len = 3, vocab_size = 4</span></span><br><span class="line">label = torch.randint(<span class="number">0</span>,vocab_size,(bath_size,seq_len))</span><br><span class="line">logits = logits.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">F.cross_entropy(logits,label) <span class="comment"># 六个单词的平均交叉熵</span></span><br><span class="line">F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) <span class="comment"># 返回所有单词的交叉熵</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mask</span></span><br><span class="line">tgt_len =torch.Tensor([<span class="number">2</span>,<span class="number">3</span>]).to(torch.int32)</span><br><span class="line">mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(<span class="number">0</span>,<span class="built_in">max</span>(tgt_len)-L)),<span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line">cross_entropy = F.cross_entropy(logits,label,reduction=<span class="string">&quot;none&quot;</span>) * mask </span><br><span class="line"><span class="built_in">print</span>(cross_entropy)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/p&gt;
&lt;p&gt;https://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;
&lt;h3 id=&quot;attention-is-all-you-nee</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门8-CONV层</title>
    <link href="https://wangtongyouwen.github.io/post/2d27b0da.html"/>
    <id>https://wangtongyouwen.github.io/post/2d27b0da.html</id>
    <published>2023-04-08T07:26:00.000Z</published>
    <updated>2023-04-17T12:55:41.594Z</updated>
    
    <content type="html"><![CDATA[<h1 id="conv2d">CONV2D</h1><p>torch.nn.Conv2d(<em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode='zeros'</em>, <em>device=None</em>, <em>dtype=None</em>)</p><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels in the input image</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – Size of the convolving kernel</li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – Padding added to all four sides of the input. Default: 0</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></li></ul><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081528114.png" alt="image-20230408152839156" style="zoom:80%;" /></p><h1 id="conv_residual_block_fusion">conv_residual_block_fusion</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">in_channels = <span class="number">2</span></span><br><span class="line">out_channels = <span class="number">2</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">w = <span class="number">9</span></span><br><span class="line">h = <span class="number">9</span></span><br><span class="line"><span class="comment"># res_block = 3*3 conv + 1*1 conv + input</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>,in_channels,w,h) <span class="comment"># 输入图片大小</span></span><br><span class="line"><span class="comment"># 方法1：原生写法</span></span><br><span class="line">conv_2d = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_pointwise = nn.Conv2d(in_channels,out_channels,<span class="number">1</span>)</span><br><span class="line">result1 = conv_2d(x) + conv_2d_pointwise(x) + x</span><br><span class="line"><span class="comment"># print(result1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：算子融合</span></span><br><span class="line"><span class="comment"># 把point-wise卷积核x本身都写成3*3卷积</span></span><br><span class="line"><span class="comment"># 最终把三个卷积写成一个卷积</span></span><br><span class="line"><span class="comment"># 1*1 -&gt; 3*3</span></span><br><span class="line"><span class="comment"># 1.改造</span></span><br><span class="line">pointwise_to_conv_weight = F.pad(conv_2d_pointwise.weight,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]) <span class="comment"># 2*2*1*1 -&gt; 2*2*3*3</span></span><br><span class="line">conv_2d_for_pointwise = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_for_pointwise.weight = nn.Parameter(pointwise_to_conv_weight)</span><br><span class="line">conv_2d_for_pointwise.bias = nn.Parameter(conv_2d_pointwise.bias)</span><br><span class="line"><span class="comment"># x -&gt; 3*3</span></span><br><span class="line"><span class="comment"># 2*2*3*3</span></span><br><span class="line">zeros = torch.unsqueeze(torch.zeros(kernel_size,kernel_size),<span class="number">0</span>) <span class="comment"># 不考虑相邻通道的影响</span></span><br><span class="line">stars = torch.unsqueeze(F.pad(torch.ones(<span class="number">1</span>,<span class="number">1</span>),[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]),<span class="number">0</span>) <span class="comment"># 不考虑周围点的影响(1*1)</span></span><br><span class="line">stars_zeros = torch.unsqueeze(torch.cat([stars,zeros],<span class="number">0</span>),<span class="number">0</span>) <span class="comment"># 第一个输出通道</span></span><br><span class="line">zeros_stars = torch.unsqueeze(torch.cat([stars,zeros],<span class="number">0</span>),<span class="number">0</span>) <span class="comment"># 第二个输出通道</span></span><br><span class="line">identity_to_conv_weight = torch.cat([stars_zeros,zeros_stars],<span class="number">0</span>)  </span><br><span class="line">identity_to_conv_bias = torch.zeros([out_channels])</span><br><span class="line">conv_2d_for_identity = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_for_identity.weight = nn.Parameter(identity_to_conv_weight)</span><br><span class="line">conv_2d_for_identity.bias = nn.Parameter(identity_to_conv_bias)</span><br><span class="line"></span><br><span class="line">result2 = conv_2d(x) + conv_2d_for_pointwise(x) + conv_2d_for_identity(x)</span><br><span class="line"><span class="comment"># print(result2)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result1,result2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.融合</span></span><br><span class="line">conv_2d_for_fusion = nn.Conv2d(in_channels,out_channels,kernel_size,padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">conv_2d_for_fusion.weight = nn.Parameter(conv_2d.weight.data + conv_2d_for_pointwise.weight.data + conv_2d_for_identity.weight.data)</span><br><span class="line">conv_2d_for_fusion.bias = nn.Parameter(conv_2d.bias.data + conv_2d_for_pointwise.bias.data + conv_2d_for_identity.bias.data)</span><br><span class="line">result3 = conv_2d_for_fusion(x)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result2,result3)))</span><br></pre></td></tr></table></figure><p>算子融合能够提升速度。</p><h1 id="convmixer-layer">ConvMixer Layer</h1><p>https://arxiv.org/pdf/2201.09792.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081948885.png" alt="image-20230408194824730" /><figcaption aria-hidden="true">image-20230408194824730</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081955360.png" alt="image-20230408195505692" /><figcaption aria-hidden="true">image-20230408195505692</figcaption></figure><p>空间混合(depthwise)+通道混合(pointwise)</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304082032487.png" alt="image-20230408203214916" /><figcaption aria-hidden="true">image-20230408203214916</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;conv2d&quot;&gt;CONV2D&lt;/h1&gt;
&lt;p&gt;torch.nn.Conv2d(&lt;em&gt;in_channels&lt;/em&gt;, &lt;em&gt;out_channels&lt;/em&gt;, &lt;em&gt;kernel_size&lt;/em&gt;, &lt;em&gt;stride=1&lt;/em&gt;, &lt;em&gt;pa</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门7-dropout</title>
    <link href="https://wangtongyouwen.github.io/post/6ae9c33a.html"/>
    <id>https://wangtongyouwen.github.io/post/6ae9c33a.html</id>
    <published>2023-04-07T14:34:02.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dropout">1 dropout</h2><ul><li>dropout class实现</li></ul><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072236280.png" alt="image-20230407223601601" style="zoom:80%;" /></p><ul><li>dropout函数实现</li></ul><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072237668.png" alt="image-20230407223740635" style="zoom:80%;" /></p><p>training = self.training</p><p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p><p>https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer,</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081429465.png" alt="image-20230408142934611" /><figcaption aria-hidden="true">image-20230408142934611</figcaption></figure><p>https://zhuanlan.zhihu.com/p/38200980</p><h3 id="在-numpy-中实现-dropout">在 numpy 中实现 dropout：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># implement dropout in numpy codes</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">rate,x,w1,w2,b1,b2</span>):</span><br><span class="line">    <span class="comment"># suppose two layers</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer1.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer1 = mask1 * layer1</span><br><span class="line">    </span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer2.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer2 = mask2 * layer2</span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">rate,x,w1,b1,w2,b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    layer1 = layer1 * (<span class="number">1</span> - rate)</span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    layer2 = layer2 * (<span class="number">1</span> - rate)</span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test scale in the train</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_train</span>(<span class="params">rate,x,w1,w2,b1,b2</span>):</span><br><span class="line">    <span class="comment"># suppose two layers</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer1.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer1 = mask1 * layer1</span><br><span class="line">    layer1 = layer1 / (<span class="number">1</span> - rate)</span><br><span class="line">    </span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>,<span class="number">1</span> - rate,layer2.shape)   <span class="comment"># p: 为1 的概率</span></span><br><span class="line">    layer2 = mask2 * layer2</span><br><span class="line">    layer2 = layer2 / (<span class="number">1</span> - rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="comment"># without the scale</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_test</span>(<span class="params">x,w1,b1,w2,b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>,np.dot(w1,x) + b1)  <span class="comment"># relu linear layer</span></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>,np.dot(w2,layer1) + b2)  <span class="comment"># relu linear layer</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure><h2 id="r-dropout">2 r dropout</h2><p>https://proceedings.neurips.cc/paper/2021/file/5a66b9200f29ac3fa0ae244cc2a51b39-Paper.pdf</p><p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20230408151201702.png" alt="image-20230408151201702" style="zoom:67%;" /></p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304081515075.png" alt="image-20230408151505168" style="zoom:80%;" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;dropout&quot;&gt;1 dropout&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;dropout class实现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/i</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础知识1-自动微分</title>
    <link href="https://wangtongyouwen.github.io/post/42120d73.html"/>
    <id>https://wangtongyouwen.github.io/post/42120d73.html</id>
    <published>2023-04-07T06:57:37.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<p>https://arxiv.org/pdf/1502.05767.pdf</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071556707.png" alt="image-20230407155605114" /><figcaption aria-hidden="true">image-20230407155605114</figcaption></figure><p>几种的常见微分方式：</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071601783.png" alt="image-20230407160104716" /><figcaption aria-hidden="true">image-20230407160104716</figcaption></figure><ul><li>符号微分：求导</li><li>数值微分：不稳定，并且不准确</li><li>自动微分：这个例子是一个前向过程，通过预设dv = 1，然后根据每一步的dv表达式求出当前，v和dv的值</li></ul><h3 id="forward-mode-ad">1 forward mode AD</h3><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071629866.png" alt="image-20230407162927134" /><figcaption aria-hidden="true">image-20230407162927134</figcaption></figure><p>分为三个部分：输入节点，隐藏节点，输出节点。其中隐藏节点也可以为称为元操作</p><p>首先设置初值为2,5，初始倒数x1为1，x2为0.</p><p>使用前向微分的特点：</p><ul><li>能够在前向运算的同时，计算前向微分的值，能够计算出每个元操作的输入节点的偏导数值</li><li>但是一次只能计算一个输入节点的偏导数</li></ul><p>或者能够采用对偶数的计算方法：</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072114545.png" alt="image-20230407211449434" style="zoom:80%;" /> <span class="math display">\[\left.\frac{d f(x)}{d x}\right|_{x=v}=\operatorname{epsilon-coefficient}(\text { dual-version }(f)(v+1 \epsilon)) \text {. }\]</span></p><h3 id="reverse-mode-ad">2 reverse mode AD</h3><p>链式法则：</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072123618.png" alt="image-20230407212309766" /><figcaption aria-hidden="true">image-20230407212309766</figcaption></figure><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304072113372.png" alt="image-20230407211303243" /><figcaption aria-hidden="true">image-20230407211303243</figcaption></figure><p>v0的倒数在这个例子中进行了梯度累加。</p><p>假设 a=f(x),b=g(a).y=h(b) <span class="math display">\[\frac{d_y}{d_x} = \frac{d_h}{d_b} *\frac{d_b}{d_a}* \frac{d_a}{d_x}\]</span> 雅克比矩阵的维度为：|y|*|b|,|b|*|a|,|a|*|x|</p><p>分别统计计算量：</p><ul><li><p>forward mode AD：|y|*|b|*(|b|*|a|,|a|*|x|) = bax + ybx</p></li><li><p>reverse mode AD: |y|*|b|*|b|*|a|*|a|*|x| = yba + yax</p></li></ul><p>假设 a = b 比较 x 和 y 的大小：</p><ul><li>当x&gt;y，输入特征大于输出特征，reverse mode 计算量小</li><li>当x&lt;y，输入特征大小于输出特征，forward mode 计算量小</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;https://arxiv.org/pdf/1502.05767.pdf&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304071556</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门6-autograd</title>
    <link href="https://wangtongyouwen.github.io/post/40d10b1c.html"/>
    <id>https://wangtongyouwen.github.io/post/40d10b1c.html</id>
    <published>2023-04-06T12:25:44.000Z</published>
    <updated>2023-04-12T12:30:11.323Z</updated>
    
    <content type="html"><![CDATA[<p>训练神经网络如何使用pytorch中的自动微分</p><p>https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062036309.png" alt="image-20230406203624607" /><figcaption aria-hidden="true">image-20230406203624607</figcaption></figure><h4 id="得到计算图">1 得到计算图</h4><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062042317.png" alt="image-20230406204239318" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure><p>通过对参数进行单个源操作，得到计算图，然后进一步进行反向梯度回传计算</p><h4 id="计算">2 计算</h4><p>我们对其中设置grad为true 的变量进行梯度回传计算。使用backward函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><ul><li>我们只能对计算图中requires_grad为true的进行梯度回传计算，比如dropout，batchnorm等都不行</li><li>由于我们只能在回传计算时生成一个图，如果我们需要对一个静态图进行多次梯度回传，我们需要把 retain_graph=True</li></ul><h4 id="将某些计算节点取消梯度回传">3 将某些计算节点取消梯度回传</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line"><span class="built_in">print</span>(z_det.requires_grad)</span><br></pre></td></tr></table></figure><ul><li>某些 frozen parameters</li><li>加速运算</li></ul><h4 id="grad.zero_">4 grad.zero_()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inp = torch.eye(<span class="number">4</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">out = (inp+<span class="number">1</span>).<span class="built_in">pow</span>(<span class="number">2</span>).t()</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First call\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSecond call\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br><span class="line">inp.grad.zero_()</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nCall after zeroing gradients\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>由于梯度在每次调用的时候都会累加(<strong>Jacobian Product</strong>),我们需要使用grad.zero_()这样的梯度才是正确的。</p><h4 id="jacobian在pytorch中的实现">5 jacobian在pytorch中的实现</h4><p>torch.autograd.functional.jacobian(<em>func</em>, <em>inputs</em>, <em>create_graph=False</em>, <em>strict=False</em>, <em>vectorize=False</em>, <em>strategy='reverse-mode'</em>)</p><p>https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062118841.png" alt="image-20230406211810400" /><figcaption aria-hidden="true">image-20230406211810400</figcaption></figure><p>这个求偏导表示：生成的sum第一个数与原张量的第二行没有关系，所以求偏导也是0</p><h4 id="向量对向量的微分">6 向量对向量的微分</h4><p>首先当其中一个向量是列向量时候：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> a + x</span><br><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">torch.autograd.functional.jacobian(func,x)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br></pre></td></tr></table></figure><p>显然这里的结果表示，f = a + x 中 ，f1,2,3 只与 x1,2,3 有关，所以是个对角阵</p><p>向量对向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> a + x</span><br><span class="line">x = torch.randn(<span class="number">3</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">torch.autograd.functional.jacobian(func,x)</span><br><span class="line">y = func(x)</span><br><span class="line">y.backward(torch.ones_like(y))</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones_like(y) @ torch.autograd.functional.jacobian(func,x)</span><br></pre></td></tr></table></figure><h4 id="矩阵对矩阵的偏导">7 矩阵对矩阵的偏导</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>,<span class="number">2</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = a @ b</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;训练神经网络如何使用pytorch中的自动微分&lt;/p&gt;
&lt;p&gt;https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://p</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础入门5-container详解</title>
    <link href="https://wangtongyouwen.github.io/post/f208623b.html"/>
    <id>https://wangtongyouwen.github.io/post/f208623b.html</id>
    <published>2023-04-06T11:55:57.000Z</published>
    <updated>2023-04-12T12:30:11.339Z</updated>
    
    <content type="html"><![CDATA[<h5 id="torch.nn.sequentialargs-module">1 torch.nn.Sequential(args: <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a>)</h5><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential</p><p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062008133.png" alt="image-20230406200820589" style="zoom:80%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self:</span><br><span class="line">        <span class="built_in">input</span> = module(<span class="built_in">input</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">input</span></span><br></pre></td></tr></table></figure><h5 id="torch.nn.modulelistmodulesnone">2 torch.nn.ModuleList(modules=None)</h5><p>(https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList)</p><p>Holds submodules in a list.</p><figure><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304062017290.png" alt="image-20230406201713167" /><figcaption aria-hidden="true">image-20230406201713167</figcaption></figure><h5 id="torch.nn.moduledictmodulesnone">3 torch.nn.ModuleDict(modules=None)</h5><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict</p><p>Holds submodules in a dictionary.</p><h5 id="torch.nn.parameterlistvaluesnone">4 torch.nn.ParameterList(values=None)</h5><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList</p><p>Holds submodules in a list.</p><h5 id="torch.nn.parameterdictparametersnone">5 torch.nn.ParameterDict(<em>parameters=None</em>)</h5><p>https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict</p><p>Holds parameters in a dictionary.</p><p>其中sequential能使用forward，其他容器不能，所以一般都使用sequential这个容器</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h5 id=&quot;torch.nn.sequentialargs-module&quot;&gt;1 torch.nn.Sequential(args: &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html#</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://wangtongyouwen.github.io/tags/pytorch/"/>
    
  </entry>
  
</feed>
