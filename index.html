<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" >
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <meta http-equiv="Content-Language" content="zh-cn">
  <link rel="dns-prefetch" href="https://wangtongyouwen.github.io">
  <title>jyh blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:type" content="website">
<meta property="og:title" content="jyh blog">
<meta property="og:url" content="https://wangtongyouwen.github.io/index.html">
<meta property="og:site_name" content="jyh blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="jyh">
<meta name="twitter:card" content="summary">
  
  
    <link rel="alternative" href="/atom.xml" title="jyh blog" type="application/atom+xml">
  
  
    <link rel="icon" type="image/x-icon" href="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051210238.jpg">
  
  <link rel="stylesheet" type="text/css" href="/css/main.0cf68a.css">
  
	<link rel="stylesheet" type="text/css" href="/css/avatarrotation.css">
  
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(45deg, #e0e5df, #96a48b);
    }
  </style>
    
  <!-- 引入font-awesome图标库 -->
  <!-- <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet"> -->
  <link href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css" rel="stylesheet">
  <!-- <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">-->
  
  <!--谷歌分析-->
  

  <!--百度统计-->
  
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  <!--百度自动推送-->
  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body>

  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: linear-gradient(45deg, #e0e5df, #96a48b)"></div>
<div class="intrude-less">
	<header id="header" class="inner">
	
		<a href="/" class="profilepic">
			<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051210238.jpg" class="js-avatar" alt="avatar">
		</a>
		
		<hgroup>
		  <div class="header-author"><a href="/">jyh</a></div>
		</hgroup>
		
		
		<p class="header-subtitle">jyh的博客</p>
		

		<nav class="header-menu">
			<ul>   
			
			   	
				  <li><a href="/" class="fa fa-home fa-fw"></a></li>
				
	        
			   	
				  <li><a href="/archives/index.html">归档</a></li>
				
	        
			   	
				  <li><a href="/categories/index.html">分类</a></li>
				
	        
			
			</ul>
		</nav>

		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
			
			  
				<a href="/tags/pytorch/">pytorch</a>
			  
	        		
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="gitee" target="_blank" href="#" title="gitee"><i class="icon-gitee"></i></a>
		        
					<a class="csdn" target="_blank" href="#" title="csdn"><i class="icon-csdn"></i></a>
		        
					<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
		        
					<a class="rss" target="_blank" href="/atom.xml" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:XXX@XXX.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
		
		<!-- 网易云音乐插件 -->
		
			
		<!--时钟-->
		
			<!--时钟-->
<br>
<div style="position:absolute; bottom:120px left:auto; width:100%;height:50%">
	<script type="text/javascript" src="https://cdn.staticfile.org/vue/2.4.2/vue.min.js"></script>
	<div id="clock" style="font-family: 'Share Tech Mono', monospace;color: #ffffff;text-align: center;position: absolute;width: 250px;left: 50%;top: 50%;-webkit-transform: translate(50%, 50%);transform: translate(-50%, -50%);color: #4B8CE1;/* text-shadow: 0 0 20px #0aafe6, 0 0 20px rgba(10, 175, 230, 0); */">
		<p style="margin: 0;padding: 0;letter-spacing: 0.1em;font-size: 15px;">{{ date }}</p>
		<p style="margin: 0;padding: 0;letter-spacing: 0.01em;font-size: 25px;">{{ time }}</p>
	</div>
	<script>
		var clock = new Vue({
			el: '#clock',
			data: {
				time: '',
				date: ''
			}
		});

		var week = ['星期日', '星期一', '星期二', '星期三', '星期四', '星期五', '星期六'];
		var timerID = setInterval(updateTime, 1000);
		updateTime();
		function updateTime() {
			var cd = new Date();
			clock.time = zeroPadding(cd.getHours(), 2) + ':' + zeroPadding(cd.getMinutes(), 2) + ':' + zeroPadding(cd.getSeconds(), 2);
			clock.date = zeroPadding(cd.getFullYear(), 4) + '-' + zeroPadding(cd.getMonth() + 1, 2) + '-' + zeroPadding(cd.getDate(), 2) + ' ' + week[cd.getDay()];
		};

		function zeroPadding(num, digit) {
			var zero = '';
			for (var i = 0; i < digit; i++) {
				zero += '0';
			}
			return (zero + num).slice(-digit);
		}
	</script>
</div>
		

	</header>		
</div>



    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<!-- <div class="overlay js-overlay" style="background: linear-gradient(45deg, #e0e5df, #96a48b)"></div> -->
	<div class="overlay js-overlay" ></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)">
		<div class="left-icon-container">
			<i class="icon icon-sort"></i></div>
		</div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<a href="/" class="profilepic">
				<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304051210238.jpg" class="js-avatar" alt="avatar">
			</a>
			
			<hgroup>
			  <div class="header-author js-header-author">jyh</div>
			</hgroup>
			
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>jyh的博客<i class="icon icon-quo-right"></i></p>
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="gitee" target="_blank" href="#" title="gitee"><i class="icon-gitee"></i></a>
			        
						<a class="csdn" target="_blank" href="#" title="csdn"><i class="icon-csdn"></i></a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
			        
						<a class="rss" target="_blank" href="/atom.xml" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:XXX@XXX.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>
			
			
			
			
				
			
				
			
				
			
			
				
			
			

			<nav class="header-menu js-header-menu">
				<ul style="width: 80%">
					
					
						<li style="width: 25%"><a href="/">主页</a></li>
					
						<li style="width: 25%"><a href="/archives/index.html">归档</a></li>
					
						<li style="width: 25%"><a href="/categories/index.html">分类</a></li>
					
					
						<li style="width: 25%"><a href="/tags/pytorch/">pytorch</a></li>
					
				</ul>	
			</nav>
			
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-yolo简介" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/6a09e8f8.html">yolo简介</a>
    </h2>
  

        
		
		  <a href="/post/6a09e8f8.html" class="archive-article-date">
  	<time datetime="2023-12-13T08:51:42.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-12-13</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">17k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">73min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h1 id="a-comprehensive-review-of-yolo-from-yolov1-to-yolov8-and-beyond">A COMPREHENSIVE REVIEW OF YOLO: FROM YOLOV1 TO YOLOV8 AND BEYOND</h1>
<p>https://arxiv.org/pdf/2304.00501v1.pdf</p>
<h2 id="abstract">abstract</h2>
<p>YOLO已成为机器人、无人驾驶汽车和视频监控应用的核心实时目标检测系统。我们对YOLO的发展进行了全面的分析，研究了从最初的YOLO到YOLOv8的每次迭代中的创新和贡献。我们首先描述标准指标和后处理;然后，我们讨论了网络架构的主要变化和每个模型的训练技巧。最后，我们总结了YOLO发展的重要经验教训，并对其未来发展进行了展望，强调了增强实时目标检测系统的潜在研究方向。</p>
<h2 id="introduction">1 Introduction</h2>
<p>实时目标检测已成为众多应用中的关键组成部分，涵盖自动驾驶汽车、机器人、视频监控和增强现实等各个领域。在各种目标检测算法中，YOLO (You Only Look Once)框架以其出色的速度和精度平衡而脱颖而出，能够快速可靠地识别图像中的目标。自成立以来，YOLO家族经历了多次迭代，每次迭代都建立在以前的版本之上，以解决局限性并提高性能(见图1)。本文旨在全面回顾YOLO框架的发展，从最初的YOLOv1到最新的YOLOv8，阐明每个版本的关键创新、差异和改进。</p>
<p>本文首先探讨了原始YOLO模型的基本概念和体系结构，为YOLO家族的后续发展奠定了基础。接下来，我们将深入研究从YOLOv2到YOLOv8的每个版本中引入的改进和增强。这些改进包括网络设计、损失函数修改、anchor修改和输入分辨率缩放等各个方面。通过研究这些发展，我们的目标是全面了解YOLO框架的发展及其对目标检测的影响。</p>
<p>除了讨论每个YOLO版本的具体进步之外，本文还强调了在整个框架开发过程中出现的速度和准确性之间的权衡。这强调了在选择最合适的YOLO模型时考虑特定应用程序的上下文和需求的重要性。</p>
<p>最后，我们展望了YOLO框架的未来方向，触及了进一步研究和开发的潜在途径，这将影响实时目标检测系统的持续进展。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231213175522050.png" alt="image-20231213175522050" style="zoom:50%;" /></p>
<h2 id="yolo在不同领域的应用">2 YOLO在不同领域的应用</h2>
<p>YOLO的实时目标检测能力在自动驾驶车辆系统中非常宝贵，可以快速识别和跟踪各种物体，如车辆、行人<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>、自行车和其他障碍物<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>。</p>
<p>这些能力已经应用于许多领域，包括用于监控的视频序列中的动作识别<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>、运动分析<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>和人机交互<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>。</p>
<p>YOLO模型已在农业中用于农作物<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>、病虫害<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>的检测和分类，协助实现精准农业技术和农业流程自动化。它们也适用于生物识别、安全和面部识别系统中的面部检测任务<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>。</p>
<p>在医学领域，YOLO已被用于癌症检测<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>、皮肤分割<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>、药丸识别<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>，提高了诊断的准确性，提高了治疗的效率。在遥感领域，它已被用于卫星和航空图像中的目标检测和分类，有助于土地利用制图、城市规划和环境监<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a><a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a><a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>。</p>
<p>安防系统集成了YOLO模型，用于实时监控和分析视频馈送，从而可以快速发现可疑活动<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>、保持社交距离和检测口罩<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>。这些模型还被应用于表面检测，以检测缺陷和异常，加强制造和生产过程中的质量控制<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a><a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a><a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>。</p>
<p>在交通应用中，YOLO模型已被用于车牌检测<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>和交通标志识别<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>等任务，有助于智能交通系统和交通管理解决方案的发展。它们已被用于野生动物检测和监测，以识别濒危物种，用于生物多样性保护和生态系统管理<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>。最后，YOLO已广泛应用于机器人应用<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a><a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>和无人机目标检测<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a><a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>。</p>
<h2 id="目标检测衡量指标和非最大抑制nms">3 目标检测衡量指标和非最大抑制(NMS)</h2>
<h3 id="ap定义">3.1 AP定义</h3>
<p>平均精度(AP)，传统上称为平均平均精度(mAP)，是评估目标检测模型性能的常用度量。它测量所有类别的平均精度，提供一个单一的值来比较不同的模型。COCO数据集没有区分AP和mAP。在本文的其余部分，我们将把这个度量称为AP。</p>
<p>在YOLOv1和YOLOv2中，用于训练和基准测试的数据集是PASCAL VOC 2007和VOC 2012<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>。然而，从YOLOv3开始，使用的数据集是Microsoft COCO (Common Objects in Context)<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>。对于这些数据集，AP的计算方法不同。下面几节将讨论AP背后的基本原理，并解释如何计算AP。</p>
<p><strong>精确度和召回率</strong>:<span class="math inline">\(\text{precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}\)</span>,<span class="math inline">\(\text{recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}\)</span> <span class="math display">\[
F = \frac{(\alpha^2+1)P*R}{\alpha^2(P+R)}
\]</span> 例如，增加检测到的对象的数量(更高的召回率)可能导致更多的误报(更低的精度)。为了考虑这种权衡，AP指标结合了精确率-召回率曲线，该曲线绘制了不同置信度阈值下的精确率和召回率。该指标通过考虑准确度-召回曲线下的面积，提供了对精度和召回率的平衡评估</p>
<p><strong>处理多个目标类别</strong>:目标检测模型必须识别和定位图像中的多个目标类别。AP度量通过分别计算每个类别的平均精度(AP)，然后在所有类别中取这些AP的平均值来解决这个问题(这就是为什么它也称为平均平均精度)。这种方法确保对每个类别单独评估模型的性能，为模型的整体性能提供更全面的评估。</p>
<p><strong>交并比</strong>：目标检测的目的是通过预测边界框来准确定位图像中的目标。AP度量结合了(IoU)交叉度量来评估预测边界框的质量。IoU是预测边界框与地面真值边界框的交集面积与并集面积之比(见图2)。IoU衡量的是地面真值边界框与预测边界框的重叠程度。COCO基准考虑多个IoU阈值来评估模型在不同定位精度水平下的性能。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214090616347.png" alt="image-20231214090616347" style="zoom:50%;" /></p>
<h3 id="计算ap">3.2 计算AP</h3>
<h4 id="voc-dataset">VOC Dataset</h4>
<p>该数据集包括<strong>20</strong>个对象类别。为了计算VOC中的AP，我们遵循以下步骤:</p>
<pre><code>1. 对于每个类别，通过改变模型预测的置信阈值来计算精确召回曲线。</code></pre>
<ol start="2" type="1">
<li><p>使用在精度-召回率曲线的11点插值抽样计算每个类别的平均精度(AP)。</p></li>
<li><p>通过取所有20个类别中AP的平均值来计算最终的平均精度(AP)。</p></li>
</ol>
<h4 id="microsoft-coco-dataset">Microsoft COCO Dataset</h4>
<p>该数据集包括80个对象类别，并使用更复杂的方法来计算AP。它不使用11点插值，而是使用101点插值，也就是说，它计算101个召回阈值的精度，从0到1，增量为0.01。此外，AP是通过对多个IoU值(而不仅仅是一个)进行平均来获得的，除了一个称为AP50的通用AP度量，它是单个IoU阈值0.5的AP。在COCO中计算AP的步骤如下:</p>
<ol type="1">
<li><p>对于每个类别，通过改变模型预测的置信阈值来计算精确召回曲线。</p></li>
<li><p>使用101点插值计算每个类别的平均精度(AP)。</p></li>
<li><p>计算不同交汇交汇(IoU)阈值的AP，通常从0.5到0.95，步长为0.05。更高的IoU阈值需要更准确的预测才能被认为是真正的阳性。</p></li>
<li><p>对于每个IoU阈值，取所有80个类别的ap的平均值。</p></li>
<li><p>最后，通过平均每个IoU阈值计算的AP值来计算总体AP。</p></li>
</ol>
<h3 id="非极大值抑制nms">3.3 非极大值抑制(NMS)</h3>
<p>非最大抑制(NMS)是一种用于目标检测算法的后处理技术，目的是减少重叠边界框的数量，提高整体检测质量。目标检测算法通常会在同一目标周围生成多个具有不同置信度分数的边界框。NMS过滤掉冗余和不相关的边界框，只保留最准确的边界框。算法1描述了这个过程。图3显示了包含多个重叠边界框的对象检测模型的典型输出和NMS后的输出。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095103611.png" alt="image-20231214095103611" style="zoom:50%;" /></p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095200651.png" alt="image-20231214095200651" style="zoom:50%;" /></p>
<h2 id="yolo-you-only-look-once">4 YOLO: You Only Look Once</h2>
<p>Joseph Redmon等人的YOLO发表于CVPR 2016<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>。它首次提出了一种实时的端到端目标检测方法。YOLO这个名字代表“你只看一次”，这是指它是一个能够通过网络的单次传递完成检测任务，而不是以前的方法，要么使用滑动窗口，然后使用分类器，需要在每张图像上运行数百或数千次，要么使用更高级的方法，将任务分为两步，其中第一步检测具有对象或区域建议的可能区域，第二步在建议上运行分类器。此外，YOLO使用更直接的基于回归的输出来预测检测输出，而Fast R-CNN<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>$使用两个单独的输出，概率分类和框坐标回归。</p>
<h3 id="yolov1-简介">4.1 YOLOv1 简介</h3>
<p>YOLOv1统一了目标检测步骤，同时检测所有的边界框。为了实现这一点，YOLO将输入图像划分为S × S网格，并预测同一类的B个边界框，以及每个网格元素对C个不同类的置信度。每个边界框预测由五个值组成:Pc, bx, by, bh, bw，其中Pc是框的置信度得分，反映模型对框中包含对象的置信度以及框的准确性。bx和by坐标是相对于网格单元格的框的中心，bh和bw是相对于完整图像的框的高度和宽度。YOLO的输出是S × S × (B × 5 + C)张量，可选地跟随非最大抑制(NMS)来去除重复检测。</p>
<p>在最初的YOLO论文中，作者使用了包含20个类(C = 20)的PASCAL VOC数据集[36];一个7 × 7的网格(S = 7)，每个网格元素最多2个类(B = 2)，给出7 × 7 × 30的输出预测。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095504303.png" alt="image-20231214095504303" style="zoom:50%;" /></p>
<p>图4显示了一个简化的输出向量，考虑了一个3乘3的网格、3个类，每个网格一个类代表8个值。在这个简化的情况下，YOLO的输出将是3 × 3 × 8。</p>
<p>YOLOv1在PASCAL VOC2007数据集上的平均精度(AP)为63.4。</p>
<h3 id="yolov1-框架">4.2 YOLOv1 框架</h3>
<p>YOLOv1架构包括24个卷积层，然后是两个完全连接的层，用于预测边界框坐标和概率。除最后一层使用线性激活函数外，所有层都使用漏整流线性单元激活<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>。受GoogLeNet<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>和Network in Network<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a>的启发，YOLO使用1 × 1卷积层来减少特征映射的数量，并保持相对较低的参数数量。作为激活层，表1描述了YOLOv1体系结构。作者还介绍了一个更轻的模型，称为Fast YOLO，由9个卷积层组成。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214095738606.png" alt="image-20231214095738606" style="zoom:50%;" /></p>
<h3 id="yolov1-训练">4.3 YOLOv1 训练</h3>
<p>作者使用ImageNet数据集以224 × 224的分辨率预训练了YOLO的前20层<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a>。然后，他们添加了随机初始化权重的最后四层，并使用PASCAL VOC 2007和VOC 2012数据集<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a>以448 × 448的分辨率对模型进行了精细调整，以增加细节，从而更准确地检测目标。</p>
<p>对于增强，作者使用最多20%输入图像大小的随机缩放和平移，以及随机曝光和饱和度，在HSV色彩空间中，上限因子为1.5。</p>
<p>YOLOv1使用了一个由多个和平方误差组成的损失函数，如图5所示。在损失函数中，<span class="math inline">\(\lambda_{\text{coord}} = 5\)</span>是给予边界框预测更多重要性的比例因子，<span class="math inline">\(λ_{\text{noobj}} = 0.5\)</span>是降低不包含对象的框的重要性的比例因子。</p>
<p>损失的前两项表示局部化损失;它计算预测的边界框位置<span class="math inline">\((x, y)\)</span>和大小<span class="math inline">\((w, h)\)</span>中的误差。请注意，这些误差仅在包含对象的框中计算(由<span class="math inline">\(\mathbb 1^{\text{obj}}_\text{ij}\)</span>表示)，只有在该网格单元中存在对象时才会受到惩罚。第三和第四个损失项表示信心损失;第三项测量在盒子中检测到对象时的置信误差(<span class="math inline">\(\mathbb 1^{\text{obj}}_\text{ij}\)</span>)，第四项测量在盒子中未检测到对象时的置信误差(<span class="math inline">\(\mathbb 1^{\text{noobj}}_\text{ij}\)</span>)。由于大多数盒子是空的，这个损失被<span class="math inline">\(\lambda_{\text{noobj}}\)</span>项加权。最后一个损失分量是分类损失，它仅在对象出现在单元格(<span class="math inline">\(\mathbb 1^{\text{obj}}_\text{ij}\)</span>)中时测量每个类别的类别条件概率的平方误差。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214100445282.png" alt="image-20231214100445282" style="zoom: 80%;" /></p>
<h3 id="yolov1-优点和不足">4.4 YOLOv1 优点和不足</h3>
<p>YOLO的简单架构，以及其新颖的全图像单次回归，使其比现有的目标检测器更快，从而实现实时性能。</p>
<p>然而，尽管YOLO的执行速度比任何目标检测器都快，但与Fast R-CNN等最先进的方法相比，定位误差更大<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>。造成这种限制的主要原因有三个:</p>
<ol type="1">
<li><p>它最多只能检测到网格单元中两个同类的物体，这限制了它预测附近物体的能力。</p></li>
<li><p>它很难预测训练数据中没有出现的长宽比物体。</p></li>
<li><p>由于下采样层，它从粗糙的目标特征中学习。</p></li>
</ol>
<h2 id="yolov2-better-faster-and-stronger">5 YOLOv2: Better, Faster, and Stronger</h2>
<p>YOLOv2由Joseph Redmon和Ali Farhadi在CVPR 2017上发表<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a>。它对原来的YOLO进行了一些改进，使其更好，保持相同的速度，也更强大-能够检测9000个类别!改进如下:</p>
<ol type="1">
<li><p><strong>batch normalization</strong> 所有卷积层的批处理归一化提高了收敛性，并作为正则化器减少过拟合。</p></li>
<li><p><strong>高分辨率的分类器</strong>。与YOLOv1一样，他们使用ImageNet在224 × 224的分辨率下对模型进行预训练。然而，这一次，他们在ImageNet上finetuned了10个epoch的模型，分辨率为448 × 448，提高了网络在更高分辨率输入下的性能。</p></li>
<li><p><strong>完全卷积</strong>。他们去掉了密集的层，使用了一个完全卷积的架构。</p></li>
<li><p><strong>使用anchor boxes来预测边界框</strong>。它们使用一组先验框或锚框，锚框是具有预定义形状的框，用于匹配对象的原型形状，如图6所示。为每个网格单元定义多个锚框，系统预测每个锚框的坐标和类。网络输出的大小与每个网格单元的锚盒数量成正比。</p></li>
</ol>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214104432020.png" alt="image-20231214104432020" style="zoom:50%;" /></p>
<ol start="5" type="1">
<li><p><strong>聚类中心</strong>。选择好的先验框有助于网络学习预测更准确的边界框。作者在训练边界盒上运行k-means聚类来找到好的先验。他们选择了五个先前的盒子，在召回率和模型复杂性之间进行了很好的权衡。</p></li>
<li><p><strong>直接位置预测</strong>。与其他预测偏移量的方法<span class="math inline">\(^{[45]}\)</span>不同，YOLOv2遵循相同的原理，预测相对于网格单元的位置坐标。网络为每个单元格预测5个边界框，每个边界框有5个值tx、ty、tw、th和to，其中to相当于YOLOv1中的P c，最终得到边界框坐标，如图7所示。</p></li>
</ol>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214104903836.png" alt="image-20231214104903836" style="zoom:50%;" /></p>
<ol start="7" type="1">
<li><strong>细粒度特性</strong>。与YOLOv1相比，YOLOv2去掉一个池化层，得到416 × 416输入图像的13 × 13输出特征图或网格。YOLOv2还使用了一个直通层，该层采用26 × 26 × 512的特征映射，并通过将相邻的特征堆叠到不同的通道中来重新组织而不是通过空间子采样丢失它们。这将生成13 × 13 × 2048个通道维度的特征图，并与低分辨率的13 × 13 × 1024个特征图进行连接，得到13 × 13 × 3072个特征图。</li>
<li><strong>多尺度的训练</strong>。由于YOLOv2不使用全连接层，因此输入可以是不同的大小。为了使YOLOv2对不同的输入大小具有鲁棒性，作者随机训练模型，每10批次改变输入大小——从320 × 320到608 × 608。</li>
</ol>
<p>通过所有这些改进，YOLOv2在PASCAL VOC2007数据集上的平均精度(AP)达到78.6%，而YOLOv1的平均精度为63.4%。</p>
<h3 id="yolov2-框架">5.1 YOLOv2 框架</h3>
<p>YOLOv2使用的骨干架构称为Darknet-19，包含19个卷积层和5个maxpooling层。与YOLOv1的架构类似，它的灵感来自于Network in Network<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a>，在3 × 3之间使用1 × 1的卷积来减少参数的数量。此外，如上所述，他们使用批归一化来正则化和帮助收敛。</p>
<p>表2显示了带有目标检测头的整个Darknet-19主干。使用PASCAL VOC数据集时，YOLOv2预测5个边界框，每个边界框有5个值和20个类。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214105332256.png" alt="image-20231214105332256" style="zoom:80%;" /></p>
<p>分类头将最后四个卷积层替换为一个包含1000个过滤器的单个卷积层，然后是一个全局平均池化层和一个Softmax。</p>
<h3 id="yolo9000-is-a-stronger-yolov2">5.2 YOLO9000 is a stronger YOLOv2</h3>
<p>作者在同一篇文章中介绍了一种训练联合分类和检测的方法。它使用COCO<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a>中的检测标记数据学习边界框坐标和ImageNet中的分类数据，以增加可检测的类别数量。在训练过程中，他们将两个数据集结合起来，这样当使用检测训练图像时，它会反向传播检测网络，当使用分类训练图像时，它会反向传播体系结构的分类部分。结果是一个能够检测超过9000个类别的YOLO模型，因此名称为YOLO9000。</p>
<h2 id="yolov3">6 YOLOv3</h2>
<p>YOLOv3<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>由Joseph Redmon和Ali Farhadi于2018年在ArXiv上发表。它包括重大变化和更大的架构，以与最先进的技术相媲美，同时保持实时性能。在下文中，我们描述了与YOLOv2相关的更改。</p>
<ol type="1">
<li><p><strong>边界框预测</strong>。与YOLOv2一样，该网络为每个边界框tx、ty、tw和th预测四个坐标;然而，这一次，YOLOv3使用逻辑回归预测每个边界框的对象得分。与ground truth重叠度最高的锚框得分为1，其余锚框得分为0。与Faster R-CNN<span class="math inline">\(^{[45]}\)</span>不同，YOLOv3只为每个ground truth对象分配一个锚框。同样，如果没有给对象分配锚盒，只会造成分类损失，不会造成定位损失和置信度损失。</p></li>
<li><p><strong>类的预测</strong>。他们没有使用softmax进行分类，而是使用二元交叉熵来训练独立的逻辑分类器，并将问题作为多标签分类。这种变化允许为同一个框分配多个标签，这可能发生在一些具有重叠标签的复杂数据集<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a>上。例如，同一个对象可以是Person和Man。</p></li>
<li><p><strong>新的主干</strong>。YOLOv3具有更大的特征提取器，由53个带有残差连接的卷积层组成。第6.1节更详细地描述了该体系结构。</p></li>
<li><p><strong>空间金字塔池化</strong>(SPP)虽然在论文中没有提到，但作者还在主干中添加了一个修改的SPP块<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a>，该块连接多个最大池化输出而不进行子采样(stride = 1)，每个块具有不同的内核大小k × k，其中k = 1,5,9,13允许更大的感受野。这个版本被称为YOLOv3-spp，是性能最好的版本，AP50提高了2.7%。</p></li>
<li><p><strong>多尺度预测</strong>。与特征金字塔网络<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>类似，YOLOv3在三个不同的尺度上预测三个盒子。第6.2节详细描述了多尺度预测机制。</p></li>
<li><p><strong>边界框先验</strong>。与YOLOv2一样，作者也使用k-means来确定锚框的边界框先验。不同之处在于，在YOLOv2中，他们为每个单元格总共使用了五个先验框，而在YOLOv3中，他们为三个不同的尺度使用了三个先验框。</p></li>
</ol>
<h3 id="yolov3-架构">6.1 YOLOv3 架构</h3>
<p>YOLOv3中呈现的架构主干称为Darknet-53。它用跨行卷积替换了所有的最大池化层，并添加了残差连接。总的来说，它包含53个卷积层。图8显示了体系结构的细节。</p>
<p>Darknet-53骨干网获得与ResNet-152相当的Top-1和Top-5精度，但几乎快了2倍。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214110812922.png" alt="image-20231214110812922" style="zoom:80%;" /></p>
<h3 id="yolov3-多尺度预测">6.2 YOLOv3 多尺度预测</h3>
<p>除了更大的架构外，YOLOv3的一个基本特征是多尺度预测，即在多个网格大小下进行预测。这有助于获得更精细的细节框，并显着提高了对小物体的预测，这是以前版本的YOLO的主要弱点之一。</p>
<p>图9所示的多尺度检测架构的工作原理如下:标记为y1的第一个输出相当于YOLOv2的输出，其中一个13 × 13的网格定义了输出。第二个输出y2是将Darknet-53的(Res × 4)之后的输出与(Res × 8)之后的输出拼接而成。feature map的大小不同，分别是13 × 13和26 × 26，所以在拼接之前需要进行上采样操作。最后，使用上采样操作，第三个输出y3将26 × 26特征映射与52 × 52特征映射连接起来。</p>
<p>对于具有80个类别的COCO数据集，每个尺度提供一个形状为N × N ×[3x(4+1+80)]的输出张量，其中N ×N为特征图(或网格单元)的大小，3表示每个单元的框数，4+1包括四个坐标和对象得分。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214112806628.png" alt="image-20231214112806628" style="zoom:80%;" /></p>
<h3 id="yolov3-结果">6.3 YOLOv3 结果</h3>
<p>当YOLOv3发布时，目标检测的基准已经从PASCAL VOC改为Microsoft COCO<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a>。因此，从这里开始，所有的yolo都在MS COCO数据集中进行评估。在20 FPS下，YOLOv3-spp的平均精度AP为36.2%，AP50为60.6%，达到了当时最先进的水平，速度提高了2倍。</p>
<h2 id="backbone-neck-and-head">7 Backbone Neck and Head</h2>
<p>此时，目标探测器的结构开始被描述为三个部分:Backbone、Neck和Head。图10显示了一个高级的Backbone、Neck和Head。</p>
<p>主干负责从输入图像中提取有用的特征。它通常是在大规模图像分类任务上训练的卷积神经网络(CNN)，例如ImageNet。主干捕获不同尺度的分层特征，较低级的特征(如边缘和纹理)在较早的层中提取，较高级的特征(如对象部分和语义信息)在较深的层中提取。</p>
<p>颈部是连接主干和头部的中间部分。它对主干提取的特征进行聚合和细化，往往侧重于增强不同尺度的空间和语义信息。</p>
<p>颈部可能包括额外的卷积层、特征金字塔网络(FPN)<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a>或其他机制来改善特征的表示。</p>
<p>头部是物体检测器的最终组成部分;它负责根据脊柱和颈部提供的特征做出预测。它通常由一个或多个特定于任务的子网组成，这些子网执行分类、定位以及最近的实例分割和姿态估计。头部处理颈部提供的特征，为每个候选对象生成预测。最后，一个后处理步骤，如非最大抑制(NMS)，过滤掉重叠的预测，只保留最可靠的检测。</p>
<p>在YOLO模型的其余部分中，我们将使用Backbone、Neck和Head来描述架构。</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214113148856.png" alt="image-20231214113148856" style="zoom:50%;" /></p>
<h2 id="yolov4">8 YOLOv4</h2>
<p>两年过去了，并没有新的YOLO版本。直到2020年4月，Alexey Bochkovskiy, Chien-Y ao Wang, Hong-Y yuan Mark Liao在ArXiv发表了YOLOv4的论文<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a>。起初，不同的作者提出了一个新的“官方”版本的YOLO，这让人感到奇怪;然而，yolo4保持了相同的YOLO理念——实时、开源、单镜头和DarkNet框架——并且改进是如此令人满意，社区迅速接受了这个版本作为官方的yolo4。</p>
<p>YOLOv4试图找到最佳的平衡，尝试了许多被归类为bag-of-freebies和bag-of-specials的变化。Bag-of-freebies是指只改变训练策略，增加训练成本，但不增加推理时间的方法，最常见的是数据增强。另一方面，bag-of-specials是稍微增加推理成本但显著提高准确率的方法。这些方法的例子包括扩大感受野<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a><a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a><a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a>，结合特征<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a><a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a><a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a><a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a>和后处理<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a><a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a><a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a><a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a>等。</p>
<p>我们将YOLOv4的主要变化总结为以下几点:</p>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214113550393.png" alt="image-20231214113550393" style="zoom:80%;" /></p>
<ol type="1">
<li><p>具有(BoS)集成的增强架构。作者尝试了多种骨干网架构，如ResNeXt50<a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a>、EfficientNet-B3<a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a>和Darknet-53。性能最好的架构是对Darknet-53的修改，采用跨级部分连接(CSPNet)<span class="math inline">\(^{[61]}\)</span>，以Mish激活函数<a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a>作为主干(见图11)。对于颈部，他们使用了来自YOLOv3- SPP的修改版本的空间金字塔池(SPP)<a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a>和与YOLOv3一样的多尺度预测，但使用了修改版本的路径聚合网络(PANet)<a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a>$而不是FPN以及修改的空间注意模块(SAM)<a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a>。最后，对于探测头，他们使用YOLOv3中的anchor。因此，将模型命名为CSPDarknet53-PANet-SPP。添加到Darknet-53中的跨级部分连接(CSP)有助于减少模型的计算量，同时保持相同的精度。在YOLOv3-spp中，SPP块在不影响推理速度的情况下增加了接受野。PANet的修改版本将这些特性连接起来，而不是像在PANet的原始文件中那样添加它们。</p></li>
<li><p>整合(BoF)的高级培训方法。除了常规的增强，如随机亮度、对比度、缩放、裁剪、翻转和旋转，作者还实现了马赛克增强，将四张图像合并为一张图像，允许检测其通常上下文之外的对象，并减少了批量规范化对大型mini-batch大小的需求。对于正则化，他们使用DropBlock<a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>作为Dropout<a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a>的替代品，但用于卷积神经网络以及类标签平滑<a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a><a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a>。对于检测器，他们增加了CIoU损耗<a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a>和Cross mini-bath normalization (CmBN)，以便从整个批次收集统计数据，而不是像常规批次归一化那样从单个小批次收集统计数据<a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a>。</p></li>
<li><p>自我对抗训练(SAT)。为了使模型对扰动更具鲁棒性，对输入图像执行对抗性攻击，以创建一个欺骗，即ground truth对象不在图像中，但保持原始标签以检测正确的对象。</p></li>
<li><p>遗传算法的超参数优化。为了找到用于训练的最优超参数，他们在前10%的周期中使用遗传算法，并使用余弦退火调度器<a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a>来改变训练期间的学习率。它开始缓慢地降低学习率，然后在训练过程中快速降低，最后略有降低。</p></li>
</ol>
<figure>
<img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214160738192.png" alt="image-20231214160738192" /><figcaption aria-hidden="true">image-20231214160738192</figcaption>
</figure>
<p>在MS COCO数据集测试开发2017上进行评估，YOLOv4在NVIDIA V100上以超过50 FPS的速度实现了43.5%的AP和65.7%的AP50。</p>
<h2 id="yolov5">9 YOLOv5</h2>
<p>YOLOv5<a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a>于2020年由Glenn Jocher在YOLOv4发布几个月后发布。在撰写本文时，还没有关于YOLOv5的科学论文，但从代码中，我们知道它使用了YOLOv4部分中描述的许多改进，主要区别在于它是在Pytorch而不是Darknet中开发的。</p>
<p>YOLOv5是开源的，由Ultralytics积极维护，有250多个贡献者，并且经常有新的改进。YOLOv5易于使用、训练和部署。Ultralytics提供iOS和Android的移动版本，以及许多标签、培训和部署的集成。</p>
<p>YOLOv5提供5个缩放版本:YOLOv5n (nano)、YOLOv5s (small)、YOLOv5m (medium)、YOLOv5l (large)、YOLOv5x (extra large)。</p>
<p>撰写本文时发布的YOLOv5版本是v7.0，其中包括能够分类和实例分割的YOLOv5版本。</p>
<p>在MS COCO数据集test-dev 2017上进行评估，YOLOv5x在图像尺寸为640像素时实现了50.7%的AP。</p>
<p>使用batchsize=32，它可以在NVIDIA V100上实现200 FPS的速度。使用1536像素的更大输入尺寸，YOLOv5实现了55.8%的AP。</p>
<h2 id="scaled-yolov4">10 Scaled-YOLOv4</h2>
<p>在YOLOv4发布一年后，同一作者在CVPR 2021上提出了Scaled-YOLOv4<a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a>。与YOLOv4不同的是，Scaled YOLOv4是在Pytorch而不是Darknet中开发的。主要的新颖之处是引入了按比例放大和按比例缩小的技术。扩大规模意味着以较低的速度为代价来提高准确性;另一方面，按比例缩小需要生成一个以牺牲准确性为代价提高速度的模型。此外，按比例缩小的模型需要更少的计算能力，并且可以在嵌入式系统上运行。</p>
<p>这个按比例缩小的架构被称为YOLOv4-tiny;它是为低端gpu设计的，可以在Jetson TX2上运行46 FPS或在RTX2080Ti上运行440 FPS，在MS COCO上实现22%的AP。</p>
<p>按比例扩大的模型体系结构称为YOLOv4-large，其中包括三种不同的尺寸P5、P6和P7。该架构是为云GPU设计的，并实现了最先进的性能，超过了所有以前的模型<a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a><a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a><a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a>，在MS COCO上具有56%的AP。</p>
<h2 id="yolor">11 YOLOR</h2>
<p>YOLOR<a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a>由YOLOv4的同一个研究小组于2021年5月在ArXiv上发表。它代表You Only Learn One Representation。在本文中，作者采用了一种不同的方法;他们开发了一种多任务学习方法，旨在通过学习一般表示和使用子网络创建任务特定表示来为各种任务(例如，分类，检测，姿态估计)创建单个模型。鉴于传统的联合学习方法通常会导致次优特征生成，YOLOR旨在通过对神经网络的隐式知识进行编码以应用于多个任务来克服这一问题，类似于人类如何利用过去的经验来解决新问题。结果表明，在神经网络中引入隐式知识对所有任务都有好处。</p>
<p>在MS COCO数据集测试开发2017上进行评估，在NVIDIA V100上30 FPS下，YOLOR的AP为55.4%，AP50为73.3%。</p>
<h2 id="yolox">12 YOLOX</h2>
<p>YOLOX<a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a>由旷视科技的研究团队于2021年7月在ArXiv上发表。在Pytorch中开发并使用来自Ultralytics的YOLOV3作为起点，它有五个主要的变化:无锚点架构，多个阳性，解耦头部，高级标签分配和强增强。它在2021年时中取得了最先进的成绩，在速度和精度之间取得了最佳平衡，在Tesla V100上，AP为50.1%，FPS为68.9%。下面，我们将介绍YOLOX相对于YOLOv3的五个主要变化:</p>
<ol type="1">
<li><p>Anchor-free。自YOLOv2以来，所有后续的YOLO版本都是基于锚点的检测器。YOLOX受无锚点最先进的目标探测器(如CornerNet<a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a>、CenterNet<a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a>和FCOS<a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a>)的启发，回归到无锚点架构，简化了训练和解码过程。与YOLOv3基线相比，无锚点使AP增加了0.9个点。</p></li>
<li><p>多积极的方面。为了弥补锚点缺失造成的巨大不平衡，作者使用中心抽样<a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a>，他们将中心3 × 3的区域指定为正区域。这种方法使AP提高了2.1分。</p></li>
<li><p>解耦。文献<a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a><a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a>表明，分类置信度与定位精度之间可能存在不一致。因此，YOLOX将两者分为两个头(如图12所示)，一个用于分类任务，另一个用于回归任务，将AP提高了1.1点，加快了模型收敛速度。</p></li>
<li><p>高级标签分配。文献<a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a>表明，当多个物体的盒子重叠时，ground truth标签分配可能存在歧义，并将分配过程表述为最优传输(Optimal Transport, OT)问题。YOLOX受到这项工作的启发，提出了一个简化的版本，叫做simOTA。这个改动使AP增加了2.3点。</p></li>
<li><p>强大的扩增。YOLOX使用MixUP<a href="#fn94" class="footnote-ref" id="fnref94" role="doc-noteref"><sup>94</sup></a>和Mosaic增强。作者发现，使用这些增强后，ImageNet预训练不再有益。强大的增强使AP增加2.4点。</p></li>
</ol>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214205859709.png" alt="image-20231214205859709" style="zoom:80%;" /></p>
<h2 id="yolov6">13 YOLOv6</h2>
<p>YOLOv6<a href="#fn95" class="footnote-ref" id="fnref95" role="doc-noteref"><sup>95</sup></a>由美团视觉AI部门于2022年9月在ArXiv上发表。与YOLOv4和YOLOv5类似，它为工业应用提供了不同尺寸的各种型号。遵循achor-free的方法<a href="#fn96" class="footnote-ref" id="fnref96" role="doc-noteref"><sup>96</sup></a><a href="#fn97" class="footnote-ref" id="fnref97" role="doc-noteref"><sup>97</sup></a>的趋势，YOLOv6采用了无锚点检测器。该模型的主要新颖之处总结如下:</p>
<ol type="1">
<li><p>一种基于RepVGG<a href="#fn98" class="footnote-ref" id="fnref98" role="doc-noteref"><sup>98</sup></a>的新型骨干网，称为EfficientRep，它比以前的YOLO骨干网使用更高的并行性。对于颈部，他们使用了PAN<a href="#fn99" class="footnote-ref" id="fnref99" role="doc-noteref"><sup>99</sup></a>，对于较大的模型，他们使用了RepBlocks<a href="#fn100" class="footnote-ref" id="fnref100" role="doc-noteref"><sup>100</sup></a>或CSPStackRep<a href="#fn101" class="footnote-ref" id="fnref101" role="doc-noteref"><sup>101</sup></a> block。受YOLOX的启发，他们开发了一个高效的分离头。</p></li>
<li><p>使用TOOD<a href="#fn102" class="footnote-ref" id="fnref102" role="doc-noteref"><sup>102</sup></a>中引入的任务对齐学习方法进行标签分配。</p></li>
<li><p>新的分类和回归损失。他们使用了变焦损失分类<a href="#fn103" class="footnote-ref" id="fnref103" role="doc-noteref"><sup>103</sup></a>和SIoU <a href="#fn104" class="footnote-ref" id="fnref104" role="doc-noteref"><sup>104</sup></a>/GIoU<a href="#fn105" class="footnote-ref" id="fnref105" role="doc-noteref"><sup>105</sup></a>回归损失。</p></li>
<li><p>回归和分类任务的自蒸馏策略。</p></li>
<li><p>一种使用RepOptimizer<a href="#fn106" class="footnote-ref" id="fnref106" role="doc-noteref"><sup>106</sup></a>和通道式蒸馏<a href="#fn107" class="footnote-ref" id="fnref107" role="doc-noteref"><sup>107</sup></a>的检测量化方案，有助于实现更快的检测器。</p></li>
</ol>
<p>在MS COCO数据集测试开发2017上进行评估，YOLOv6-L在NVIDIA Tesla T4上以50 FPS左右的速度实现了52.5%的AP和70%的AP50。</p>
<h2 id="yolov7">14 YOLOv7</h2>
<p>YOLOv7<a href="#fn108" class="footnote-ref" id="fnref108" role="doc-noteref"><sup>108</sup></a>于2022年7月由YOLOv4和YOLOR的同一作者发表在ArXiv上。当时，它在速度和精度上超过了所有已知的物体探测器，在5 FPS到160 FPS的范围内。与YOLOv4一样，它只使用MS COCO数据集进行训练，没有预先训练主干。YOLOv7提出了几个架构变化和一系列bag-of-freebies，在不影响推理速度的情况下提高了准确性，只影响了训练时间。</p>
<p>The architecture changes of YOLOv7 are:</p>
<ol type="1">
<li><strong>扩展高效层聚合网络(E-ELAN)</strong>。ELAN<a href="#fn109" class="footnote-ref" id="fnref109" role="doc-noteref"><sup>109</sup></a>是一种策略，通过控制最短最长的梯度路径，允许深度模型更有效地学习和收敛。YOLOv7提出的E-ELAN适用于具有无限堆叠计算块的模型。E-ELAN通过洗牌和合并基数来结合不同群体的特征，在不破坏原有梯度路径的情况下增强网络的学习能力。</li>
<li><strong>基于连接的模型的模型缩放</strong>。缩放通过调整模型的某些属性来生成不同大小的模型。YOLOv7的体系结构是基于串联的体系结构，其中标准缩放技术(如深度缩放)会导致过渡层的输入通道和输出通道之间的比率变化，从而导致模型的硬件使用减少。YOLOv7提出了一种新的基于串联的模型缩放策略，该策略将块的深度和宽度以相同的因子进行缩放，以保持模型的最优结构。</li>
</ol>
<p>The bag-of-freebies used in YOLOv7 include:</p>
<ol type="1">
<li><p><strong>计算后的重新参数化卷积</strong>。与YOLOv6一样，YOLOv7的架构也受到了重参数化卷积(RepConv)的启发<a href="#fn110" class="footnote-ref" id="fnref110" role="doc-noteref"><sup>110</sup></a>。然而，他们发现RepConv中的身份连接破坏了ResNet<a href="#fn111" class="footnote-ref" id="fnref111" role="doc-noteref"><sup>111</sup></a>中的残差和DenseNet<a href="#fn112" class="footnote-ref" id="fnref112" role="doc-noteref"><sup>112</sup></a>中的连接。出于这个原因，他们删除了身份连接并将其称为RepConvN。</p></li>
<li><p><strong>粗标签分配辅助head和细标签分配主head</strong>。lead head负责最终产出，auxiliary head协助训练。</p></li>
<li><p><strong>批量规范化的conv-bn-activation</strong>。这将批归一化的均值和方差集成到推理阶段卷积层的偏差和权重中。</p></li>
<li><p>在YOLOR中启发的隐性知识<a href="#fn113" class="footnote-ref" id="fnref113" role="doc-noteref"><sup>113</sup></a>。</p></li>
<li><p>指数移动平均作为最终的推理模型。</p></li>
</ol>
<p>YOLOv7与由同一作者开发的以前的YOLO模型相比的增强。</p>
<p>与YOLOv4相比，YOLOv7的参数减少了75%，计算量减少了36%，同时平均精度(AP)提高了1.5%。</p>
<p>与YOLOv4-tiny相比，YOLOv7-tiny在保持相同AP的情况下，分别减少了39%和49%的参数和计算量。</p>
<p>最后，与YOLOR相比，YOLOv7分别减少了43%和15%的参数数量和计算量，同时AP略有增加0.4%</p>
<p>在MS COCO数据集test-dev 2017上进行评估，YOLOv7-E6在NVIDIA V100上以50 FPS的速度，以1280像素的输入大小实现了55.9%的AP和73.5%的AP50。</p>
<h2 id="damo-yolo">15 DAMO-YOLO</h2>
<p>DAMO-YOLO<a href="#fn114" class="footnote-ref" id="fnref114" role="doc-noteref"><sup>114</sup></a>由阿里巴巴集团于2022年11月在ArXiv发表。DAMO-YOLO受到当前技术的启发，包括以下内容:</p>
<ol type="1">
<li><p>神经结构搜索(NAS)。他们使用阿里巴巴开发的MAE-NAS<a href="#fn115" class="footnote-ref" id="fnref115" role="doc-noteref"><sup>115</sup></a>方法自动寻找高效架构。</p></li>
<li><p>large neck。受GiraffeDet<a href="#fn116" class="footnote-ref" id="fnref116" role="doc-noteref"><sup>116</sup></a>、CSPNet<a href="#fn117" class="footnote-ref" id="fnref117" role="doc-noteref"><sup>117</sup></a>和ELAN<a href="#fn118" class="footnote-ref" id="fnref118" role="doc-noteref"><sup>118</sup></a>的启发，作者设计了一种可以实时工作的颈部，称为Efficient-RepGFPN。</p></li>
<li><p>samll head。作者发现，大颈和小颈的性能更好，他们只留下一个线性层用于分类，一个用于回归。他们称这种方法为ZeroHead。</p></li>
<li><p>AlignedOTA 标签分配。动态标签分配方法，如OTA<a href="#fn119" class="footnote-ref" id="fnref119" role="doc-noteref"><sup>119</sup></a>和TOOD<a href="#fn120" class="footnote-ref" id="fnref120" role="doc-noteref"><sup>120</sup></a>，由于其相对于静态方法的显著改进而受到欢迎。然而，分类和回归之间的不一致仍然是一个问题，部分原因是分类和回归损失之间的不平衡。为了解决这一问题，他们的AlignOTA方法将焦损失<a href="#fn121" class="footnote-ref" id="fnref121" role="doc-noteref"><sup>121</sup></a>引入到分类成本中，并使用预测和ground truth box的IoU作为软标签，可以为每个目标选择对齐的样本，从全局角度解决问题。</p></li>
<li><p>知识蒸馏。他们提出的策略包括两个阶段:第一阶段是教师指导学生，第二阶段是学生自主微调。此外，他们在蒸馏方法中加入了两个增强功能:Align模块，它使学生的特征与教师的特征具有相同的分辨率，以及Channel-wise Dynamic Temperature，它使教师和学生的特征标准化，以减少实际值差异的影响。</p></li>
</ol>
<p>作者生成了名为DAMO-YOLO-Tiny/Small/Medium的缩放模型，最佳模型在NVIDIA V100上以233 FPS的速度实现了50.0%的AP</p>
<h2 id="yolov8">16 YOLOv8</h2>
<p>YOLOv8<a href="#fn122" class="footnote-ref" id="fnref122" role="doc-noteref"><sup>122</sup></a>于2023年1月由开发YOLOv5的公司Ultralytics发布。因为在撰写本文时，还没有关于YOLOv8的论文，所以我们需要深入了解与其他YOLO版本相比的架构决策。按照目前的趋势，YOLOv8是无锚的，减少了框预测的数量，加快了非最大印象(NMS)。此外，YOLOv8在训练过程中使用马赛克增强;然而，因为已经发现，如果在整个训练过程中使用这种增强可能是有害的，所以在最后十个时代禁用它。</p>
<p>YOLOv8可以从命令行界面(CLI)运行，也可以作为PIP包安装。此外，它还具有用于标记、培训和部署的多个集成。</p>
<p>YOLOv8提供了5个缩放版本:YOLOv8n(nano)、YOLOv8s(small)、YOLOv8m(medium)、YOLOv8l(large)和YOLOv8x(extra large)。</p>
<p>在MS COCO数据集测试开发2017上进行评估，YOLOv8x在图像尺寸为640像素时实现了53.9%的AP(相比之下，在相同输入尺寸下，YOLOv5的AP为50.7%)，在NVIDIA A100和TensorRT上的速度为280 FPS。</p>
<h2 id="pp-yolo-pp-yolov2和pp-yoloe">17 PP-YOLO, PP-YOLOv2和PP-YOLOE</h2>
<p>PP-YOLO模型已经与我们描述的YOLO模型并行发展。但是，我们决定将它们分组在一个部分中，因为它们从YOLOv3开始，并且在以前的PP-YOLO版本的基础上逐渐改进。尽管如此，这些模型对YOLO的演变产生了影响。PP-YOLO<a href="#fn123" class="footnote-ref" id="fnref123" role="doc-noteref"><sup>123</sup></a>与YOLOv4和YOLOv5类似，是基于YOLOv3的。该研究于2020年7月由百度公司的研究人员发表在ArXiv上。</p>
<p>作者使用了PaddlePaddle<a href="#fn124" class="footnote-ref" id="fnref124" role="doc-noteref"><sup>124</sup></a>深度学习平台，因此其名称为PP。遵循我们从YOLOv4开始看到的趋势，PP-YOLO增加了十个现有的技巧来提高探测器的准确性，保持速度不变。根据作者的说法，本文并不是要介绍一种新的目标检测器，而是要展示如何一步一步地构建一个更好的检测器。PP-YOLO使用的大多数技巧与yolo4中使用的技巧不同，重叠的技巧使用不同的实现。PP-YOLO对YOLOv3的变化如下:</p>
<ol type="1">
<li><p>ResNet50-vd骨干网络在最后阶段用一个增强了可变形卷积的架构取代DarkNet-53骨干网<a href="#fn125" class="footnote-ref" id="fnref125" role="doc-noteref"><sup>125</sup></a>，以及一个蒸馏的预训练模型，该模型在ImageNet上具有更高的分类精度。这个架构被称为ResNet5-vd-dcn。</p></li>
<li><p>更大的批处理规模以提高训练的稳定性，它们从64增加到192，同时更新了训练计划和学习率。</p></li>
<li><p>维护训练参数的移动平均线，并使用它们代替最终训练值。</p></li>
<li><p>DropBlock只应用于FPN。</p></li>
<li><p>在另一个分支中，将IoU损失与边界盒回归的l1损失一起添加。</p></li>
<li><p>添加了IoU预测分支来测量定位精度以及IoU感知损失。在推理过程中，YOLOv3将分类概率和客观评分相乘计算最终检测结果，PP-YOLO还将预测IoU相乘考虑定位精度。</p></li>
<li><p>采用类似于YOLOv4的网格敏感方法改进网格边界的边界框中心预测。</p></li>
<li><p>采用矩阵式NMS<a href="#fn126" class="footnote-ref" id="fnref126" role="doc-noteref"><sup>126</sup></a>，可以并行运行，比传统NMS运行速度更快。</p></li>
<li><p>CoordConv<a href="#fn127" class="footnote-ref" id="fnref127" role="doc-noteref"><sup>127</sup></a>用于FPN的1 × 1卷积，在检测头的第一个卷积层上。CoordConv允许网络学习平移不变性，提高检测定位。</p></li>
<li><p>空间金字塔池化仅在顶部特征图上使用，以增加主干的接受野。</p></li>
</ol>
<h3 id="pp-yolo-数据增强和预处理">17.1 PP-YOLO 数据增强和预处理</h3>
<p>PP-YOLO采用了以下增强和预处理:</p>
<ol type="1">
<li><p>混合训练<a href="#fn128" class="footnote-ref" id="fnref128" role="doc-noteref"><sup>128</sup></a>，从<span class="math inline">\(Beta(\alpha,\beta)\)</span>分布中抽样权值，其中α = 1.5， β = 1.5。</p></li>
<li><p>随机颜色失真。</p></li>
<li><p>随机的扩大。</p></li>
<li><p>随机裁剪和随机翻转的概率为0.5。</p></li>
<li><p>RGB通道z-score归一化，均值为[0.485,0.456,0.406]，标准差为[0.229,0.224,0.225]。</p></li>
<li><p>从[320,352,384,416,448,480,512,544,576,608]中均匀绘制多个图像大小。</p></li>
</ol>
<h3 id="results">17.2 results</h3>
<p>在MS COCO数据集测试开发2017上进行评估，PP-YOLO在NVIDIA V100上以73 FPS的速度实现了45.9%的AP和65.2%的AP50。</p>
<h3 id="pp-yolov2">17.3 PP-YOLOv2</h3>
<p>PP-YOLOv2<a href="#fn129" class="footnote-ref" id="fnref129" role="doc-noteref"><sup>129</sup></a>于2021年4月在ArXiv上发表，并对PP-YOLO进行了四项改进，将NVIDIA V100上69 FPS的AP性能从45.9%提高到49.5%。PP-YOLOv2对PP-YOLO的修改如下:</p>
<ol type="1">
<li><p>骨干网从ResNet50修改为ResNet101。</p></li>
<li><p>PAN (Path aggregation network)，替代类似于YOLOv4的FPN。</p></li>
<li><p>Mish激活函数。与YOLOv4和YOLOv5不同的是，它们仅在检测颈部应用了mish激活功能，使主干与ReLU保持不变。</p></li>
<li><p>较大的输入尺寸有助于提高小对象的性能。他们将最大的输入大小从608扩展到768，并将每个GPU的批处理大小从24张减少到12张。输入大小从[320,352,384,416,448,480,512,544,576,608,640,672,704,736,768]中均匀抽取。</p></li>
<li><p>修改后的IoU感知分支。他们修改了IoU感知损失计算的计算，使用软标签格式代替软权重格式。</p></li>
</ol>
<h3 id="pp-yoloe">17.4 PP-YOLOE</h3>
<p>PP-YOLOE<a href="#fn130" class="footnote-ref" id="fnref130" role="doc-noteref"><sup>130</sup></a>于2022年3月在ArXiv上发表。它对PP-YOLOv2进行了改进，在NVIDIA V100上以78.1 FPS实现了51.4% AP的性能。PP-YOLOE对PP-YOLOv2的主要变化有:</p>
<ol type="1">
<li><p>Anchor-free。PP-YOLOE遵循<a href="#fn131" class="footnote-ref" id="fnref131" role="doc-noteref"><sup>131</sup></a><a href="#fn132" class="footnote-ref" id="fnref132" role="doc-noteref"><sup>132</sup></a><a href="#fn133" class="footnote-ref" id="fnref133" role="doc-noteref"><sup>133</sup></a><a href="#fn134" class="footnote-ref" id="fnref134" role="doc-noteref"><sup>134</sup></a>作品所引领的时代潮流，采用无锚式架构。</p></li>
<li><p>新的 backbone 和 neck。受TreeNet<a href="#fn135" class="footnote-ref" id="fnref135" role="doc-noteref"><sup>135</sup></a>的启发，作者使用结合残差和密集连接的RepResBlocks修改了脊柱和颈部的结构。</p></li>
<li><p>任务一致性学习(TAL)。YOLOX首先提出了任务错位问题，即分类置信度和定位精度在所有情况下都不一致。为了减少这个问题，PP-YOLOE实现了TOOD<a href="#fn136" class="footnote-ref" id="fnref136" role="doc-noteref"><sup>136</sup></a>中提出的TAL，其中包括动态标签分配和任务对齐损失。</p></li>
<li><p>高效任务对齐头(ET-head)。与YOLOX的分类和定位磁头分离不同，PP-YOLOE使用基于ood的单个磁头来提高速度和准确性。</p></li>
<li><p>变焦(VFL)和分布焦损失(DFL)。VFL<a href="#fn137" class="footnote-ref" id="fnref137" role="doc-noteref"><sup>137</sup></a>使用目标分数对阳性样本进行权重损失，对IoU高的样本给予更高的权重。这在训练期间优先考虑高质量的样本。同样，两者都使用IACS作为目标，允许对分类和定位质量进行联合学习，从而实现训练和推理的一致性。另一方面，DFL<a href="#fn138" class="footnote-ref" id="fnref138" role="doc-noteref"><sup>138</sup></a>将Focal Loss从离散标签扩展到连续标签，从而成功地优化了结合质量估计和类别预测的改进表示。这允许在实际数据中准确地描述灵活的分布，从而消除不一致的风险。</p></li>
</ol>
<p>与之前的YOLO版本一样，作者通过改变脊柱和颈部的宽度和深度来生成多个缩放模型。这些型号分别为PP-YOLOE-s(小型)、PP-YOLOE-m(中型)、PP-YOLOE-l(大型)和PP-YOLOE-x(超大型)。</p>
<h2 id="discussion">18 discussion</h2>
<p>本文检查了15个YOLO版本，从最初的YOLO模型到最新的YOLOv8。表4提供了所讨论的YOLO版本的概述。从这个表中，我们可以识别出几个关键模式:</p>
<ul>
<li><p>锚点:最初的YOLO模型相对简单，没有使用锚点，而目前的状态依赖于带锚点的两级探测器。YOLOv2加入了锚点，从而提高了边界框预测的准确性。这种趋势持续了5年，直到YOLOX推出了一种无锚的方法，取得了最先进的效果。从那时起，后续的YOLO版本已经放弃了锚的使用。</p></li>
<li><p>框架:最初，YOLO是使用Darknet框架开发的，随后的版本也紧随其后。然而，当Ultralytics将YOLOv3移植到PyTorch时，剩余的YOLO版本是使用PyTorch开发的，从而导致增强的激增。另一种使用的深度学习语言是PaddlePaddle，这是一个最初由百度开发的开源框架。</p></li>
<li><p>骨干:随着时间的推移，YOLO模型的骨干架构发生了重大变化。从包含简单卷积层和最大池化层的Darknet架构开始，后来的模型在YOLOv4中加入了跨阶段部分连接(CSP)，在YOLOv6和YOLOv7中加入了重新参数化，在DAMO-YOLO中加入了神经结构搜索。</p></li>
<li><p>性能:虽然YOLO模型的性能随着时间的推移有所提高，但值得注意的是，它们通常优先考虑速度和准确性的平衡，而不是仅仅关注准确性。这种权衡是YOLO框架的一个重要方面，它允许跨各种应用程序进行实时对象检测。</p></li>
</ul>
<p><img src="C:\Users\jyh\AppData\Roaming\Typora\typora-user-images\image-20231214214908034.png" alt="image-20231214214908034" style="zoom:80%;" /></p>
<h3 id="在速度和准确性之间进行权衡">18.1 在速度和准确性之间进行权衡</h3>
<p>YOLO系列目标检测模型一直专注于平衡速度和准确性，旨在提供实时性能而不牺牲检测结果的质量。随着YOLO框架在各种迭代中不断发展，这种权衡一直是一个反复出现的主题，每个版本都以不同的方式寻求优化这些相互竞争的目标。在最初的YOLO模型中，主要关注的是实现高速目标检测。该模型利用单个卷积神经网络(CNN)直接从输入图像中预测物体的位置和类别，从而实现实时处理。然而，这种对速度的强调导致了准确性的妥协，主要是在处理小对象或具有重叠边界框的对象时。</p>
<p>随后的YOLO版本引入了改进和增强，以解决这些限制，同时保持框架的实时功能。例如，YOLOv2 (YOLO9000)引入锚盒和穿透层来改善物体的定位，从而提高精度。此外，YOLOv3通过采用多尺度特征提取架构增强了模型的性能，从而可以在各种尺度上更好地检测目标。</p>
<p>随着YOLO框架的发展，速度和准确性之间的权衡变得更加微妙。像YOLOv4和YOLOv5这样的模型引入了创新，比如新的网络主干网、改进的数据增强技术和优化的训练策略。这些发展在不显著影响模型实时性能的情况下显著提高了精度。</p>
<p>从缩放yolo4开始，所有官方YOLO模型都对速度和精度之间的权衡进行了微调，提供不同的模型尺度以适应特定的应用和硬件要求。例如，这些版本通常提供针对边缘设备优化的轻量级模型，以准确性换取降低的计算复杂性和更快的处理时间。</p>
<h2 id="yolo-未来">19 YOLO 未来</h2>
<p>随着YOLO框架的不断发展，我们预计以下趋势和可能性将影响未来的发展:</p>
<ol type="1">
<li><p>研究人员和开发人员将利用深度学习、数据增强和训练技术方面的最新方法，继续完善YOLO架构。这个持续的创新过程可能会提高模型的性能、健壮性和效率。</p></li>
<li><p>基准进化。目前用于评估目标检测模型的基准，COCO 2017，最终可能会被一个更先进、更具挑战性的基准所取代。这反映了前两个YOLO版本中使用的VOC 2007基准的转变，反映了随着模型变得更加复杂和准确，对更高要求的基准的需求。</p></li>
<li><p>YOLO模型及其应用的扩展。随着YOLO框架的发展，我们预计每年发布的YOLO模型数量将会增加，应用范围也会相应扩大。随着该框架变得更加通用和强大，它可能会被应用于更广泛的领域，从家用电器设备到自动驾驶汽车。</p></li>
<li><p>扩展到新的领域。YOLO模型有潜力将其功能扩展到物体检测和分割之外，扩展到视频中的物体跟踪和3D关键点估计等领域。随着这些模型的发展，它们可能成为解决更广泛的计算机视觉任务的新解决方案的基础。</p></li>
<li><p>对各种硬件的适应性。YOLO模型将进一步跨越硬件平台，从物联网设备到高性能计算集群。这种适应性将支持在各种上下文中部署YOLO模型，具体取决于应用程序的需求和约束。此外，通过定制模型以适应不同的硬件规格，YOLO可以为更多的用户和行业提供访问和有效的服务。</p></li>
</ol>
<h2 id="reference">reference</h2>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>W. Lan, J. Dang, Y . Wang, and S. Wang, “Pedestrian detection based on yolo network model,” in 2018 IEEE international conference on mechatronics and automation (ICMA), pp. 1547–1551, IEEE, 2018.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>W.-Y . Hsu and W.-Y . Lin, “Adaptive fusion of multi-scale yolo for pedestrian detection,” IEEE Access, vol. 9, pp. 110063–110073, 2021.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>A. Benjumea, I. Teeti, F. Cuzzolin, and A. Bradley, “Y olo-z: Improving small object detection in yolov5 for autonomous vehicles,” arXiv preprint arXiv:2112.11798, 2021.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>N. M. A. A. Dazlee, S. A. Khalil, S. Abdul-Rahman, and S. Mutalib, “Object detection for autonomous vehicles with sensor-based technology using yolo,” International Journal of Intelligent Systems and Applications in Engineering, vol. 10, no. 1, pp. 129–134, 2022.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>S. Liang, H. Wu, L. Zhen, Q. Hua, S. Garg, G. Kaddoum, M. M. Hassan, and K. Y u, “Edge yolo: Real-time intelligent object detection system based on edge-cloud cooperation in autonomous vehicles,” IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 12, pp. 25345–25360, 2022.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Q. Li, X. Ding, X. Wang, L. Chen, J. Son, and J.-Y . Song, “Detection and identification of moving objects at busy traffic road based on yolo v4,” The Journal of the Institute of Internet, Broadcasting and Communication, vol. 21, no. 1, pp. 141–148, 2021.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>S. Shinde, A. Kothari, and V . Gupta, “Y olo based human action recognition and localization,” Procedia computer science, vol. 133, pp. 831–838, 2018.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>A. H. Ashraf, M. Imran, A. M. Qahtani, A. Alsufyani, O. Almutiry, A. Mahmood, M. Attique, and M. Habib, “Weapons detection for security and video surveillance using cnn and yolo-v5s,” CMC-Comput. Mater . Contin, vol. 70, pp. 2761–2775, 2022.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Y . Zheng and H. Zhang, “Video analysis in sports by lightweight object detection network under the background of sports industry development,” Computational Intelligence and Neuroscience, vol. 2022, 2022.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>H. Ma, T. Celik, and H. Li, “Fer-yolo: Detection and classification based on facial expressions,” in Image and Graphics: 11th International Conference, ICIG 2021, Haikou, China, August 6–8, 2021, Proceedings, Part I 11, pp. 28–39, Springer, 2021.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Y . Tian, G. Yang, Z. Wang, H. Wang, E. Li, and Z. Liang, “Apple detection during different growth stages in orchards using the improved yolo-v3 model,” Computers and electronics in agriculture, vol. 157, pp. 417–426, 2019.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>D. Wu, S. Lv, M. Jiang, and H. Song, “Using channel pruning-based yolo v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments,” Computers and Electronics in Agriculture, vol. 178, p. 105742, 2020.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>M. Lippi, N. Bonucci, R. F. Carpio, M. Contarini, S. Speranza, and A. Gasparri, “A yolo-based pest detection system for precision agriculture,” in 2021 29th Mediterranean Conference on Control and Automation (MED), pp. 342–347, IEEE, 2021.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>W. Y ang and Z. Jiachun, “Real-time face detection based on yolo,” in 2018 1st IEEE international conference on knowledge innovation and invention (ICKII), pp. 221–224, IEEE, 2018.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>W. Chen, H. Huang, S. Peng, C. Zhou, and C. Zhang, “Y olo-face: a real-time face detector,” The Visual Computer, vol. 37, pp. 805–813, 2021.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>M. A. Al-Masni, M. A. Al-Antari, J.-M. Park, G. Gi, T.-Y . Kim, P . Rivera, E. V alarezo, M.-T. Choi, S.-M. Han, and T.-S. Kim, “Simultaneous detection and classification of breast masses in digital mammograms via a deep learning yolo-based cad system,” Computer methods and programs in biomedicine, vol. 157, pp. 85–94, 2018.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>Y . Nie, P . Sommella, M. O’Nils, C. Liguori, and J. Lundgren, “Automatic detection of melanoma with yolo deep convolutional neural networks,” in 2019 E-Health and Bioengineering Conference (EHB), pp. 1–4, IEEE, 2019.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>H. M. Ünver and E. Ayan, “Skin lesion segmentation in dermoscopic images with combination of yolo and grabcut algorithm,” Diagnostics, vol. 9, no. 3, p. 72, 2019.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>L. Tan, T. Huangfu, L. Wu, and W. Chen, “Comparison of retinanet, ssd, and yolo v3 for real-time pill identification,” BMC medical informatics and decision making, vol. 21, pp. 1–11, 2021.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>L. Cheng, J. Li, P . Duan, and M. Wang, “A small attentional yolo model for landslide detection from satellite remote sensing images,” Landslides, vol. 18, no. 8, pp. 2751–2765, 2021.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>M.-T. Pham, L. Courtrai, C. Friguet, S. Lefèvre, and A. Baussard, “Y olo-fine: One-stage detector of small objects under various backgrounds in remote sensing images,” Remote Sensing, vol. 12, no. 15, p. 2501, 2020.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p>Y . Qing, W. Liu, L. Feng, and W. Gao, “Improved yolo network for free-angle remote sensing target detection,” Remote Sensing, vol. 13, no. 11, p. 2171, 2021.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>Z. Zakria, J. Deng, R. Kumar, M. S. Khokhar, J. Cai, and J. Kumar, “Multiscale and direction target detecting in remote sensing images via modified yolo-v4,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 15, pp. 1039–1048, 2022.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>P . Kumar, S. Narasimha Swamy, P . Kumar, G. Purohit, and K. S. Raju, “Real-time, yolo-based intelligent surveillance and monitoring system using jetson tx2,” in Data Analytics and Management: Proceedings of ICDAM, pp. 461–471, Springer, 2021.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>K. Bhambani, T. Jain, and K. A. Sultanpure, “Real-time face mask and social distancing violation detection system using yolo,” in 2020 IEEE Bangalore humanitarian technology conference (B-HTC), pp. 1–6, IEEE, 2020.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p>J. Li, Z. Su, J. Geng, and Y . Yin, “Real-time detection of steel strip surface defects based on improved yolo detection network,” IF AC-PapersOnLine, vol. 51, no. 21, pp. 76–81, 2018.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>E. N. Ukhwah, E. M. Y uniarno, and Y . K. Suprapto, “Asphalt pavement pothole detection using deep learning method based on yolo neural network,” in 2019 International Seminar on Intelligent Technology and Its Applications (ISITIA), pp. 35–40, IEEE, 2019.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p>Y . Du, N. Pan, Z. Xu, F. Deng, Y . Shen, and H. Kang, “Pavement distress detection and classification based on yolo network,” International Journal of Pavement Engineering, vol. 22, no. 13, pp. 1659–1672, 2021.<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>R.-C. Chen et al., “Automatic license plate recognition via sliding-window darknet-yolo deep learning,” Image and Vision Computing, vol. 87, pp. 47–56, 2019.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p>C. Dewi, R.-C. Chen, X. Jiang, and H. Y u, “Deep convolutional neural network for enhancing traffic sign recognition developed on yolo v4,” Multimedia Tools and Applications, vol. 81, no. 26, pp. 37821–37845, 2022.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p>A. M. Roy, J. Bhaduri, T. Kumar, and K. Raj, “Wildect-yolo: An efficient and robust computer vision-based accurate object localization model for automated endangered wildlife detection,” Ecological Informatics, vol. 75, p. 101919, 2023.<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>S. Kulik and A. Shtanko, “Experiments with neural net object detection system yolo on small training datasets for intelligent robotics,” in Advanced Technologies in Robotics and Intelligent Systems: Proceedings of ITR 2019, pp. 157–162, Springer, 2020.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p>D. H. Dos Reis, D. Welfer, M. A. De Souza Leite Cuadros, and D. F. T. Gamarra, “Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm,” Applied Artificial Intelligence, vol. 33, no. 14, pp. 1290–1305, 2019.<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p>O. Sahin and S. Ozer, “Y olodrone: Improved yolo architecture for object detection in drone images,” in 2021 44th International Conference on Telecommunications and Signal Processing (TSP), pp. 361–365, IEEE, 2021.<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>C. Chen, Z. Zheng, T. Xu, S. Guo, S. Feng, W. Yao, and Y . Lan, “Y olo-based uav technology: A review of the research and its applications,” Drones, vol. 7, no. 3, p. 190, 2023.<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p>M. Everingham, L. V an Gool, C. K. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc)challenge,” International journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in European conference on computer vision, pp. 740–755, Springer, 2014.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p>J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788, 2016.<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p>R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, pp. 1440–1448, 2015.<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p>A. L. Maas, A. Y . Hannun, A. Y . Ng, et al., “Rectifier nonlinearities improve neural network acoustic models,” in Proc. icml, vol. 30, p. 3, Atlanta, Georgia, USA, 2013.<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>C. Szegedy, W. Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan, V . V anhoucke, and A. Rabinovich, “Going deeper with convolutions,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p>M. Lin, Q. Chen, and S. Y an, “Network in network,” arXiv preprint arXiv:1312.4400, 2013.<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p>O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., “Imagenet large scale visual recognition challenge,” International journal of computer vision, vol. 115, no. 3, pp. 211–252, 2015.<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><p>M. Everingham, L. V an Gool, C. K. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc)challenge,” International journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><p>R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, pp. 1440–1448, 2015.<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><p>J. Redmon and A. Farhadi, “Y olo9000: better, faster, stronger,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263–7271, 2017.<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47" role="doc-endnote"><p>M. Lin, Q. Chen, and S. Y an, “Network in network,” arXiv preprint arXiv:1312.4400, 2013.<a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48" role="doc-endnote"><p>T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in European conference on computer vision, pp. 740–755, Springer, 2014.<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49" role="doc-endnote"><p>J. Redmon and A. Farhadi, “Y olov3: An incremental improvement,” arXiv preprint arXiv:1804.02767, 2018.<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50" role="doc-endnote"><p>I. Krasin, T. Duerig, N. Alldrin, V . Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. V eit, et al., “Openimages: A public dataset for large-scale multi-label and multi-class image classification,” Dataset available from https://github. com/openimages, vol. 2, no. 3, p. 18, 2017.<a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn51" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52" role="doc-endnote"><p>T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn53" role="doc-endnote"><p>T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in European conference on computer vision, pp. 740–755, Springer, 2014.<a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54" role="doc-endnote"><p>T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.<a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn55" role="doc-endnote"><p>A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, “Y olov4: Optimal speed and accuracy of object detection,” arXiv preprint arXiv:2004.10934, 2020.<a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn56" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.<a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn57" role="doc-endnote"><p>L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Y uille, “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,” IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834–848, 2017.<a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn58" role="doc-endnote"><p>S. Liu, D. Huang, et al., “Receptive field block net for accurate and fast object detection,” in Proceedings of the European conference on computer vision (ECCV), pp. 385–400, 2018.<a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn59" role="doc-endnote"><p>T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.<a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn60" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.<a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn61" role="doc-endnote"><p>B. Hariharan, P . Arbeláez, R. Girshick, and J. Malik, “Hypercolumns for object segmentation and fine-grained localization,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 447–456, 2015.<a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn62" role="doc-endnote"><p>Q. Zhao, T. Sheng, Y . Wang, Z. Tang, Y . Chen, L. Cai, and H. Ling, “M2det: A single-shot object detector based on multi-level feature pyramid network,” in Proceedings of the AAAI conference on artificial intelligence, vol. 33, pp. 9259–9266, 2019.<a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn63" role="doc-endnote"><p>A. L. Maas, A. Y . Hannun, A. Y . Ng, et al., “Rectifier nonlinearities improve neural network acoustic models,” in Proc. icml, vol. 30, p. 3, Atlanta, Georgia, USA, 2013.<a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn64" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015.<a href="#fnref64" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn65" role="doc-endnote"><p>D. Misra, “Mish: A self regularized non-monotonic neural activation function,” arXiv preprint arXiv:1908.08681, vol. 4, no. 2, pp. 10–48550, 2019.<a href="#fnref65" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn66" role="doc-endnote"><p>N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, “Soft-nms–improving object detection with one line of code,” in Proceedings of the IEEE international conference on computer vision, pp. 5561–5569, 2017.<a href="#fnref66" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn67" role="doc-endnote"><p>S. Xie, R. Girshick, P . Dollár, Z. Tu, and K. He, “Aggregated residual transformations for deep neural networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492–1500, 2017.<a href="#fnref67" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn68" role="doc-endnote"><p>M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,” in International conference on machine learning, pp. 6105–6114, PMLR, 2019.<a href="#fnref68" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn69" role="doc-endnote"><p>D. Misra, “Mish: A self regularized non-monotonic neural activation function,” arXiv preprint arXiv:1908.08681, vol. 4, no. 2, pp. 10–48550, 2019.<a href="#fnref69" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn70" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.<a href="#fnref70" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn71" role="doc-endnote"><p>S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8759–8768, 2018.<a href="#fnref71" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn72" role="doc-endnote"><p>S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional block attention module,” in Proceedings of the European conference on computer vision (ECCV), pp. 3–19, 2018.<a href="#fnref72" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn73" role="doc-endnote"><p>G. Ghiasi, T.-Y . Lin, and Q. V . Le, “Dropblock: A regularization method for convolutional networks,” Advances in neural information processing systems, vol. 31, 2018.<a href="#fnref73" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn74" role="doc-endnote"><p>N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple way to prevent neural networks from overfitting,” The journal of machine learning research, vol. 15, no. 1, pp. 1929–1958, 2014.<a href="#fnref74" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn75" role="doc-endnote"><p>C. Szegedy, V . V anhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the inception architecture for computer vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016.<a href="#fnref75" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn76" role="doc-endnote"><p>M. A. Islam, S. Naha, M. Rochan, N. Bruce, and Y . Wang, “Label refinement network for coarse-to-fine semantic segmentation,” arXiv preprint arXiv:1703.00551, 2017.<a href="#fnref76" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn77" role="doc-endnote"><p>Z. Zheng, P . Wang, W. Liu, J. Li, R. Y e, and D. Ren, “Distance-iou loss: Faster and better learning for bounding box regression,” in Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 12993–13000, 2020.<a href="#fnref77" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn78" role="doc-endnote"><p>S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in International conference on machine learning, pp. 448–456, PMLR, 2015.<a href="#fnref78" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn79" role="doc-endnote"><p>I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent with warm restarts,” arXiv preprint arXiv:1608.03983, 2016.<a href="#fnref79" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn80" role="doc-endnote"><p>G. Jocher, “YOLOv5 by Ultralytics.” https://github.com/ultralytics/yolov5, 2020. Accessed: February 30, 2023.<a href="#fnref80" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn81" role="doc-endnote"><p>C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao, “Scaled-yolov4: Scaling cross stage partial network,” in Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 13029–13038, 2021.<a href="#fnref81" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn82" role="doc-endnote"><p>M. Tan, R. Pang, and Q. V . Le, “Efficientdet: Scalable and efficient object detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020.<a href="#fnref82" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn83" role="doc-endnote"><p>T.-Y . Lin, P . Goyal, R. Girshick, K. He, and P . Dollár, “Focal loss for dense object detection,” in Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017.<a href="#fnref83" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn84" role="doc-endnote"><p>X. Long, K. Deng, G. Wang, Y . Zhang, Q. Dang, Y . Gao, H. Shen, J. Ren, S. Han, E. Ding, et al., “Pp-yolo: An effective and efficient implementation of object detector,” arXiv preprint arXiv:2007.12099, 2020.<a href="#fnref84" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn85" role="doc-endnote"><p>C.-Y . Wang, I.-H. Yeh, and H.-Y . M. Liao, “Y ou only learn one representation: Unified network for multiple tasks,” arXiv preprint arXiv:2105.04206, 2021.<a href="#fnref85" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn86" role="doc-endnote"><p>Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Y olox: Exceeding yolo series in 2021,” arXiv preprint arXiv:2107.08430, 2021.<a href="#fnref86" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn87" role="doc-endnote"><p>H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,” in Proceedings of the European conference on computer vision (ECCV), pp. 734–750, 2018.<a href="#fnref87" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn88" role="doc-endnote"><p>K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet: Keypoint triplets for object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6569–6578, 2019.<a href="#fnref88" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn89" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref89" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn90" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref90" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn91" role="doc-endnote"><p>G. Song, Y . Liu, and X. Wang, “Revisiting the sibling head in object detector,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11563–11572, 2020.<a href="#fnref91" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn92" role="doc-endnote"><p>Y . Wu, Y . Chen, L. Y uan, Z. Liu, L. Wang, H. Li, and Y . Fu, “Rethinking classification and localization for object detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10186–10195, 2020.<a href="#fnref92" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn93" role="doc-endnote"><p>Z. Ge, S. Liu, Z. Li, O. Y oshie, and J. Sun, “Ota: Optimal transport assignment for object detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 303–312, 2021.<a href="#fnref93" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn94" role="doc-endnote"><p>H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.<a href="#fnref94" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn95" role="doc-endnote"><p>C. Li, L. Li, H. Jiang, K. Weng, Y . Geng, L. Li, Z. Ke, Q. Li, M. Cheng, W. Nie, et al., “Y olov6: A single-stage object detection framework for industrial applications,” arXiv preprint arXiv:2209.02976, 2022.<a href="#fnref95" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn96" role="doc-endnote"><p>Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Y olox: Exceeding yolo series in 2021,” arXiv preprint arXiv:2107.08430, 2021.<a href="#fnref96" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn97" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref97" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn98" role="doc-endnote"><p>X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making vgg-style convnets great again,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13733–13742, 2021.<a href="#fnref98" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn99" role="doc-endnote"><p>S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8759–8768, 2018.<a href="#fnref99" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn100" role="doc-endnote"><p>X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making vgg-style convnets great again,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13733–13742, 2021.<a href="#fnref100" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn101" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, Y .-H. Wu, P .-Y . Chen, J.-W. Hsieh, and I.-H. Y eh, “Cspnet: A new backbone that can enhance learning capability of cnn,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 390–391, 2020.<a href="#fnref101" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn102" role="doc-endnote"><p>C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object detection,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3490–3499, IEEE Computer Society, 2021.<a href="#fnref102" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn103" role="doc-endnote"><p>H. Zhang, Y . Wang, F. Dayoub, and N. Sunderhauf, “V arifocalnet: An iou-aware dense object detector,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8514–8523, 2021.<a href="#fnref103" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn104" role="doc-endnote"><p>Z. Gevorgyan, “Siou loss: More powerful learning for bounding box regression,” arXiv preprint arXiv:2205.12740, 2022.<a href="#fnref104" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn105" role="doc-endnote"><p>H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, “Generalized intersection over union: A metric and a loss for bounding box regression,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 658–666, 2019.<a href="#fnref105" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn106" role="doc-endnote"><p>X. Ding, H. Chen, X. Zhang, K. Huang, J. Han, and G. Ding, “Re-parameterizing your optimizers rather than architectures,” arXiv preprint arXiv:2205.15242, 2022.<a href="#fnref106" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn107" role="doc-endnote"><p>C. Shu, Y . Liu, J. Gao, Z. Yan, and C. Shen, “Channel-wise knowledge distillation for dense prediction,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5311–5320, 2021.<a href="#fnref107" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn108" role="doc-endnote"><p>C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao, “Y olov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,” arXiv preprint arXiv:2207.02696, 2022.<a href="#fnref108" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn109" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, and I.-H. Y eh, “Designing network design strategies through gradient path analysis,” arXiv preprint arXiv:2211.04800, 2022.<a href="#fnref109" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn110" role="doc-endnote"><p>X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making vgg-style convnets great again,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13733–13742, 2021.<a href="#fnref110" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn111" role="doc-endnote"><p>K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.<a href="#fnref111" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn112" role="doc-endnote"><p>G. Huang, Z. Liu, L. V an Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.<a href="#fnref112" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn113" role="doc-endnote"><p>C.-Y . Wang, I.-H. Yeh, and H.-Y . M. Liao, “Y ou only learn one representation: Unified network for multiple tasks,” arXiv preprint arXiv:2105.04206, 2021.<a href="#fnref113" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn114" role="doc-endnote"><p>X. Xu, Y . Jiang, W. Chen, Y . Huang, Y . Zhang, and X. Sun, “Damo-yolo: A report on real-time object detection design,” arXiv preprint arXiv:2211.15444, 2022.<a href="#fnref114" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn115" role="doc-endnote"><p>Alibaba, “TinyNAS.” https://github.com/alibaba/lightweight-neural-architecture-search,2023. Accessed: March 18, 2023.<a href="#fnref115" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn116" role="doc-endnote"><p>Z. Tan, J. Wang, X. Sun, M. Lin, H. Li, et al., “Giraffedet: A heavy-neck paradigm for object detection,” in International Conference on Learning Representations, 2021.<a href="#fnref116" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn117" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, Y .-H. Wu, P .-Y . Chen, J.-W. Hsieh, and I.-H. Y eh, “Cspnet: A new backbone that can enhance learning capability of cnn,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 390–391, 2020.<a href="#fnref117" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn118" role="doc-endnote"><p>C.-Y . Wang, H.-Y . M. Liao, and I.-H. Y eh, “Designing network design strategies through gradient path analysis,” arXiv preprint arXiv:2211.04800, 2022.<a href="#fnref118" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn119" role="doc-endnote"><p>Z. Ge, S. Liu, Z. Li, O. Y oshie, and J. Sun, “Ota: Optimal transport assignment for object detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 303–312, 2021.<a href="#fnref119" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn120" role="doc-endnote"><p>C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object detection,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3490–3499, IEEE Computer Society, 2021.<a href="#fnref120" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn121" role="doc-endnote"><p>T.-Y . Lin, P . Goyal, R. Girshick, K. He, and P . Dollár, “Focal loss for dense object detection,” in Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017.<a href="#fnref121" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn122" role="doc-endnote"><p>G. Jocher, A. Chaurasia, and J. Qiu, “YOLO by Ultralytics.” https://github.com/ultralytics/ultralytics, 2023. Accessed: February 30, 2023.<a href="#fnref122" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn123" role="doc-endnote"><p>X. Long, K. Deng, G. Wang, Y . Zhang, Q. Dang, Y . Gao, H. Shen, J. Ren, S. Han, E. Ding, et al., “Pp-yolo: An effective and efficient implementation of object detector,” arXiv preprint arXiv:2007.12099, 2020.<a href="#fnref123" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn124" role="doc-endnote"><p>Y . Ma, D. Y u, T. Wu, and H. Wang, “Paddlepaddle: An open-source deep learning platform from industrial practice,” Frontiers of Data and Domputing, vol. 1, no. 1, pp. 105–115, 2019.<a href="#fnref124" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn125" role="doc-endnote"><p>J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei, “Deformable convolutional networks,” in Proceedings of the IEEE international conference on computer vision, pp. 764–773, 2017.<a href="#fnref125" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn126" role="doc-endnote"><p>W. Xinlong, Z. Rufeng, K. Tao, L. Lei, and S. Chunhua, “Solov2: Dynamic, faster and stronger,” in Proc. NIPS, 2020.<a href="#fnref126" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn127" role="doc-endnote"><p>R. Liu, J. Lehman, P . Molino, F. Petroski Such, E. Frank, A. Sergeev, and J. Y osinski, “An intriguing failing of convolutional neural networks and the coordconv solution,” Advances in neural information processing systems, vol. 31, 2018.<a href="#fnref127" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn128" role="doc-endnote"><p>H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.<a href="#fnref128" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn129" role="doc-endnote"><p>X. Huang, X. Wang, W. Lv, X. Bai, X. Long, K. Deng, Q. Dang, S. Han, Q. Liu, X. Hu, et al., “Pp-yolov2: A practical object detector,” arXiv preprint arXiv:2104.10419, 2021.<a href="#fnref129" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn130" role="doc-endnote"><p>S. Xu, X. Wang, W. Lv, Q. Chang, C. Cui, K. Deng, G. Wang, Q. Dang, S. Wei, Y . Du, et al., “Pp-yoloe: An evolved version of yolo,” arXiv preprint arXiv:2203.16250, 2022.<a href="#fnref130" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn131" role="doc-endnote"><p>Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627–9636, 2019.<a href="#fnref131" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn132" role="doc-endnote"><p>K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet: Keypoint triplets for object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6569–6578, 2019.<a href="#fnref132" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn133" role="doc-endnote"><p>H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,” in Proceedings of the European conference on computer vision (ECCV), pp. 734–750, 2018.<a href="#fnref133" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn134" role="doc-endnote"><p>Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Y olox: Exceeding yolo series in 2021,” arXiv preprint arXiv:2107.08430, 2021.<a href="#fnref134" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn135" role="doc-endnote"><p>L. Rao, “Treenet: A lightweight one-shot aggregation convolutional network,” arXiv preprint arXiv:2109.12342, 2021.<a href="#fnref135" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn136" role="doc-endnote"><p>C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object detection,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3490–3499, IEEE Computer Society, 2021.<a href="#fnref136" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn137" role="doc-endnote"><p>H. Zhang, Y . Wang, F. Dayoub, and N. Sunderhauf, “V arifocalnet: An iou-aware dense object detector,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8514–8523, 2021.<a href="#fnref137" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn138" role="doc-endnote"><p>X. Li, W. Wang, L. Wu, S. Chen, X. Hu, J. Li, J. Tang, and J. Y ang, “Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection,” Advances in Neural Information Processing Systems, vol. 33, pp. 21002–21012, 2020.<a href="#fnref138" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">yolo</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">summary</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/6a09e8f8.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-强化学习1-无状态问题" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/800a73d6.html">强化学习1-无状态问题</a>
    </h2>
  

        
		
		  <a href="/post/800a73d6.html" class="archive-article-date">
  	<time datetime="2023-06-16T02:48:01.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-06-16</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">987字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">4min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h2 id="问题描述">问题描述</h2>
<p>探索 + 利用</p>
<ul>
<li>构建中奖概率，以及中奖的期望</li>
</ul>
<figure class="highlight plaintext"><figcaption><span>notebook</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#每个老虎机的中奖概率,0-1之间的均匀分布</span><br><span class="line">probs = np.random.uniform(size=10)</span><br><span class="line"></span><br><span class="line">min_num = np.argmin(probs)</span><br><span class="line">temp = probs[0]</span><br><span class="line">probs[0] = probs[min_num]</span><br><span class="line">probs[min_num]=temp</span><br><span class="line"></span><br><span class="line">#记录每个老虎机的返回值，reward期望</span><br><span class="line">rewards = [[1] for _ in range(10)]</span><br><span class="line"></span><br><span class="line">probs, rewards</span><br></pre></td></tr></table></figure>
<ul>
<li>尝试一次</li>
</ul>
<figure class="highlight plaintext"><figcaption><span>notebook</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def try_and_play():</span><br><span class="line">    i = choose_one()</span><br><span class="line"></span><br><span class="line">    #玩老虎机,得到结果</span><br><span class="line">    reward = 0</span><br><span class="line">    if random.random() &lt; probs[i]:</span><br><span class="line">        reward = 1</span><br><span class="line"></span><br><span class="line">    #记录玩的结果</span><br><span class="line">    rewards[i].append(reward)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">try_and_play()</span><br><span class="line"></span><br><span class="line">rewards</span><br></pre></td></tr></table></figure>
<ul>
<li>尝试5000次，统计结果：</li>
</ul>
<figure class="highlight plaintext"><figcaption><span>notebook</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#记录每个老虎机的返回值，reward期望</span><br><span class="line">rewards = [[1] for _ in range(10)]</span><br><span class="line">ef get_result():</span><br><span class="line">    #玩N次</span><br><span class="line">    for _ in range(5000):</span><br><span class="line">        try_and_play()</span><br><span class="line"></span><br><span class="line">    #期望的最好结果</span><br><span class="line">    target = probs.max() * 5000</span><br><span class="line"></span><br><span class="line">    #实际玩出的结果</span><br><span class="line">    result = sum([sum(i) for i in rewards])</span><br><span class="line"></span><br><span class="line">    return target, result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">get_result()</span><br></pre></td></tr></table></figure>
<h2 id="算法描述">算法描述</h2>
<h3 id="贪婪算法">1 贪婪算法</h3>
<ul>
<li>大概率选择目前中奖率最高的，小概率随机探索</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#贪婪算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#有小概率随机选择一根拉杆</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.01</span>:</span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算每个老虎机的奖励平均</span></span><br><span class="line">    rewards_mean = [np.mean(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#选择期望奖励估值最大的拉杆</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(rewards_mean)</span><br></pre></td></tr></table></figure>
<h3 id="递减的贪婪算法">2 递减的贪婪算法</h3>
<ul>
<li>随着次数的增多，随机选择的概率逐渐下降</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机选择的概率递减的贪婪算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#求出现在已经玩了多少次了</span></span><br><span class="line">    played_count = <span class="built_in">sum</span>([<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#随机选择的概率逐渐下降</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">1</span> / played_count:</span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算每个老虎机的奖励平均</span></span><br><span class="line">    rewards_mean = [np.mean(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#选择期望奖励估值最大的拉杆</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(rewards_mean)</span><br></pre></td></tr></table></figure>
<h3 id="上界置信函数">3 上界置信函数</h3>
<p><span class="math display">\[
UCB = \frac{\sqrt{\sum\text{played\_count}}}{2*\text{played\_count}}
\]</span></p>
<p>其中<span class="math inline">\(\text{play\_count}\)</span>是一个矩阵，表示每个老虎机玩的次数</p>
<ul>
<li>能够衡量每个老虎机玩的次数，玩的次数多会越接近0</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机选择的概率递减的贪婪算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#求出每个老虎机各玩了多少次</span></span><br><span class="line">    played_count = [<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line">    played_count = np.array(played_count)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#求出上置信界</span></span><br><span class="line">    <span class="comment">#分子是总共玩了多少次,取根号后让他的增长速度变慢</span></span><br><span class="line">    <span class="comment">#分母是每台老虎机玩的次数,乘以2让他的增长速度变快</span></span><br><span class="line">    <span class="comment">#随着玩的次数增加,分母会很快超过分子的增长速度,导致分数越来越小</span></span><br><span class="line">    <span class="comment">#具体到每一台老虎机,则是玩的次数越多,分数就越小,也就是ucb的加权越小</span></span><br><span class="line">    <span class="comment">#所以ucb衡量了每一台老虎机的不确定性,不确定性越大,探索的价值越大</span></span><br><span class="line">    fenzi = played_count.<span class="built_in">sum</span>()**<span class="number">0.5</span></span><br><span class="line">    fenmu = played_count * <span class="number">2</span></span><br><span class="line">    ucb = fenzi / fenmu</span><br><span class="line"></span><br><span class="line">    <span class="comment">#ucb本身取根号</span></span><br><span class="line">    <span class="comment">#大于1的数会被缩小,小于1的数会被放大,这样保持ucb恒定在一定的数值范围内</span></span><br><span class="line">    ucb = ucb**<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算每个老虎机的奖励平均</span></span><br><span class="line">    rewards_mean = [np.mean(i) <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line">    rewards_mean = np.array(rewards_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#ucb和期望求和</span></span><br><span class="line">    ucb += rewards_mean</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ucb.argmax()</span><br></pre></td></tr></table></figure>
<h3 id="汤普森采样函数">4 汤普森采样函数</h3>
<ul>
<li>介绍一下beta分布：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#beta分布测试</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当数字小的时候,beta分布的概率有很大的随机性&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(np.random.beta(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当数字大时,beta分布逐渐稳定&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(np.random.beta(<span class="number">1e5</span>, <span class="number">1e5</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>当rewards总和小的时候：表示实验的次数少，则分布有很大的随机性，表示大量探索；</li>
<li>当rewards总和变大的时候，表示实验次数大幅增加，则分布变得稳定，选择概率最大的那一个进行实验</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_one</span>():</span><br><span class="line">    <span class="comment">#求出每个老虎机出1的次数+1</span></span><br><span class="line">    count_1 = [<span class="built_in">sum</span>(i) + <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#求出每个老虎机出0的次数+1</span></span><br><span class="line">    count_0 = [<span class="built_in">sum</span>(<span class="number">1</span> - np.array(i)) + <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> rewards]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#按照beta分布计算奖励分布,这可以认为是每一台老虎机中奖的概率</span></span><br><span class="line">    beta = np.random.beta(count_1, count_0)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> beta.argmax()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">强化学习</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">基础</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/800a73d6.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-优雅的使用rss打破信息茧房" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/3e9dbae2.html">优雅的使用rss打破信息茧房</a>
    </h2>
  

        
		
		  <a href="/post/3e9dbae2.html" class="archive-article-date">
  	<time datetime="2023-06-05T01:54:30.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-06-05</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">1.2k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">4min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h1 id="前言">前言</h1>
<p>当今世界信息错综复杂，获取信息的渠道也很多。我曾是一个忠实的b站用户，但是不得不说现在的b站变了味，似乎可能会出现下一个全新的信息源。打破信息茧房的最好方式，并不是找到一个单独的平台，而是找到那些独立的信息提供方，也就是说，我们需要一个跨平台，无广告，无推荐算法的全新方式获取更有广度，更有深度的信息。</p>
<p>最近看到了一个老但是又重新鲜活起来的新技术——RSS，详细的技术细节，可以自行百度。重点是这项技术能够实现以下几点：</p>
<ul>
<li>多平台信息源：各种信息源(微信公众号,b站,Twitter,blogs)</li>
<li>跨平台：windows,linux,os,etc</li>
<li>方便易用可拓展</li>
<li>能够充分利用碎片化时间</li>
</ul>
<p>你可以帮这项技术当做一个记事本，这个记事本能够定时刷新最新资讯：</p>
<ol type="1">
<li>这个资讯不是推荐算法给你的，而是你自己订阅的</li>
<li>不局限于单个平台</li>
</ol>
<figure>
<img src="https://pic3.zhimg.com/80/v2-caf461e861957f5f2ecf6b654d989752_720w.webp" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="前置准备">前置准备</h1>
<ol type="1">
<li><p>需要亿点点科学上网：知名在线 RSS 服务 Feedly（2018.11）、Inoreader（2020.4） 先后被墙，打着万物皆可RSS大旗的 RssHub 官方域名也被墙了（2020.3）</p></li>
<li><p>一个github账号(vercel搭建服务)</p></li>
<li><p>Google play 用于安装移动端软件</p></li>
<li><p>vercel 用于配置 RssHub 服务(后文详解)</p></li>
</ol>
<h1 id="基础玩法">基础玩法</h1>
<ol type="1">
<li>找到RSS源</li>
</ol>
<p>https://github.com/weekend-project-space/top-rss-list</p>
<p>https://github.com/ChanceYu/front-end-rss</p>
<p>公共的RSS源很多，但是大部分都很老旧，国内的源质量不够，国外的源检索和阅读有一定障碍。</p>
<ol start="2" type="1">
<li>找到一个合适的RSS阅读器</li>
</ol>
<p>https://www.inoreader.com/ 推荐</p>
<p>https://feedly.com/</p>
<h1 id="进阶玩法">进阶玩法</h1>
<h2 id="前置准备-1">1 前置准备</h2>
<p>1 RssHub生成源服务器配置 https://github.com/DIYgod/RSSHub</p>
<p>2 vercel 账号(github)可以直接登陆</p>
<p>3 RssHub-Radar 嗅取网页中存在的rss订阅 https://github.com/DIYgod/RSSHub-Radar</p>
<p>4 RSSAid 安卓端嗅取rss订阅 https://github.com/LeetaoGoooo/RSSAid</p>
<h2 id="rsshub">2 RssHub</h2>
<p>RSSHub 是一个开源、简单易用、易于扩展的 RSS 生成器，可以给任何奇奇怪怪的内容生成 RSS 订阅源。RSSHub 借助于开源社区的力量快速发展中，目前已适配数百家网站的上千项内容</p>
<p>可以配合浏览器扩展 <a target="_blank" rel="noopener" href="https://github.com/DIYgod/RSSHub-Radar">RSSHub Radar</a> 和 移动端辅助 App <a target="_blank" rel="noopener" href="https://github.com/Cay-Zhang/RSSBud">RSSBud</a> (iOS) 与 <a target="_blank" rel="noopener" href="https://github.com/LeetaoGoooo/RSSAid">RSSAid</a> (Android) 食用</p>
<p>首先介绍一下为什么需要自己部署RssHub服务？</p>
<p>Rss这种服务打击商业行为，所以许多网页会将其原本的域名直接屏蔽(本质也是一种爬虫)，所以只有使用自己的服务才不会被屏蔽，而且自己搭建的数据更加安全</p>
<ul>
<li>如何部署？</li>
</ul>
<p>https://docs.rsshub.app/install/</p>
<ul>
<li>如何使用vercel部署？(想象成一个云服务器，能够直接部署github上面的任何项目)</li>
</ul>
<ol type="1">
<li>点击下面链接</li>
</ol>
<p>https://vercel.com/import/project?template=https://github.com/DIYgod/RSSHub</p>
<ol start="2" type="1">
<li>完成一系列的注册后，一键部署(fork到自己的github仓库即可)</li>
<li>想要自动更新？</li>
</ol>
<p>https://github.com/apps/pull</p>
<ul>
<li>不会自己部署？提供下面的域名</li>
</ul>
<p><code>服务器1</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frsshub.rssforever.com%2F">https://rsshub.rssforever.com</a> <code>服务器2</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.qiuyuair.com%2F">https://rss.qiuyuair.com</a> <code>服务器3</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.injahow.cn%2F">https://rss.injahow.cn</a> <code>服务器4</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.feiyuyu.net%2F">https://rss.feiyuyu.net</a> <code>服务器5</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.shab.fun%2F">https://rss.shab.fun</a> <code>服务器6</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.itggg.cn%2F">https://rss.itggg.cn</a> <code>服务器7</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frsshub.uneasy.win%2F">https://rsshub.uneasy.win</a> <code>服务器8</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frss.injahow.cn%2F">https://rss.injahow.cn</a> <code>服务器9</code> ：<a href="https://sspai.com/link?target=https%3A%2F%2Frsshub.anyant.xyz%2F">https://rsshub.anyant.xyz</a></p>
<h2 id="rsshub-radar">3 RssHub-Radar</h2>
<ul>
<li>为什么需要构建RssHub服务器？</li>
</ul>
<p>配合Radar浏览器插件能够直接嗅探任何存在的RSS订阅(真的应有尽有)：RssHub提供路由，这个域名就由你自己搭建的服务来提供咯！</p>
<ul>
<li>如何使用Radar？</li>
</ul>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202306051249350.png" alt="image-20230605124920895" /><figcaption aria-hidden="true">image-20230605124920895</figcaption>
</figure>
<blockquote>
<p>注意这里只需要填入域名即可！</p>
</blockquote>
<p>以b站为例：</p>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202306051316754.png" alt="image-20230605125151705" /><figcaption aria-hidden="true">image-20230605125151705</figcaption>
</figure>
<p>你可以选择复制这个链接，或者直接subscribe, 注意这个subscribe必须要先登陆(在设置中找到目标网站，登陆后即可)</p>
<h2 id="feeddd订阅微信公众号">4 Feeddd订阅微信公众号</h2>
<p>由于微信自建生态，所以其中的信息比较难以获取rss订阅，但是通过<a target="_blank" rel="noopener" href="https://hamibot.com/">Hamibot</a>能够动态获取推文(人工采集)</p>
<p>https://github.com/feeddd/feeds</p>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202306051316326.png" alt="image-20230605130927842" /><figcaption aria-hidden="true">image-20230605130927842</figcaption>
</figure>
<p>其实还有大量的公众号没有动态更新，所以单单微信这个生态而言，使用RSS是不方便的。</p>
<h1 id="end">end</h1>
<p>如果觉得RSS方便的话，一定要去star一下以上提到的所有项目，因为这是反商业化的，也是互联网精神的所在。</p>
<blockquote>
<p>rss才是真正的互联网精神的产物，正如开源一样，如何破除现有的信息茧房？并不是找到更多获取信息的渠道，而是打通blogger与user之间的通道！</p>
<p>less is more...</p>
</blockquote>
<p>more links:</p>
<p>https://sspai.com/post/75340</p>
<p>https://zhuanlan.zhihu.com/p/349349861</p>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">tools</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/3e9dbae2.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-tips-github自动fork" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/3627c7a8.html">tips:github自动fork</a>
    </h2>
  

        
		
		  <a href="/post/3627c7a8.html" class="archive-article-date">
  	<time datetime="2023-05-21T04:30:16.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-05-21</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">108字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">1min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <ol type="1">
<li>在fork后的项目中新建：</li>
</ol>
<p>.github/workflows/main.yml</p>
<ol start="2" type="1">
<li>在main.yml中输入：</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Upstream</span> <span class="string">Sync</span></span><br><span class="line"></span><br><span class="line"><span class="attr">permissions:</span></span><br><span class="line">  <span class="attr">contents:</span> <span class="string">write</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">schedule:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">cron:</span> <span class="string">&quot;0 */12 * * *&quot;</span> <span class="comment"># every 12 hours</span></span><br><span class="line">  <span class="attr">workflow_dispatch:</span> <span class="comment"># manual sync</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">sync:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">repo</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">ref:</span> <span class="string">master</span> <span class="comment"># [branch name]</span></span><br><span class="line">        <span class="attr">fetch-depth:</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Configure</span> <span class="string">Git</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        git config user.name &quot;GitHub Actions Bot&quot;</span></span><br><span class="line"><span class="string">        git config user.email &quot;email@xxx.com&quot;  # send email to you </span></span><br><span class="line"><span class="string"></span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Sync</span> <span class="string">with</span> <span class="string">upstream</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        git remote add upstream https://github.com/[https locations]</span></span><br><span class="line"><span class="string">        git fetch upstream</span></span><br><span class="line"><span class="string">        git merge upstream/master --no-edit</span></span><br><span class="line"><span class="string">        git push</span></span><br></pre></td></tr></table></figure>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">tips</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color2">github</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/3627c7a8.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-pytorch项目6-语音合成" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/2b566acf.html">pytorch项目6-语音合成</a>
    </h2>
  

        
		
		  <a href="/post/2b566acf.html" class="archive-article-date">
  	<time datetime="2023-05-06T11:09:28.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-05-06</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">4.9k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">19min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h1 id="conditional-variational-autoencoder-with-adversarial-learning-for-end-to-end-text-to-speech">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</h1>
<p>http://proceedings.mlr.press/v139/kim21f/kim21f.pdf</p>
<h2 id="论文概述">1 论文概述</h2>
<p>在这项工作中，作者提出了一种并行的<strong>端到端文本到语音</strong>（TTS）方法，其生成的音频比当前的两阶段TTS系统更自然。这种方法采用了增强正规化流的变分推断和对抗训练过程，从而提高了生成建模的表达能力。此外，作者还提出了一种随机持续时间预测器，用于从输入文本合成具有多样节奏的语音。</p>
<p>通过对潜在变量的不确定性建模和随机持续时间预测器，该方法表达了自然的一对多关系，即输入文本可以以不同的音高和节奏以多种方式发音。在LJ语音（单一发音者数据集）上进行的主观人类评估（平均意见分数，MOS）表明，该方法优于目前最好的公开可用TTS系统，并实现了与基准真实数据相当的MOS。</p>
<p>总之，作者提出了一种端到端的TTS方法，采用<strong>变分推断</strong>、<strong>正规化流</strong>和<strong>对抗训练</strong>过程，生成更自然的音频。通过<strong>随机持续时间预测器</strong>和隐变量的不确定性建模，该方法能够实现文本到语音的<strong>自然一对多关系</strong>。实验结果表明，该方法在性能上优于现有的TTS系统，并在主观评估中取得了令人满意的成绩。</p>
<ul>
<li>过去研究：文本-&gt;频谱-&gt;波形</li>
<li>现在：文本-&gt;波形</li>
</ul>
<p>提出的方法主要在前三个小节中描述：条件变分自编码器（VAE）的公式化；从变分推断中得出的对齐估计；用于提高合成质量的对抗性训练。</p>
<ol type="1">
<li><strong>条件变分自编码器</strong>（Conditional VAE）公式化：VITS模型采用了条件变分自编码器作为基本框架，通过引入潜在变量来捕捉输入文本的多样性，并进行端到端训练。</li>
<li><strong>变分推断中的对齐估计</strong>（Alignment Estimation）：VITS模型利用变分推断方法推导出与语音特征对齐的潜在变量表示，这有助于在生成过程中更好地捕捉文本与语音之间的对应关系。</li>
<li><strong>改进合成质量的对抗训练</strong>（Adversarial Training）：为了提高合成语音的质量，VITS模型采用了对抗训练方法，在生成过程中对抗性地优化生成器和判别器。</li>
</ol>
<p>整体架构包括文本编码器、持续时间预测器、潜在变量推断器、语音解码器和对抗性判别器等组件。通过联合训练这些组件，<strong>VITS</strong>模型可以在端到端的设置下实现高质量的文本到语音合成。</p>
<h2 id="method">2 method</h2>
<h3 id="conditional-vae">2.1 conditional VAE</h3>
<p><span class="math display">\[
\log p_\theta(x|c)\ge\mathbb{E}_{q_\phi(z|x)}\Big[\log p_\theta(x|z)-\log\frac{q_\phi(z|x)}{p_\theta(z|c)}\Big]
\]</span></p>
<p>左边：需要最大化的目标似然 x:目标(语音波形)，c:条件(text，说话人的音色)</p>
<p>右边：elbo(evidence lower bound) 如果是纯flow是能够得到解析式的 【重构器-KL散度:后验分布/先验分布】</p>
<p>最大化elbo：1. 解码器(重构器)在给定隐变量z的情况下，x对应的最大似然</p>
<ol start="2" type="1">
<li>最小化KL散度：后验与先验之间的距离，<span class="math inline">\(\phi\)</span>模型在给定目标的条件下的分布与<span class="math inline">\(\theta\)</span>模型在给定条件下的分布越接近越好</li>
</ol>
<h4 id="一般的vae目标函数推导">一般的VAE目标函数推导</h4>
<ol type="1">
<li><span class="math inline">\(P(X)=\int_{z} P(X|z)\times P(z)\)</span> <span class="math inline">\(X\)</span>为目标函数，<span class="math inline">\(Z\)</span>为隐变量，此处为对联合分布进行积分得到边缘分布</li>
<li>对于大部分<span class="math inline">\(z\)</span>,我们计算出来的<span class="math inline">\(P(X|z)\)</span>为0，因此需要缩小<span class="math inline">\(z\)</span>的样本空间，此处借用另外一个分布<span class="math inline">\(Q(z|X)\)</span>来缩小<span class="math inline">\(z\)</span>的样本空间(或者说增大<span class="math inline">\(z\)</span>产生<span class="math inline">\(X\)</span>的可能性)，并且当<span class="math inline">\(z\)</span>服从分布<span class="math inline">\(Q(z|X)\)</span>时，再计算<span class="math inline">\(P(X|z)\)</span>的期望值比较简单</li>
<li>那么，当<span class="math inline">\(z\)</span>服从分布<span class="math inline">\(Q(z|X)\)</span>时，<span class="math inline">\(P(X|z)\)</span>的期望值与目标数据的分布<span class="math inline">\(P(X)\)</span>之间是什么关系呢？可以用一个KL散度公式算起：<span class="math inline">\(D[Q(z|X)||P(z|X)]\)</span></li>
</ol>
<p><span class="math display">\[
D[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X)-\log P(z|X)]
\]</span></p>
<ol start="4" type="1">
<li>为了引出<span class="math inline">\(P(X)\)</span>，可以基于贝叶斯公式来改写<span class="math inline">\(P(z|X)\)</span></li>
</ol>
<p><span class="math display">\[
p(z|X)=\frac{P(z,X)}{P(X)}=\frac{P(X|z)\times P(z)}{P(X)}
\]</span></p>
<ol start="5" type="1">
<li>这时KL散度可以重新写成:</li>
</ol>
<p><span class="math display">\[
D[Q(z|X)||P(z|X)]=\mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X)-\log P(X|z) -\log P(z)+\log P(X)]
\]</span></p>
<ol start="6" type="1">
<li><p>因为其中的<span class="math inline">\(P(X)\)</span>与<span class="math inline">\(z\sim Q(z|X)\)</span>无关，因此可以移到期望公式外面;同时，由于: <span class="math display">\[
D[Q(z|X)||P(z)]=\mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X)-\log P(z)]
\]</span></p>
<ol start="7" type="1">
<li>KL散度进一步改写成</li>
</ol></li>
</ol>
<p><span class="math display">\[
D[Q(z|X)||P(z|X)]=-\mathbb{E}_{z\sim Q(z|X)}[\log P(X|z)] + D[Q(z|X)||P(z)]+\log P(X)
\]</span></p>
<ol start="8" type="1">
<li>因此</li>
</ol>
<p><span class="math display">\[
\log P(X)-\textcolor{red}{D[Q(z|X)||P(z|X)]} =\mathbb{E}_{z\sim Q(z|X)}[\log P(X|z)] - D[Q(z|X)||P(z)]
\]</span></p>
<ol start="9" type="1">
<li>红色部分恒大于0，故</li>
</ol>
<p><span class="math display">\[
\log P(X) \ge \mathbb{E}_{z\sim Q(z|X)}[\log P(X|z)] - D[Q(z|X)||P(z)]
\]</span></p>
<p>​ 当且仅当<span class="math inline">\(Q(z|X)\)</span>可以逼近<span class="math inline">\(P(z|X)\)</span>等式成立</p>
<ol start="10" type="1">
<li><p>目标数据的对数似然公式下界如式(8)所示，并且右边第一项是解码器，第二项是先验分布<span class="math inline">\(P(z)\)</span>与后验分布<span class="math inline">\(Q(z|X)\)</span>之间的距离</p></li>
<li><p>如果希望<span class="math inline">\(\log P(X)\)</span>越大越好，也就是相当于希望解码器基于<span class="math inline">\(z\)</span>分布预测<span class="math inline">\(X\)</span>的概率越大越好，同时先验分布<span class="math inline">\(P(z)\)</span>与后验分布<span class="math inline">\(Q(z|X)\)</span>之间的距离越小越好</p></li>
</ol>
<h4 id="vits中的条件vae目标函数推导">VITS中的条件VAE目标函数推导</h4>
<ol type="1">
<li>在本文中，我们可以从KL散度公式算起</li>
</ol>
<p><span class="math display">\[
D[Q(z|X)||P(z|X,c)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X) - log P(z|X,c)]
\]</span></p>
<ol start="2" type="1">
<li>类似的，我们可以yoga贝叶斯公式来改写$ P(z|X,c)$</li>
</ol>
<p><span class="math display">\[
P(z|X,c) = \frac{P(z,X|c)}{P(X|c)}=\frac{P(X|z)\times P(z|c)}{P(X|c)}
\]</span></p>
<ol start="3" type="1">
<li>于是KL散度可以重新写成:</li>
</ol>
<p><span class="math display">\[
D[Q(z|X)||P(z|X,c)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X) - log P(X|z)-\log P(z|c)] + \log P(X|c)
\]</span></p>
<p>​ 因为<span class="math inline">\(P(X|c)\)</span>与<span class="math inline">\(z\sim Q(z|X)\)</span>无关，因此可以移到期望公式之外</p>
<ol start="4" type="1">
<li>同时，由于：</li>
</ol>
<p><span class="math display">\[
D[Q(z|X)||P(z|c)] = \mathbb{E}_{z\sim Q(z|X)}[\log Q(z|X) -\log P(z|c)]
\]</span></p>
<p>​ KL散度式(9)可以进一步化解为： <span class="math display">\[
D[Q(z|X)||P(z|X,c)] = -\mathbb{E}_{z\sim Q(z|X)}[log P(X|z)] + D[Q(z|X)||P(z|c)] + \log P(X|c)
\]</span></p>
<ol start="5" type="1">
<li>因此</li>
</ol>
<p><span class="math display">\[
\log P(X|c) -\textcolor{red}{D[Q(z|X)||P(z|X,c)]} = \mathbb{E}_{z\sim Q(z|X)}[log P(X|z)] -D[Q(z|X)||P(z|c)]
\]</span></p>
<p>红色部分是恒大于0，所以得到: <span class="math display">\[
\log P(X|c)  \ge \mathbb{E}_{z\sim Q(z|X)}[log P(X|z)] -D[Q(z|X)||P(z|c)]
\]</span> 当且仅当<span class="math inline">\(Q(z|X)\)</span>可以逼近<span class="math inline">\(P(z|X,c)\)</span>时，等式成立</p>
<ol start="6" type="1">
<li>VITS的目标数据的对数似然公式下界中：</li>
</ol>
<ul>
<li><span class="math inline">\(X\)</span>表示音频的梅尔频谱</li>
<li><span class="math inline">\(c\)</span>表示<strong>文本信息和对其信息</strong>，<span class="math inline">\(z\)</span>表示隐含变量。其中，对齐信息是一个<span style="color:olive">硬对齐矩阵，形状为[|text|,|z|]</span><span class="math inline">\(\textcolor{olive}{}\)</span></li>
<li>文中提到，发现在<span class="math inline">\(P(z|c)\)</span>表示成简单的高斯分布之后再加一个flow变换，音质效果更好</li>
</ul>
<h4 id="重构loss右边等式第一项">重构loss(右边等式第一项)</h4>
<p>在重构损失中，作者使用了梅尔频谱图（mel-spectrogram）作为目标数据点，而不是原始波形。梅尔频谱图用<span class="math inline">\(X_{mel}\)</span>表示</p>
<p>这里的关键点是：</p>
<ol type="1">
<li>使用梅尔频谱图作为目标数据点，而不是原始波形。</li>
<li>将潜在变量<span class="math inline">\(z\)</span>上采样到波形域<span class="math inline">\(\hat y\)</span>。</li>
<li>将波形域<span class="math inline">\(\hat y\)</span>转换为梅尔频谱图域<span class="math inline">\(\hat X_{mel}\)</span>。</li>
<li>使用预测和目标梅尔频谱图之间的<span class="math inline">\(L1\)</span>损失作为重构损失。</li>
</ol>
<p><span class="math display">\[
L_{\text{recon}} = ||X_{mel}-\hat X_{mel}||
\]</span></p>
<p>这可以看作是在假设数据分布为拉普拉斯分布的情况下进行的<strong>最大似然估计</strong>，同时忽略了常数项。通过使用近似于人类听觉系统响应的<strong>梅尔刻度</strong>，将<strong>重构损失定义在梅尔频谱图域以提高感知质量</strong>。注意，从原<strong>始波形估计梅尔频谱图不需要可训练参数</strong>，因为它只使用STFT和线性投影到梅尔刻度。此外，估计仅在训练期间使用，而不是在推理期间使用。在实践中，我们不会对整个潜在变量<span class="math inline">\(z\)</span>进行上采样，而是将部分序列用作解码器的输入，这是用于高效端到端训练的窗口生成器训练。</p>
<p>这段话的关键点包括：</p>
<ol type="1">
<li>重构损失可以看作是在假设拉普拉斯分布的情况下进行的最大似然估计。</li>
<li>在梅尔频谱图域定义重构损失，以提高感知质量。</li>
<li>从原始波形估计梅尔频谱图不需要可训练参数。</li>
<li>在实践中，仅将部分潜在变量序列用作解码器的输入，以实现高效的端到端训练。</li>
</ol>
<h4 id="kl散度右边等式第二项">KL散度(右边等式第二项)</h4>
<ol type="1">
<li>先验编码器的输入条件<span class="math inline">\(c\)</span>由音素<span class="math inline">\(c_{text}\)</span>和音素与潜在变量之间的对齐<span class="math inline">\(A\)</span>组成。(因为文本和音素之间不可能是一一对应的，单个文本可能存在多个音素)</li>
<li>对齐是一个硬单调注意力矩阵，表示每个输入音素扩展的长度以便与目标语音进行时间对齐。</li>
<li>在训练迭代中需要估计对齐。</li>
<li>为了提供更高分辨率的信息，使用线性刻度频谱图<span class="math inline">\(X_{lin}\)</span>作为输入，而不是<strong>梅尔频谱图</strong>。修改后的输入并不违反变分推断的属性。</li>
</ol>
<p><span class="math display">\[
L_{kl}=\log q_{\phi} (z|X_{lin}) - \log p_{\theta} (z|c_{text},A) \\
z \sim q_{\phi}(z|X_{lin}) = N (z;\mu_{\phi}(X_{lin}),\sigma_{\phi}(X_{lin}))
\]</span></p>
<p><span class="math inline">\(z\)</span>是从后验分布中采样得到的，这是一个高斯分布。</p>
<p><span class="math inline">\(X\)</span>没有用波形频谱(复杂)，也没有用梅尔谱(相对简单)，而是采用了线性刻度频谱</p>
<p>我们使用因子化正态分布来参数化先验和后验编码器。我们发现增加先验分布的表达能力对于生成逼真的样本很重要。因此，我们在因子化正态先验分布的基础上应用<strong>归一化流</strong>，它允许将一个简单分布通过可逆变换转换成一个更复杂的分布，遵循变量变换规则： <span class="math display">\[
p_\theta(z|c)=N(f_\theta(z);\mu_{\theta}(c))\left|\text{det}\frac{\partial f_\theta(z)}{\partial z}\right| \\
c=[c_{text},A]
\]</span></p>
<h3 id="alignment-estimation">2.2 Alignment Estimation</h3>
<h5 id="mas">MAS</h5>
<p>为了在输入文本和目标语音之间估计对齐关系A，我们采用了<strong>单调对齐搜索</strong>(Monotonic Alignment Search，MAS)，这是一种通过正则化流参数化的数据似然最大化来搜索对齐关系的方法。 <span class="math display">\[
\begin{aligned}
A &amp; =\underset{\hat{A}}{\arg \max } \log p\left(X \mid c_{\text {text }}, \hat{A}\right) \\
&amp; =\underset{\hat{A}}{\arg \max } \log N\left(f(x) ; \mu\left(c_{\text {text }}, \hat{A}\right), \sigma\left(c_{\text {text }}, \hat{A}\right)\right)
\end{aligned}
\]</span> MAS利用了单调性假设，即输入文本中的字符和目标语音中的帧之间的对齐关系是单调的。这使得搜索过程更加高效，因为只需要在单调增加的路径上搜索。</p>
<ul>
<li>monotonic(单调)</li>
<li>non-skipping</li>
</ul>
<p>在我们的设置中直接应用MAS是困难的，因为我们的目标是ELBO（变分下界），而不是精确的对数似然。因此，我们重新定义MAS，使其寻找最大化ELBO的对齐关系，这可以简化为寻找最大化潜变量对数似然的对齐关系。</p>
<p>为了实现这一目标，我们可以在训练过程中根据<strong>当前模型参数来迭代更新对齐关系</strong>。在每次迭代过程中，我们通过最大化ELBO来调整对齐关系，并相应地更新模型参数。</p>
<p>这种方法允许我们在优化变分下界的同时，学习输入文本与目标语音之间的对齐关系。通过这种方式，我们可以在训练过程中逐渐改进模型的性能，并在推理阶段生成更自然的语音样本。</p>
<p><span class="math display">\[
\begin{array}{l}
\underset{\hat{A}}{\arg \max } \log p_{\theta}\left(X_{\text {mel }} \mid z\right)-\log \frac{q_{\phi}\left(z \mid x_{\text {lin }}\right)}{p_{\theta}\left(z \mid c_{\text {text }}, \hat{A}\right)} 
=\underset{\hat{A}}{\arg \max } \log p_{\theta}\left(z \mid c_{\text {text }}, \hat{A}\right)=\log N\left(f_{\theta}(z) ; \mu_{\theta}\left(c_{\text {text }}, \hat{A}\right), \sigma_{\theta}\left(c_{\text {text }}, \hat{A}\right)\right)
\end{array}
\]</span></p>
<h5 id="duration-prediction-from-text--时长模型">DURATION PREDICTION FROM TEXT--时长模型</h5>
<p>我们可以通过对估计对齐矩阵 <span class="math inline">\(A\)</span> 的每一行中的所有列求和来计算每个输入令牌的持续时间 <span class="math inline">\(d_i\)</span>：<span class="math inline">\(\sum_j A_{i,j}\)</span>. 该持续时间可以用来训练一个确定性的持续时间预测器，如之前的工作所提出的（Kim et al., 2020），但它不能表达一个人每次以不同语速发音的方式。为了生成类似于人类的语音节奏，我们设计了一个随机持续时间预测器，使其样本遵循给定条件变分自编码器在对抗学习环境下进行端到端文本转语音的音素持续时间分布。随机持续时间预测器是一个基于流的生成模型，通常通过最大似然估计进行训练。然而，直接应用最大似然估计是困难的，因为每个输入音素的持续时间是1）一个离散的整数，需要进行去量化以使用连续的归一化流，以及2）一个标量，由于可逆性限制了高维变换。我们应用变分去量化（Flow++）和变分数据增强（VFlow）来解决这些问题。</p>
<p>具体来说，我们引入了两个随机变量 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(\nu\)</span>，它们具有与持续时间序列 d 相同的时间分辨率和维度，用于变分去量化和变分数据增强。我们将 <span class="math inline">\(u\)</span> 的支持限制在 [0, 1) 之间，以使差值 d−u 变成一组正实数，并将 <span class="math inline">\(\nu\)</span> 和 d 逐通道地连接，以形成更高维的潜在表示。我们通过近似后验分布 <span class="math inline">\(q_{\phi}(u,\nu|d,c_{text})\)</span> 对这两个变量进行采样。所得到的目标是音素持续时间对数似然的变分下界： <span class="math display">\[
\begin{gathered}
\log p_{\theta}(d|c_{t e x t})\ge 
\mathbb{E}_{q_{\phi}(u,\nu|d,c_{t e x t})}\Big[\operatorname{log}\frac{p_{\theta}(d-u,\nu|c_{t e x t})}{q_{\phi}(u,\nu|d,c_{t e x t})}\Big] 
\end{gathered}
\]</span> 持续时间训练损失 <span class="math inline">\(L_{dur}\)</span> 是负变分下界。我们将停止梯度运算符应用于输入条件，以防止将输入的梯度反向传播，从而使持续时间预测器的训练不影响其他模块的训练。</p>
<p>采样过程相对简单；通过随机噪声经过随机持续时间预测器的逆变换，对音素持续时间进行采样，然后将其转换为整数。</p>
<h3 id="adversarial-training">2.3 Adversarial Training</h3>
<p>为了在我们的学习系统中采用对抗性训练，我们添加了一个鉴别器<span class="math inline">\(D\)</span>，用于区分生成器<span class="math inline">\(G\)</span>生成的输出和真实波形<span class="math inline">\(y\)</span>。在这项工作中，我们使用了两种在语音合成中成功应用的损失类型；用于对抗性训练的最小平方损失函数，以及用于训练生成器的额外特征匹配损失：</p>
<p><span class="math display">\[
\begin{gathered}
L_{adv}(D) =\mathbb{E}_{(y,z)}\Big[(D(y)-1)^{2}+(D(G(z)))^{2}\Big],  \\
L_{adv}(G) =\mathbb{E}_z\Big[(D(G(z))-1)^2\Big],  \\
L_{fm}(G) =\mathbb{E}_{(y,z)}\Big[\sum\limits_{l=1}^T\frac{1}{N_l}\|D^l(y)-D^l(G(z))\|_1\Big]
\end{gathered}
\]</span></p>
<h3 id="final-loss">2.4 final loss</h3>
<p><span class="math display">\[
L_{vae} = L_{recon} +L_{kl} +L_{dur} +L_{adv}(G) +L_{fm}(G)
\]</span></p>
<h2 id="architecture">3 architecture</h2>
<p>整个建议模型的架构包括后验编码器、先验编码器、解码器、鉴别器和随机持续时间预测器。后验编码器和鉴别器仅用于训练，而不是用于推理。</p>
<h3 id="后验编码器">3.1 后验编码器</h3>
<p>对于后验编码器，使用 WaveGlow 和 Glow-TTS 中使用的非因果 WaveNet 残差块。</p>
<p>WaveNet 残差块由带有门控激活单元和跳过连接的膨胀卷积层组成。块上方的线性投影层产生正态后验分布的均值和方差。对于多说话者情况，我们在残差块中使用全局条件化添加说话者嵌入。</p>
<h3 id="先验编码器">3.2 先验编码器</h3>
<p>先验编码器由处理输入音素 <span class="math inline">\(c_{text}\)</span> 的文本编码器和改善先验分布灵活性的归一化流 <span class="math inline">\(f_\theta\)</span> 组成。文本编码器是一个变压器编码器，它使用相对位置表示而不是绝对位置编码。我们可以通过文本编码器从 <span class="math inline">\(c_{text}\)</span> 获取隐藏表示 <span class="math inline">\(h_{text}\)</span>，并通过文本编码器上方的线性投影层产生用于构造先验分布的均值和方差。归一化流是由 WaveNet 残差块堆叠组成的仿射耦合层的堆叠)。为简化起见，我们设计归一化流为具有雅可比行列式为一的体积保持变换。对于多说话者设置，我们通过全局调节将说话者嵌入添加到归一化流中的残差块。</p>
<h3 id="解码器">3.3 解码器</h3>
<p>解码器本质上是 HiFi-GAN V1 生成器。它由一堆转置卷积组成，每个转置卷积后面都跟着一个多感受野融合模块（MRF）。MRF 的输出是具有不同接收字段大小的残差块输出的和。对于多说话者设置，我们添加一个线性层，将说话者嵌入转换并将其添加到输入潜变量<span class="math inline">\(z\)</span>。</p>
<h3 id="判别器">3.4 判别器</h3>
<p>我们遵循 HiFi-GAN 提出的<strong>多周期鉴别器的鉴别器架构</strong>。多周期鉴别器是一种马尔可夫窗口为基础的子鉴别器的混合，每一个子鉴别器都在不同周期模式的输入波形上操作。</p>
<h3 id="随机时长预测器">3.5 随机时长预测器</h3>
<p>随机持续时间预测器根据条件输入 <span class="math inline">\(h_{text}\)</span> 估计<strong>音素持续时间</strong>的分布。为了有效地参数化随机持续时间预测器，我们堆叠具有<strong>膨胀和深度可分离卷积层</strong>的残差块。我们还将神经样条流应用于耦合层，这些神经样条流采用单调有理二次样条实现可逆非线性变换。与常用的仿射耦合层相比，神经样条流在参数数量相似的情况下提高了变换的表达能力。对于多说话者设置，我们添加一个线性层，将说话者嵌入转换并将其添加到输入 <span class="math inline">\(h_{text}\)</span> 中。</p>
<h2 id="code">4 code</h2>
<p>https://github.com/jaywalnut310/vits</p>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">pytorch</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">project</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/2b566acf.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-用pytorch实现基础网络13-Glow(待完善)" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/c5690277.html">用pytorch实现基础网络13-Glow</a>
    </h2>
  

        
		
		  <a href="/post/c5690277.html" class="archive-article-date">
  	<time datetime="2023-05-06T10:56:32.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-05-06</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">266字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">1min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h1 id="glow-generative-flow-with-invertible-11-convolutions">Glow: Generative Flow with Invertible 11 Convolutions</h1>
<p>https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf</p>
<p>在这篇论文中，作者提出了一种名为Glow的流式生成模型。流式生成模型（Flow-based generative models）在计算精确对数似然（log-likelihood）、进行精确的潜在变量推断以及并行化训练和合成方面具有概念上的吸引力。Glow模型采用了一种可逆的 1×1 卷积操作，该方法相对简单。</p>
<p>通过使用Glow模型，作者在标准基准测试中实现了对数似然的显著改进。更令人瞩目的是，作者证明了优化纯对数似然目标的基于流的生成模型能够高效地生成和操作大型图像，且生成的图像具有逼真的外观。这表明，Glow这种类型的流式生成模型在生成逼真图像方面具有很大的潜力。</p>
<ul>
<li>显式表示目标分布</li>
<li>可逆</li>
</ul>
<p>数据的分布：得到对应的似然(给定模型下，数据的似然)</p>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">pytorch</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">network</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/c5690277.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-pytorch项目5-CLIP搭建相似图像检索系统" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/342f7c36.html">pytorch项目5-CLIP搭建相似图像检索系统</a>
    </h2>
  

        
		
		  <a href="/post/342f7c36.html" class="archive-article-date">
  	<time datetime="2023-05-05T11:22:46.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-05-05</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">1.4k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">7min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h2 id="准备工作">1 准备工作</h2>
<h3 id="clip模型的调用">1.1 clip模型的调用</h3>
<p>https://github.com/openai/CLIP</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">conda install --<span class="built_in">yes</span> -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install ftfy regex tqdm</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install git+https://github.com/openai/CLIP.git</span></span><br></pre></td></tr></table></figure>
<p><code>clip.available_models()</code> Returns the names of the available CLIP models.</p>
<h3 id="准备数据集">1.2 准备数据集</h3>
<p>与项目三中使用相同的数据集，这个数据集的划分代码略 <a href="https://wangtongyouwen.github.io/post/4627104a.html">[pytorch项目3-基于ResNet的水果蔬菜分类](https://wangtongyouwen.github.io/post/4627104a.html)</a></p>
<h2 id="train.py">2 train.py</h2>
<h3 id="get_args_parser">2.1 get_args_parser()</h3>
<p>这部分是为了能够接收从命令行中读取的参数，其中最关键的是几个数据的路径：训练样本，测试样本，输出的结果照片以及推断出的模型feature_dict</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_args_parser</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;image search task&#x27;</span>, add_help=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># Model parameters</span></span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--input_size&#x27;</span>, default=<span class="number">128</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;images input size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dateset_dir&#x27;</span>, default=<span class="string">&#x27;./dataset_fruit_veg/train&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to load images &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--test_image_dir&#x27;</span>, default=<span class="string">&#x27;./dataset_fruit_veg/val_images&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;images to test, split by comma &quot;,&quot;&#x27;</span>)  <span class="comment"># split from the test dataset, there is no Crossover</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_dir&#x27;</span>, default=<span class="string">&#x27;./output_di&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;path where to save, empty for no saving&#x27;</span>) <span class="comment"># 相似照片</span></span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--model_name&#x27;</span>,default=<span class="string">&#x27;resnet50&#x27;</span>, <span class="comment"># resnet50,resnet152,clip</span></span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;model name&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--feature_dict_file&#x27;</span>,default=<span class="string">&#x27;corpus_feature_dict,npy&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;filename where to save image representations&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--topk&#x27;</span>,default=<span class="number">7</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;k most similar image representations&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--mode&#x27;</span>,default=<span class="string">&#x27;extract&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;extract or predict, for extracting features or predicting similar images from corpus&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure>
<h3 id="main">2.2 main</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model_names = timm.list_models(pretrained=<span class="literal">True</span>)</span><br><span class="line">    args = get_args_parser()</span><br><span class="line">    args = args.parse_args()</span><br><span class="line"></span><br><span class="line">    model = <span class="literal">None</span></span><br><span class="line">    preprocess = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.model_name != <span class="string">&quot;clip&quot;</span>:</span><br><span class="line">        model = timm.create_model(args.model_name, pretrained=<span class="literal">True</span>)  <span class="comment"># resnet50 resnet152</span></span><br><span class="line">        n_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;number of trainable params (M): %.2f&quot;</span> % (n_parameters / <span class="number">1.e6</span>))</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model, preprocess = clip.load(<span class="string">&quot;ViT-B/32&quot;</span>, device=device)  <span class="comment"># preprocess就是已经归一化后的结果，无需再进行归一化操作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.mode == <span class="string">&quot;extract&quot;</span>:</span><br><span class="line">        <span class="comment"># 第一阶段：图像表征提取</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;use pretrained model <span class="subst">&#123;args.model_name&#125;</span> to extract features&quot;</span>)</span><br><span class="line">        allVectors = extract_features(args, model, image_path=args.dataset_dir, preprocess=preprocess)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 第二阶段：图像检索</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;use pretrained model <span class="subst">&#123;args.model_name&#125;</span> to search <span class="subst">&#123;args.topk&#125;</span> similar images from corpus&quot;</span>)</span><br><span class="line"></span><br><span class="line">        test_images = glob.glob(os.path.join(args.test_image_dir, <span class="string">&quot;*.png&quot;</span>))</span><br><span class="line">        test_images += glob.glob(os.path.join(args.test_image_dir, <span class="string">&quot;*.jpg&quot;</span>))</span><br><span class="line">        test_images += glob.glob(os.path.join(args.test_image_dir, <span class="string">&quot;*.jpeg&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loading image representation dictionary</span></span><br><span class="line">        allVectors = np.load(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>/<span class="subst">&#123;args.feature_dict_file&#125;</span>&quot;</span>, allow_pickle=<span class="literal">True</span>)  <span class="comment"># 导入字典</span></span><br><span class="line">        allVectors = allVectors.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reading test images</span></span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> tqdm.tqdm(test_images):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;reading <span class="subst">&#123;image_file&#125;</span>...&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> args.model_name == <span class="string">&quot;clip&quot;</span>:</span><br><span class="line">                <span class="comment"># CLIP model</span></span><br><span class="line">                allVectors[image_file] = extract_feature_by_CLIP(model, preprocess, image_file)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># resnet50, resnet152</span></span><br><span class="line">                allVectors[image_file] = extract_feature_single(args, model, image_file)</span><br><span class="line"></span><br><span class="line">        sim, keys = getSimilarityMatrix(allVectors)</span><br><span class="line">        <span class="built_in">print</span>(keys)</span><br><span class="line">        result = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> tqdm.tqdm(test_images):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;sorting most similar images as <span class="subst">&#123;image_file&#125;</span>...&quot;</span>)</span><br><span class="line">            index = keys.index(image_file)</span><br><span class="line">            sim_vec = sim[index]</span><br><span class="line"></span><br><span class="line">            indexs = np.argsort(sim_vec)[::-<span class="number">1</span>][<span class="number">1</span>:args.topk]  <span class="comment"># 排序：从小到大的索引</span></span><br><span class="line">            simImages, simScores = [], []</span><br><span class="line">            <span class="keyword">for</span> ind <span class="keyword">in</span> indexs:</span><br><span class="line">                simImages.append(keys[ind])</span><br><span class="line">                simScores.append(sim_vec[ind])</span><br><span class="line">            result[image_file] = (simImages, simScores)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;starting to show similar images...&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> test_images:</span><br><span class="line">            plotSimilarImages(args, image_file, result[image_file][<span class="number">0</span>], result[image_file][<span class="number">1</span>], numRow=<span class="number">1</span>,</span><br><span class="line">                              numCol=args.topk)</span><br></pre></td></tr></table></figure>
<ul>
<li>判断模型是resnet50，resnet152 or clip</li>
<li>判断模式是否是 extract or predict</li>
</ul>
<p>predict: 部分代码的步骤：</p>
<ol type="1">
<li>读取图片，判断模型类型，读取特征字典</li>
<li>将目标图片与特征图片中的所有值做余弦相似度比较，得到的结果矩阵：allvector</li>
<li>排序，取出前n个目标</li>
<li>把图片地址和图片计算得到的score储存，最后进行输出</li>
</ol>
<h2 id="extract">3 extract</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_single</span>(<span class="params">args, model, file</span>):</span><br><span class="line">    img_rgb = Image.<span class="built_in">open</span>(file).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">    image = img_rgb.resize((args.input_size, args.input_size), Image.LANCZOS)</span><br><span class="line">    image = torchvision.transforms.ToTensor()(image)</span><br><span class="line"></span><br><span class="line">    trainset_mean = [<span class="number">0.4729932</span>, <span class="number">0.43474569</span>, <span class="number">0.3264319</span>]</span><br><span class="line">    trainset_std = [<span class="number">0.37707761</span>, <span class="number">0.36121109</span>, <span class="number">0.34872371</span>]</span><br><span class="line"></span><br><span class="line">    image = torchvision.transforms.Normalize(mean=trainset_mean, std=trainset_std)(image).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        features = model.forward_features(image)</span><br><span class="line">        vec = model.global_pool(features)</span><br><span class="line">        vec = vec.squeeze().numpy()</span><br><span class="line"></span><br><span class="line">    img_rgb.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vec</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>这部分代码将先对图像进行归一化，然后在模型的forward_features层输出特征，在进入池化层最后得到代表图片的字典信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_by_CLIP</span>(<span class="params">model, preprocess, file</span>):</span><br><span class="line">    image = preprocess(Image.<span class="built_in">open</span>(file)).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        vec = model.encode_image(image)</span><br><span class="line">        vec = vec.squeeze().cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vec</span><br></pre></td></tr></table></figure>
<ul>
<li>使用CLIP中的encode_image输出特征信息</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_features</span>(<span class="params">args, model, image_path=<span class="string">&quot;&quot;</span>, preprocess=<span class="literal">None</span></span>):</span><br><span class="line">    allVectors = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> image_file <span class="keyword">in</span> tqdm.tqdm(glob.glob(os.path.join(image_path, <span class="string">&quot;*&quot;</span>, <span class="string">&quot;*.jpg&quot;</span>))):  <span class="comment"># image_path:train # tqdm:进度条</span></span><br><span class="line">        <span class="keyword">if</span> args.model_name == <span class="string">&quot;clip&quot;</span>:</span><br><span class="line">            allVectors[image_file] = extract_feature_by_CLIP(model, preprocess, image_file)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            allVectors[image_file] = extract_feature_single(args, model, image_file)</span><br><span class="line"></span><br><span class="line">    os.makedirs(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>/<span class="subst">&#123;args.feature_dict_file&#125;</span>&quot;</span>, allVectors)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> allVectors</span><br></pre></td></tr></table></figure>
<h2 id="计算余弦相似度">4 计算余弦相似度</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算余弦相似度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getSimilarityMatrix</span>(<span class="params">vectors_dict</span>):</span><br><span class="line">    v = np.array(<span class="built_in">list</span>(vectors_dict.values()))  <span class="comment"># [NUM,H]</span></span><br><span class="line">    numerator = np.matmul(v, v.T)  <span class="comment"># [NUM,NUM]</span></span><br><span class="line">    denominator = np.matmul(np.linalg.norm(v, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>),</span><br><span class="line">                            np.linalg.norm(v, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>).T)  <span class="comment"># [NUM,NUM]</span></span><br><span class="line"></span><br><span class="line">    sim = numerator / denominator</span><br><span class="line">    keys = <span class="built_in">list</span>(vectors_dict.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sim, keys</span><br></pre></td></tr></table></figure>
<h2 id="绘制结果">5 绘制结果</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setAxes</span>(<span class="params">ax, image, query=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    value = kwargs.get(<span class="string">&quot;value&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> query:</span><br><span class="line">        ax.set_xlabel(<span class="string">&quot;Query Image\n&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(image), fontsize=<span class="number">12</span>)</span><br><span class="line">        ax.xaxis.label.set_color(<span class="string">&quot;red&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.set_xlabel(<span class="string">&quot;score=&#123;1:1.3f&#125;\n&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(image, value), fontsize=<span class="number">12</span>)</span><br><span class="line">        ax.xaxis.label.set_color(<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotSimilarImages</span>(<span class="params">args, image, simImages, simValues, numRow=<span class="number">1</span>, numCol=<span class="number">4</span></span>):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    <span class="comment"># set width and height in inches</span></span><br><span class="line"></span><br><span class="line">    fig.set_size_inches(<span class="number">18.5</span>, <span class="number">10.5</span>)</span><br><span class="line">    fig.suptitle(<span class="string">f&quot;use engine model: <span class="subst">&#123;args.model_name&#125;</span>&quot;</span>, fontsize=<span class="number">35</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, numCol * numRow):</span><br><span class="line">        ax = []</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(image)</span><br><span class="line">            ax = fig.add_subplot(numRow, numCol, <span class="number">1</span>)</span><br><span class="line">            setAxes(ax, image.split(os.sep)[-<span class="number">1</span>], query=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(simImages[j - <span class="number">1</span>])</span><br><span class="line">            ax.append(fig.add_subplot(numRow,numCol,j+<span class="number">1</span>))</span><br><span class="line">            setAxes(ax[-<span class="number">1</span>],<span class="string">&quot;_&quot;</span>.join(simImages[j-<span class="number">1</span>].split(os.sep)[-<span class="number">2</span>:]),value=simValues[j-<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># truncated_filename = &quot;&quot;.join(simImages[j - 1].split(os.sep)[-2:])</span></span><br><span class="line">            <span class="comment"># truncated_filename = truncated_filename[:15]  # 只显示前 15 个字符</span></span><br><span class="line">            <span class="comment"># setAxes(ax[-1], truncated_filename, value=simValues[j - 1])</span></span><br><span class="line">        img = img.convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        plt.imshow(img)</span><br><span class="line">        img.close()</span><br><span class="line">    fig.savefig(<span class="string">f&quot;<span class="subst">&#123;args.save_dir&#125;</span>/<span class="subst">&#123;args.model_name&#125;</span>_search_top_<span class="subst">&#123;args.topk-<span class="number">1</span>&#125;</span>_<span class="subst">&#123;image.split(os.sep)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]&#125;</span>.png&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>这部分展示代码具有参考性，可以用来动态显示结果。</li>
</ul>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">pytorch</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">project</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/342f7c36.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-pytorch进阶2-improved-diffusion-model" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/954ae01.html">pytorch进阶2-autoregressive diffusion model</a>
    </h2>
  

        
		
		  <a href="/post/954ae01.html" class="archive-article-date">
  	<time datetime="2023-05-04T09:59:09.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-05-04</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">669字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">3min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <p>Improved Denoising Diffusion Probabilistic Models</p>
<p>http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf</p>
<p>Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting</p>
<p>http://proceedings.mlr.press/v139/rasul21a/rasul21a.pdf</p>
<p>Denoising Diffusion Probabilistic Models</p>
<p>https://arxiv.org/pdf/2006.11239.pdf</p>
<h2 id="回顾">回顾</h2>
<h3 id="如何将扩散模型与自回归模型结合">1 如何将扩散模型与自回归模型结合？</h3>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041925965.png" alt="image-20230504192539224" /><figcaption aria-hidden="true">image-20230504192539224</figcaption>
</figure>
<p>目标函数发生了变化： <span class="math display">\[
\mathbb{E}_{\mathbf{x}^0_t,\epsilon,n}\left[\|\epsilon-\epsilon_\theta(\sqrt{\bar{\alpha}_n}\mathbf{x}^0_t+\sqrt{1-\bar{\alpha}_n}\epsilon,\mathbf{h}_{t-1},n)\|^2\right],
\]</span> <img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041930347.png" alt="image-20230504192959481" /></p>
<ul>
<li><p>为什么使用采样？而不是按照顺序</p>
<p>这是与随机梯度下降算法相同，目的是为了使训练更加鲁棒</p></li>
</ul>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041932387.png" alt="image-20230504193228721" /><figcaption aria-hidden="true">image-20230504193228721</figcaption>
</figure>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041943539.png" alt="image-20230504194302000" /><figcaption aria-hidden="true">image-20230504194302000</figcaption>
</figure>
<h3 id="improved-denoising-diffusion-probabilistic-models">2 Improved Denoising Diffusion Probabilistic Models</h3>
<ul>
<li>可学习的方差<span class="math inline">\(\sum_\theta(x_t,t)\)</span></li>
<li>噪声方案的改进</li>
</ul>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041946113.png" alt="image-20230504194648255" /><figcaption aria-hidden="true">image-20230504194648255</figcaption>
</figure>
<ul>
<li>损失函数的改进</li>
</ul>
<p><img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305041948618.png" alt="image-20230504194753966" /> <span class="math display">\[
L_{hybrid} = L_{simple} + \lambda L_{vlb}
\]</span></p>
<h3 id="思维导图">3 思维导图</h3>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305042024133.png" alt="image-20230504202447257" /><figcaption aria-hidden="true">image-20230504202447257</figcaption>
</figure>
<h2 id="代码分析">代码分析</h2>
<p>https://github.com/openai/improved-diffusion</p>
<h3 id="image_train.py">1 image_train.py</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_argparser</span>():</span><br><span class="line">    defaults = <span class="built_in">dict</span>(</span><br><span class="line">        data_dir=<span class="string">&quot;&quot;</span>,</span><br><span class="line">        schedule_sampler=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">        lr=<span class="number">1e-4</span>,</span><br><span class="line">        weight_decay=<span class="number">0.0</span>,</span><br><span class="line">        lr_anneal_steps=<span class="number">0</span>,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        microbatch=-<span class="number">1</span>,  <span class="comment"># -1 disables microbatches</span></span><br><span class="line">        ema_rate=<span class="string">&quot;0.9999&quot;</span>,  <span class="comment"># comma-separated list of EMA values</span></span><br><span class="line">        log_interval=<span class="number">10</span>,</span><br><span class="line">        save_interval=<span class="number">10000</span>,</span><br><span class="line">        resume_checkpoint=<span class="string">&quot;&quot;</span>,</span><br><span class="line">        use_fp16=<span class="literal">False</span>,</span><br><span class="line">        fp16_scale_growth=<span class="number">1e-3</span>,</span><br><span class="line">    )</span><br><span class="line">    defaults.update(model_and_diffusion_defaults())</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    add_dict_to_argparser(parser, defaults)</span><br><span class="line">    <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_dict_to_argparser</span>(<span class="params">parser, default_dict</span>):</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> default_dict.items():</span><br><span class="line">        v_type = <span class="built_in">type</span>(v)</span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            v_type = <span class="built_in">str</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(v, <span class="built_in">bool</span>):</span><br><span class="line">            v_type = str2bool</span><br><span class="line">        parser.add_argument(<span class="string">f&quot;--<span class="subst">&#123;k&#125;</span>&quot;</span>, default=v, <span class="built_in">type</span>=v_type)</span><br></pre></td></tr></table></figure>
<ul>
<li>这一部分的代码可以在命令行中传参进行简化：传入一个字典自动解析。从字典中自动生成argument parser</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    args = create_argparser().parse_args()</span><br><span class="line"></span><br><span class="line">    dist_util.setup_dist()</span><br><span class="line">    logger.configure()</span><br><span class="line"></span><br><span class="line">    logger.log(<span class="string">&quot;creating model and diffusion...&quot;</span>)</span><br><span class="line">    model, diffusion = create_model_and_diffusion(</span><br><span class="line">        **args_to_dict(args, model_and_diffusion_defaults().keys())</span><br><span class="line">    )</span><br><span class="line">    model.to(dist_util.dev())</span><br><span class="line">    schedule_sampler = create_named_schedule_sampler(args.schedule_sampler, diffusion)</span><br><span class="line"></span><br><span class="line">    logger.log(<span class="string">&quot;creating data loader...&quot;</span>)</span><br><span class="line">    data = load_data(</span><br><span class="line">        data_dir=args.data_dir,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        image_size=args.image_size,</span><br><span class="line">        class_cond=args.class_cond,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    logger.log(<span class="string">&quot;training...&quot;</span>)</span><br><span class="line">    TrainLoop(</span><br><span class="line">        model=model,</span><br><span class="line">        diffusion=diffusion,</span><br><span class="line">        data=data,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        microbatch=args.microbatch,</span><br><span class="line">        lr=args.lr,</span><br><span class="line">        ema_rate=args.ema_rate,</span><br><span class="line">        log_interval=args.log_interval,</span><br><span class="line">        save_interval=args.save_interval,</span><br><span class="line">        resume_checkpoint=args.resume_checkpoint,</span><br><span class="line">        use_fp16=args.use_fp16,</span><br><span class="line">        fp16_scale_growth=args.fp16_scale_growth,</span><br><span class="line">        schedule_sampler=schedule_sampler,</span><br><span class="line">        weight_decay=args.weight_decay,</span><br><span class="line">        lr_anneal_steps=args.lr_anneal_steps,</span><br><span class="line">    ).run_loop()</span><br></pre></td></tr></table></figure>
<p>传参-&gt;获得模型-&gt;获得数据-&gt;训练</p>
<p>...improved-diffusion-main_diffusion_util.py</p>
<h3 id="create_model_and_diffusion">2 create_model_and_diffusion</h3>
<h4 id="create_gaussian_diffusion">2.1 create_gaussian_diffusion</h4>
<ul>
<li>生成扩散过程的模型框架</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_gaussian_diffusion</span>(<span class="params"></span></span><br><span class="line"><span class="params">    *,</span></span><br><span class="line"><span class="params">    steps=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">    learn_sigma=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    sigma_small=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    noise_schedule=<span class="string">&quot;linear&quot;</span>,</span></span><br><span class="line"><span class="params">    use_kl=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    predict_xstart=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    rescale_timesteps=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    rescale_learned_sigmas=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    timestep_respacing=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    betas = gd.get_named_beta_schedule(noise_schedule, steps)</span><br><span class="line">    <span class="keyword">if</span> use_kl:</span><br><span class="line">        loss_type = gd.LossType.RESCALED_KL</span><br><span class="line">    <span class="keyword">elif</span> rescale_learned_sigmas:</span><br><span class="line">        loss_type = gd.LossType.RESCALED_MSE</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loss_type = gd.LossType.MSE</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> timestep_respacing:</span><br><span class="line">        timestep_respacing = [steps]</span><br><span class="line">    <span class="keyword">return</span> SpacedDiffusion(</span><br><span class="line">        use_timesteps=space_timesteps(steps, timestep_respacing),</span><br><span class="line">        betas=betas,</span><br><span class="line">        model_mean_type=(</span><br><span class="line">            gd.ModelMeanType.EPSILON <span class="keyword">if</span> <span class="keyword">not</span> predict_xstart <span class="keyword">else</span> gd.ModelMeanType.START_X</span><br><span class="line">        ),</span><br><span class="line">        model_var_type=(</span><br><span class="line">            (</span><br><span class="line">                gd.ModelVarType.FIXED_LARGE</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> sigma_small</span><br><span class="line">                <span class="keyword">else</span> gd.ModelVarType.FIXED_SMALL</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> learn_sigma</span><br><span class="line">            <span class="keyword">else</span> gd.ModelVarType.LEARNED_RANGE</span><br><span class="line">        ),</span><br><span class="line">        loss_type=loss_type,</span><br><span class="line">        rescale_timesteps=rescale_timesteps,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>get_named_beta_schedule 生成betas的策略 linear increasing/cosine</li>
<li>class SpacedDiffusion(GaussianDiffusion)</li>
<li>GaussianDiffusion</li>
</ol>
<p>:param betas: a 1-D numpy array of betas for each diffusion timestep,starting at T and going to 1. :param model_mean_type: a ModelMeanType determining what the model outputs. :param model_var_type: a ModelVarType determining how variance is output. :param loss_type: a LossType determining the loss function to use. :param rescale_timesteps: if True, pass floating point timesteps into the model so that they are always scaled like in the original paper (0 to 1000).</p>
<ol start="4" type="1">
<li>p_mean_variance</li>
<li>_vb_terms_bpd</li>
<li>training_losses</li>
</ol>
<h4 id="unet">2.2 Unet</h4>
<p>详见Unet(笔记)</p>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">pytorch</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">diffusion model</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/954ae01.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-pytorch进阶1-DDPM" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/8c9cf490.html">pytorch进阶1-DDPM</a>
    </h2>
  

        
		
		  <a href="/post/8c9cf490.html" class="archive-article-date">
  	<time datetime="2023-05-03T11:07:37.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-05-03</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">5.3k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">28min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <p>Denoising Diffusion Probabilistic Models</p>
<p>https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf</p>
<p>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</p>
<p>http://proceedings.mlr.press/v37/sohl-dickstein15.pdf</p>
<h2 id="生成模型">1 生成模型</h2>
<ul>
<li>seq2seq</li>
<li>gan</li>
<li>flow（数学推理严谨）</li>
<li>VAE</li>
<li>diffusion model</li>
</ul>
<h2 id="条件概率公式与高斯分布的kl散度">2 条件概率公式与高斯分布的KL散度</h2>
<h3 id="条件概率的一般形式">2.1 条件概率的一般形式</h3>
<p><span class="math display">\[
P(A,B,C)=P(C|B,A)P(B,A) = P(C|B,A)P(B|A)P(A) \\
P(B,C|A) = P(B|A)P(C|A,B)
\]</span></p>
<h3 id="基于马尔科夫假设的条件概率">2.2 基于马尔科夫假设的条件概率</h3>
<p>如果满足马尔科夫链关系 A-&gt;B-&gt;C,那么有： <span class="math display">\[
P(A,B,C)=P(C|B,A)P(B,A)=P(C|B)P(B|A)P(A) \\
P(B,C|A)=P(B|A)P(C|B)
\]</span></p>
<h3 id="高斯分布的kl散度公式">2.3 高斯分布的KL散度公式</h3>
<p>对于两个单一变量的高斯分布p和q而言，他们的KL散度为 <span class="math display">\[
KL(p,q) = log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
\]</span></p>
<h3 id="参数重整化">2.4 参数重整化</h3>
<p>若希望从高斯分布<span class="math inline">\(N(\mu,\sigma^2)\)</span>中采样，可以先从标准分布<span class="math inline">\(N(0,1)\)</span>采样出<span class="math inline">\(z\)</span>，再得到<span class="math inline">\(\sigma*z+\mu\)</span>。这样做的好处是将随机性转移到了<span class="math inline">\(z\)</span>这个变量上，而<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>则当做仿射变换网络的一部分</p>
<h2 id="vae与多层vae回顾">3 VAE与多层VAE回顾</h2>
<p>https://zhuanlan.zhihu.com/p/34998569</p>
<h3 id="单层vae的原理公式与置信下界">3.1 单层VAE的原理公式与置信下界</h3>
<p><span class="math display">\[
p(x)=\int_z p_{\theta}(x|z)p(z)\\
p(x)=\int_z q_{\phi}(z|x) \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \\
\log p(x)=\log \mathbb{E}_{z \sim q_{\phi}(z \mid x)}\left[\frac{p_{\theta}(x \mid z) p(z)}{q_{\phi}(z \mid x)}\right] \\
\log p(x) \geq \mathbb{E}_{z \sim q_{\phi}(z \mid x)}\left[\log \frac{p_{\theta}(x \mid z) p(z)}{q_{\phi}(z \mid x)}\right]
\]</span></p>
<h3 id="多层vae的原理公式与置信下界">3.2 多层VAE的原理公式与置信下界</h3>
<p><span class="math display">\[
\begin{array}{c}
p(x)=\int_{z_{1}} \int_{z_{2}} p_{\theta}\left(x, z_{1}, z_{2}\right) d z_{1}, d z_{2} \\
p(x)=\iint q_{\phi}\left(z_{1}, z_{2} \mid x\right) \frac{p_{\theta}\left(x, z_{1}, z_{2}\right)}{q_{\phi}\left(z_{1}, z_{2} \mid x\right)} \\
p(x)=\mathbb{E}_{z_{1}, z_{2} \sim q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\left[\frac{p_{\theta}\left(x, z_{1}, z_{2}\right)}{q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\right] \\
\log p(x) \geq \mathbb{E}_{z_{1}, z_{2} \sim q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\left[\log \frac{p_{\theta}\left(x, z_{1}, z_{2}\right)}{q_{\phi}\left(z_{1}, z_{2} \mid x\right)}\right] \\
p\left(x, z_{1}, z_{2}\right)=p\left(x \mid z_{1}\right) p\left(z_{1} \mid z_{2}\right) p\left(z_{2}\right) \\
q\left(z_{1}, z_{2} \mid x\right)=q\left(z_{1} \mid x\right) q\left(z_{2} \mid z_{1}\right) \\
\mathcal{L}(\theta, \phi)=\mathbb{E}_{q(21,2 q \mid x)}\left[\log p\left(x \mid z_{1}\right)-\log q\left(z_{1} \mid x\right)+\log p\left(z_{1} \mid z_{2}\right)-\log q\left(z_{2} \mid z_{1}\right)+\log p\left(z_{2}\right)\right]
\end{array}
\]</span></p>
<h2 id="diffusion-model">4 diffusion model</h2>
<p>从目标分布中，希望能找到逆扩散过程的规律，然后利用这个规律将任意噪声恢复。</p>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305032040155.png" alt="image-20221112164034826" /><figcaption aria-hidden="true">image-20221112164034826</figcaption>
</figure>
<h3 id="前向过程">4.1 前向过程</h3>
<p>不断往数据中加入噪声，最后得到了纯噪声（扩散过程）</p>
<p>每个时刻都要添加高斯噪声，后一时刻都是有前一时刻增加噪声得到的；</p>
<p>其实这个过程可以看作不断<strong>构建标签</strong>（噪声）的过程</p>
<ol type="1">
<li>给定初始数据分布<span class="math inline">\(x_0 \thicksim q(x)\)</span>,可以不断向分布中添加高斯噪声，该噪声的标准差是以固定值<span class="math inline">\(\beta_t\)</span>而确定的，均值是以固定值值<span class="math inline">\(\beta_t\)</span>和当前t时刻的数据<span class="math inline">\(x_t\)</span>决定的。这个过程是一个马尔科夫链过程。</li>
<li>随着t的不断增大，最终数据分布<span class="math inline">\(x_T\)</span>变成了一个各向独立的高斯分布。</li>
</ol>
<p>Given a data point sampled from a real data distribution <span class="math inline">\(x_0 \thicksim q(x)\)</span>,let us define a forward diffusion process in which we add small amount of Gaussian noise to the sample in <span class="math inline">\(T\)</span> steps, producing a sequence of noisy samples <span class="math inline">\(x_1,\dots,x_T\)</span>. The step sizes are controlled by a variance schedule <span class="math inline">\({\beta_t \in (0,1)}^t_{t=1}\)</span></p>
<p>给定一个从真实数据分布中采样的数据点<span class="math inline">\(x_0 \thicksim q(x)\)</span>，让我们定义一个向前扩散过程，在该过程中，我们在<span class="math inline">\(T\)</span>步中向样本添加少量的高斯噪声，产生一系列噪声样本<span class="math inline">\(x_1，\dots，x_T\)</span>。步长由方差时间表<span class="math inline">\({\beta_t \in (0,1)}^t_{t=1}\)</span>控制。 <span class="math display">\[
q(X_t|X_{t-1})=\N(X_t;\sqrt{1-\beta_t}X_{t-1},\beta_t I) \\
q(X_{1:T}|X_0) = \prod^T_{t=1} q(X_t|X_{t-1})
\]</span> The data sample <span class="math inline">\(x_0\)</span> gradually loses its distinguishable features as the step <span class="math inline">\(t\)</span> becomes larger. Eventually when <span class="math inline">\(T \rightarrow \infty,X_T\)</span> is euivalent to an isotropic Gaussian distribution</p>
<p>随着步骤<span class="math inline">\(t\)</span>变得更大，数据样本<span class="math inline">\(x_0\)</span>逐渐失去其可区分的特征。最终当<span class="math inline">\(T \rightarrow \infty\)</span>时，<span class="math inline">\(X_T\)</span>等同于各向同性的高斯分布。</p>
<ol start="3" type="1">
<li>任何时刻的<span class="math inline">\(q(X_t)\)</span>推导也可以完全基于<span class="math inline">\(x_0\)</span>和<span class="math inline">\(\beta _t\)</span>来计算出来，而不需要做迭代</li>
</ol>
<p>注意这里，两个正态分布 <span class="math inline">\(X\thicksim N(\mu_1,\sigma_1)\)</span> 和 <span class="math inline">\(Y\thicksim N(\mu_2,\sigma_2)\)</span>的叠加后的分布<span class="math inline">\(aX+bY\)</span>的均值为<span class="math inline">\(a\mu_1+b\mu_2\)</span>，方差为<span class="math inline">\(a^2\sigma_1^2+b^2\sigma_2^2\)</span>，所以<span class="math inline">\(\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}z_{t-2}+\sqrt{1-\alpha_tz_{t-1}}z_{t-1}\)</span>可以参数重整化为只含有一个随机变量<span class="math inline">\(z\)</span>构成的<span class="math inline">\(\sqrt{1-\alpha_t\alpha_{t-1}}z\)</span>的形式</p>
<h4 id="第一个公式如何得到x_t时刻的分布前向过程">第一个公式，如何得到<span class="math inline">\(X_t\)</span>时刻的分布（前向过程）？</h4>
<p><span class="math display">\[
\alpha _t = 1- \beta_t
\]</span></p>
<p>式（6）中<span class="math inline">\(\beta_t\)</span>表示图像的加入噪声的程度，随着训练的进行要逐渐增大，论文中从0.0001到0.002扩大。 <span class="math display">\[
x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_1
\]</span> 式(7)中<span class="math inline">\(z_1\)</span>表示噪声（高斯噪声） <span class="math display">\[
\begin{align}
          x_t &amp; = \sqrt{\alpha _t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}z_{2})+\sqrt{1-\alpha_t}z_1 \\
           &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\bar z_2 \\
           &amp; = \sqrt{\bar \alpha_t}x_0+\sqrt{1-\bar \alpha_t} z_t
          \end{align}
\]</span> 其中<span class="math inline">\(z_1和z_2\)</span>分别表示<span class="math inline">\(N(0,1-\alpha_t)和N(0,\alpha_t(1-\alpha_{t-1}))\)</span>，独立同分布可相加得到，最后的<span class="math inline">\(\bar z_2\)</span></p>
<p>其中<span class="math inline">\(\bar \alpha_t\)</span>表示累乘<span class="math inline">\(\bar \alpha_t=\alpha_1\cdot \alpha_2 \cdot \dots \alpha_{t-1}\cdot \alpha_{t}\)</span> <span class="math display">\[
q(X_t|X_0) = \N(X_t;\sqrt{\bar \alpha_t}X_0,(1-\bar \alpha_t)I)
\]</span> Usually, we can afford a larger update step when the sample gets noisier, so <span class="math inline">\(\beta_1&lt;\beta_2&lt;...&lt;\beta_T\)</span> and therfore <span class="math inline">\(\bar \alpha_1 &gt; \dots \bar \alpha_T\)</span></p>
<h3 id="逆向过程">4.2 逆向过程</h3>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305032044361.png" alt="image-20221112171756756" /><figcaption aria-hidden="true">image-20221112171756756</figcaption>
</figure>
<p>逆过程是从高斯噪声中恢复原始数据，我们可以假设它也是一个高斯分布，但是无法逐步地去拟合分布，所以需要构建一个参数分布来去做估计。逆扩散过程仍然是一个马尔科夫链过程。 <span class="math display">\[
p_\theta(X_{0:T}) = p(X_T)\prod^T_{t=1}p_\theta(X_{t-1}|X_t)\\
p_\theta(X_{t-1}|X_t) = \N (X_{t-1};\mu_{\theta}(X_t,t),\Sigma_{\theta}(x_t,t))
\]</span> If we can reverse the above process and sample from <span class="math inline">\(q(X_{t-1}|X_t)\)</span>, we will be able to recreate the true sample from a Gaussian noise input, <span class="math inline">\(X_T \thicksim \mathcal{N}(0,I)\)</span>. Note that if <span class="math inline">\(\beta_t\)</span> is small enough, <span class="math inline">\(q(X_{t-1}|X_t)\)</span> will also be Gaussian. Unfortunately, we cannot easily estimate <span class="math inline">\(q(X_{t-1}|X_t)\)</span> because it needs to use the entire dataset adn therefore we need to learn a model <span class="math inline">\(p_\theta\)</span> to approximate these conditional probabilities in order to run reverse diffusion process.</p>
<p>如果我们能够逆转上述过程并从<span class="math inline">\(q(X_{t-1}|X_t)\)</span>中抽样，我们将能够从高斯噪声输入<span class="math inline">\(X_T \thicksim \mathcal{N}(0,I)\)</span>中重建真实样本。请注意，如果<span class="math inline">\(\beta_t\)</span>足够小，<span class="math inline">\(q(X_{t-1}|X_t)\)</span>也将是高斯分布。不幸的是，我们无法轻易估计<span class="math inline">\(q(X_{t-1}|X_t)\)</span>，因为它需要使用整个数据集，因此我们需要学习一个模型<span class="math inline">\(p_\theta\)</span>来逼近这些条件概率，以便运行反向扩散过程。</p>
<h3 id="后验的扩散条件概率">4.3 后验的扩散条件概率</h3>
<p><span class="math inline">\(q(X_{t-1}|X_t,X_0)\)</span>分布式可以用公式表示的，可就是说给定<span class="math inline">\(X_t,X_0\)</span>，就能够计算出<span class="math inline">\(X_{t-1}\)</span> <span class="math display">\[
q(X_{t-1}|X_t) = \N(X_{t-1};\bar \mu(X_t,X_0),\bar \beta_t I)
\]</span></p>
<p><span class="math display">\[
q(X_{t-1}|X_t,X_0)=q(X_t|X_{t-1},X_0)\frac{q(X_{t-1}|X_0)}{q((X_t|X_0))}
\]</span></p>
<p><span class="math display">\[
q(X_{t-1}|X_0)\Longrightarrow    \sqrt{\bar \alpha_{t-1}}x_0+\sqrt{1-\bar \alpha_{t-1}}z \sim N(\sqrt{\bar \alpha_{t-1}}x_0,1-\bar \alpha_{t-1})
\]</span></p>
<p><span class="math display">\[
q(X_{t}|X_0)\Longrightarrow  \sqrt{\bar \alpha_{t}}x_0+\sqrt{1-\bar \alpha_{t}}z \sim N(\sqrt{\bar \alpha_{t}}x_0,1-\bar \alpha_{t})
\]</span></p>
<p><span class="math display">\[
q(X_{t}|X_{t-1},X_0)\Longrightarrow  \sqrt{ \alpha_{t}}x_{t-1}+\sqrt{1- \alpha_{t}}z \sim N(\sqrt{ \alpha_{t}}x_0,1- \alpha_{t})
\]</span></p>
<p>将式(16~18)代入式(15)得： <span class="math display">\[
q(X_{t-1}|X_t,X_0)\propto \exp \left(-\frac{1}{2}\left(\frac{\left(\mathbf{x}_{t}-\sqrt{\alpha_{t}} \mathbf{x}_{t-1}\right)^{2}}{\beta_{t}}+\frac{\left(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t}}\right)\right)
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}
=\exp \left(-\frac{1}{2}\left(\frac{\mathbf{x}_{t}^{2}-2 \sqrt{\alpha_{t}} \mathbf{x}_{t} \mathbf{x}_{t-1}+\alpha_{t} \mathbf{x}_{t-1}^{2}}{\beta_{t}}+\frac{\mathbf{x}_{t-1}^{2}-2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} \mathbf{x}_{t-1}+\bar{\alpha}_{t-1} \mathbf{x}_{0}^{2}}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t}}\right)\right) \\
=\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^{2}-\left(\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{2 \sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) \mathbf{x}_{t-1}+C\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)\right)\right) 
\end{array}
\]</span></p>
<p><span class="math display">\[
\exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)=\exp \left(-\frac{1}{2}\left(\frac{1}{\sigma^{2}} x^{2}-\frac{2 \mu}{\sigma^{2}} x+\frac{\mu^{2}}{\sigma^{2}}\right)\right)
\]</span></p>
由式(20)可知： $$
<span class="math display">\[\begin{cases}
\bar \beta_t = \frac{1}{\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar \alpha_{t-1}}} = \frac{1-\bar \alpha_{t-1}}{1-\bar \alpha_t} \cdot \beta_t  \\
\bar\mu_t(X_t,X_0)=\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0} \\

\end{cases}\]</span>
<p>$$</p>
<p>其中: <span class="math display">\[
x_0=\frac{1}{\sqrt{\bar\alpha_t}}(x_t-\sqrt{1-\bar \alpha_t}z_t)
\]</span> 将<span class="math inline">\(x_0\)</span>代入式（22）最终得到<span class="math inline">\(\tilde{\mu}_{t}\)</span></p>
<p><span class="math display">\[
\tilde{\mu}_{t}=\frac{1}{\sqrt{a_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{a}_{t}}} {z}_{t}\right)
\]</span> 其中<span class="math inline">\(z_t\)</span>是什么？是估计的每个时刻的噪声，即<span class="math inline">\(X_T\)</span>时刻的噪声</p>
<ul>
<li>虽然无法直接求解，但是能通过训练一个模型进行计算</li>
<li>采用Unet模型</li>
<li>模型的输入参数为当前时刻的分布和时刻t</li>
<li>模型的真实结果其实是由前向过程中添加的noise标签（显然是已知的）</li>
</ul>
<h3 id="目标数据分布的似然函数">4.4 目标数据分布的似然函数</h3>
<p>我们可以在负对数似然函数的基础上加上一个KL散度，于是就构成了负对数似然的上界，上界越小，负对数似然自然也就越小，那么对数似然就越大了 <span class="math display">\[
\begin{aligned}
-\log p_{\theta}({\mathbf x}_{0})&amp; \leq-\log p_\theta(\mathbf{x}_0)+D_{\mathrm{KL}}(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\|p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0))  \\
&amp;=-\log p_\theta(\mathbf{x}_0)+\mathbb{E}_{\mathbf{x}_1:T\sim q(\mathbf{x}_1:T|\mathbf{x}_0)}\left[\log\dfrac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})/p_\theta(\mathbf{x}_0)}\right] \\
&amp;=-\log p_\theta\mathfrak{(x}_0)+\mathbb{E}_q\Big[\log\dfrac{q(\mathbf{x}_1:T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0:T)}+\log p_\theta(\mathbf{x}_0)\Big] \\
&amp;=\mathbb{E}_q\Big[\log\dfrac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})}\Big] \\
\mathrm{Let}~L_{\mathrm{VLB}}&amp; =\mathbb{E}_{q(\mathbf{x}_0:T)}\Big[\log\dfrac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0:T)}\Big]\geq-\mathbb{E}_{q(\mathbf{x}_0)}\log p_\theta(\mathbf{x}_0) 
\end{aligned}
\]</span> 进一步可以写出如上公式的交叉熵的上界，接下来，对交叉熵的上界进行化简 <span class="math display">\[
q(X_t|X_{t-1}) =q(X_t|X_{t-1},X_0) = \frac{q(X_t,X_{t-1},X_0)}{q(X_{t-1},X_0)}=\frac{q(X_{t-1}|X_t,X_0)q(X_t|X_0)q(X_0)}{q(X_{t-1},X_0)}=\frac{q(X_{t-1}|X_t,X_0)q(X_t|X_0)}{q(X_{t-1}|X_0)}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
L_{\mathrm{VLB}}&amp; =\mathbb{E}_{q(\mathbf{x}_{0:T)}}\Big[\log\frac{q(\mathbf{x}_{1:T}|\mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0:T})}\Big]  \\
&amp;=\mathbb{E}_{q}\Big[\log{\frac{\prod_{t=1}^{T}q(\mathbf{x}_{t}|\mathbf{x}_{t-1})}{p_{\theta}(\mathbf{x}_{T})\prod_{t=1}^{T}p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})}}\Big] \\
&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=1}^T\log\dfrac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}\Big] \\
&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=2}^T\log\dfrac{q(\mathbf{x}_t|\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x_k})}\Big] \\
&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=2}^T\log\Big(\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}\cdot\dfrac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}\Big)+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}\Big] \\
&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum_{t=2}^T\log\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+\sum_{t=2}^T\log\dfrac{q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}\Big] \\
&amp;=\mathbb{E}_q\Big[-\log p_\theta(\mathbf{x}_T)+\sum\limits_{=2}^T\log\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+\log\dfrac{q(\mathbf{x}_T|\mathbf{x}_0)}{q(\mathbf{x}_1|\mathbf{x}_0)}+\log\dfrac{q(\mathbf{x}_1|\mathbf{x}_0)}{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}\Big] \\
&amp;=\mathbb{E}_q\Big[\log\dfrac{q(\mathbf{x}_T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_T)}+\sum\limits_{t=2}^T\log\dfrac{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)\Big] \\
&amp;=\mathbb{E}_q[\underbrace{D_{\mathrm{KL}}(q(\mathbf{x}_T|\mathbf{x}_0)\parallel p_\theta(\mathbf{x}_T))}_{L_T}+\sum_{t=2}^T\underbrace{D_{\mathrm{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\parallel p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))}_{L_{t-1}}-\underbrace{\log p_0(\mathbf{x}_0|\mathbf{x}_1)]}_{L_0}
\end{aligned}
\]</span></p>
<p>这里论文将<span class="math inline">\(p_{\theta}(X_{t-1}|X_t)\)</span>分布的方差设置为一个与<span class="math inline">\(\beta\)</span>相关的常数，因此可训练的参数值存在于其均值中</p>
<p>对于两个单一变量的高斯分布p和q而言，他们的kl散度如式（3） <span class="math display">\[
KL(p,q) = log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
\]</span></p>
<p><span class="math display">\[
\begin{gathered}
L_{t-1}=\mathbb{E}_{q}\left[{\frac{1}{2\sigma_{t}^{2}}}\|{\tilde{\mu}}_{t}(\mathbf{x}_{t},\mathbf{x}_{0})-{\boldsymbol{\mu}}_{\theta}(\mathbf{x}_{t},t)\|^{2}\right]+C \\
 r_{t-1}-C=E_{x_{0},\kappa}\left[{\frac{1}{2\sigma_{t}^{2}}}\left\|{\tilde{\mu}}_{t}\left(\mathbf{x}_{t}(\mathbf{x}_{0},\epsilon),{\frac{1}{\sqrt{\alpha t}}}(\mathbf{x}_{0},\epsilon)-{\sqrt{1-{\bar{\alpha}}_{t}}}\epsilon)\right)-\mu_{0}(\mathbf{x_t}(\mathbf{x}_{0},\epsilon),t)\right\|^{2}\right] \\
=\mathbb{E}_{\mathbf{x}_{0},\epsilon}\left[\dfrac{1}{2\sigma_{t}^{2}}\left\|\dfrac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}(\mathbf{x}_{0},\epsilon)-\dfrac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon\right)-\mu_{\theta}(\mathbf{x}_{t}(\mathbf{x}_{0},\epsilon),t)\right\|^{2}\right] \\
 \mu_{\theta}(\mathbf{x}_{t},t)={\bar{\mu}}_{t}\left(\mathbf{x}_{t},{\frac{1}{\sqrt{\bar{\alpha}t}}}(\mathbf{x}_{t}-{\sqrt{1-{\bar{\alpha}}_{t}}}\epsilon_{\theta}(\mathbf{x}_{t}))\right)={\frac{1}{\sqrt{\alpha_{t}}}}\left(\mathbf{x}_{t}-{\frac{\beta_{t}}{\sqrt{1-{\bar{\alpha}}_{t}}}}\epsilon_{\theta}(\mathbf{x}_{t},t)\right) \\
\mathbb{E}_{\mathbf{x}_{0},\epsilon}\left[\frac{\beta_{t}^{2}}{2\sigma_{t}^{2}\alpha_{t}(1-\bar{\alpha}_{t})}\left\|\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,t)\right\|^{2}\right] \\
L_{\mathrm{simple}}(\theta):=\mathbb{E}_{t,\mathbf{x}_{0},\epsilon}\Big[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}(\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon},t)\right\|^{2}\Big] 
\end{gathered}
\]</span></p>
<ul>
<li><span class="math inline">\(\tilde{\mu}_{t}\)</span>是可以计算得到的，所以可以用<span class="math inline">\(\mu_{\theta}\)</span>去预测<span class="math inline">\(\tilde{\mu}_{t}\)</span>网络，进而得到 loss function</li>
<li>预测均值的方法不太合理，预测噪声的方式更加合理。预测目标转移成了<span class="math inline">\(\varepsilon\)</span></li>
</ul>
<p><span class="math display">\[
\mathbf x_t(\mathbf x_0,\varepsilon ) = \sqrt {\bar {\alpha_n} } \mathbf x_0 + \sqrt{1-\bar{\alpha_n}}\varepsilon \\ \varepsilon \sim\mathcal{N}(0,I)
\]</span></p>
<p>然后式（24）可以转换为： <span class="math display">\[
{\mu}_{\theta}(\mathbf x_t,t)=\frac{1}{\sqrt{a_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{a}_{t}}} \varepsilon_{\theta}(\mathbf x_t,t)\right)
\]</span></p>
<h2 id="算法代码介绍">5 算法代码介绍</h2>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202305032258822.png" alt="image-20230503225817264" /><figcaption aria-hidden="true">image-20230503225817264</figcaption>
</figure>
<p>使用jupyter notebook 进行演示</p>
<h3 id="选择一个数据集">5.1 选择一个数据集</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_s_curve</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">s_curve, _ = make_s_curve(<span class="number">10</span>**<span class="number">4</span>,noise=<span class="number">0.1</span>)</span><br><span class="line">s_curve = s_curve[:,[<span class="number">0</span>,<span class="number">2</span>]] / <span class="number">10.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;shape of moons:&quot;</span>,np.shape(s_curve))</span><br><span class="line"></span><br><span class="line">data = s_curve.T</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(*data,color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">datasets = torch.Tensor(s_curve).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<h3 id="确定超参数的值">5.2 确定超参数的值</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">100</span> <span class="comment"># 对于步骤，一开始可以由beta，分布的均值和标准差来共同确定</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定每一步的beta</span></span><br><span class="line">betas = torch.linspace(-<span class="number">6</span>,<span class="number">6</span>,num_steps)</span><br><span class="line">betas = torch.sigmoid(betas) * (<span class="number">0.5e-2</span> - <span class="number">1e-5</span>) + <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 alpha，alpha_prod,alpha_prod_previous,alpha_bar_sqrt等变量的值</span></span><br><span class="line">alphas = <span class="number">1</span> - betas</span><br><span class="line">alphas_prod = torch.cumprod(alphas,<span class="number">0</span>)</span><br><span class="line">alphas_prod_p = torch.cat([torch.tensor([<span class="number">1</span>]).<span class="built_in">float</span>(),alphas_prod[:-<span class="number">1</span>]], <span class="number">0</span>) <span class="comment"># p表示previous</span></span><br><span class="line">alphas_bar_sqrt = torch.sqrt(alphas_prod)</span><br><span class="line">one_minus_alphas_bar_log = torch.log(<span class="number">1</span> - alphas_prod)</span><br><span class="line">one_minus_alphas_bar_sqrt = torch.sqrt(<span class="number">1</span> - alphas_prod)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> alphas.shape == alphas_prod.shape == alphas_prod_p.shape == \</span><br><span class="line">alphas_bar_sqrt.shape == one_minus_alphas_bar_log.shape == one_minus_alphas_bar_sqrt.shape</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;all the same shape:&quot;</span>,betas.shape)</span><br></pre></td></tr></table></figure>
<h3 id="确定扩散过程任意时刻的采样值">5.3 确定扩散过程任意时刻的采样值</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算任意时刻的x的采样值，基于x_0和参数重整化技巧</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">q_x</span>(<span class="params">x_0,t</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;可以基于x[0]得到任意时刻t的x[t]&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    noise = torch.randn_like(x_0) <span class="comment"># noise 是从正态分布中生成的随机噪声</span></span><br><span class="line">    alphas_t = alphas_bar_sqrt[t]</span><br><span class="line">    alphas_1_m_t = one_minus_alphas_bar_sqrt[t]</span><br><span class="line">    <span class="keyword">return</span> (alphas_t * x_0 + alphas_1_m_t * noise) <span class="comment"># 在x[0]的基础上添加噪声</span></span><br></pre></td></tr></table></figure>
<h3 id="演示原始数据分布加噪100步后的效果">5.4 演示原始数据分布加噪100步后的效果</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_shows = <span class="number">20</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">2</span>,<span class="number">10</span>,figsize=(<span class="number">28</span>,<span class="number">3</span>))</span><br><span class="line">plt.rc(<span class="string">&quot;text&quot;</span>,color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 共有10000个点，每个点包含两个坐标</span></span><br><span class="line"><span class="comment"># 生成100步以内每隔5步加噪声后的图像</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_shows):</span><br><span class="line">    j = i // <span class="number">10</span></span><br><span class="line">    k = i % <span class="number">10</span></span><br><span class="line">    q_i = q_x(datasets,torch.tensor([i*num_steps // num_shows])) <span class="comment"># 生成t时刻的采样数据</span></span><br><span class="line"></span><br><span class="line">    axs[j,k].scatter(q_i[:,<span class="number">0</span>],q_i[:,<span class="number">1</span>],color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    axs[j,k].set_axis_off()</span><br><span class="line">    axs[j,k].set_title(<span class="string">&quot;$q(\mathbf&#123;x&#125;_&#123;&quot;</span>+<span class="built_in">str</span>(i*num_steps//num_shows)+<span class="string">&quot;&#125;)$&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="编写拟合逆扩散过程高斯分布的模型">5.5 编写拟合逆扩散过程高斯分布的模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLPDiffusion</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,n_steps,num_units=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MLPDiffusion,self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.linears = nn.ModuleList(</span><br><span class="line">        [</span><br><span class="line">            nn.Linear(<span class="number">2</span>,num_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_units,num_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_units,num_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_units,<span class="number">2</span>),</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        self.step_embeddings = nn.ModuleList([</span><br><span class="line">            nn.Embedding(n_steps,num_units),</span><br><span class="line">            nn.Embedding(n_steps,num_units),</span><br><span class="line">            nn.Embedding(n_steps,num_units),</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x_0,t</span>):</span><br><span class="line">        x = x_0</span><br><span class="line">        <span class="keyword">for</span> idx,embedding_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.step_embeddings):</span><br><span class="line">            t_embedding = embedding_layer(t)</span><br><span class="line">            x = self.linears[<span class="number">2</span>*idx](x)</span><br><span class="line">            x += t_embedding</span><br><span class="line">            x = self.linears[<span class="number">2</span>*idx+<span class="number">1</span>](x)</span><br><span class="line">            </span><br><span class="line">        x = self.linears[-<span class="number">1</span>](x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="编写训练的误差函数">5.6 编写训练的误差函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">diffusion_loss_fn</span>(<span class="params">model,x_0,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,n_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对任意时刻t进行采样计算&quot;&quot;&quot;</span></span><br><span class="line">    batch_size = x_0.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机CIA杨一个时刻t，为了提高训练速度，这里我确保t不重复</span></span><br><span class="line">    </span><br><span class="line">    t = torch.randint(<span class="number">0</span>,n_steps,size=(batch_size//<span class="number">2</span>,))</span><br><span class="line">    t = torch.cat([t,n_steps-<span class="number">1</span>-t],dim=<span class="number">0</span>)</span><br><span class="line">    t = t.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># x0的系数</span></span><br><span class="line">    a = alphas_bar_sqrt[t]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># eps的系数</span></span><br><span class="line">    am1 = one_minus_alphas_bar_sqrt[t]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成随机噪声eps</span></span><br><span class="line">    e = torch.randn_like(x_0)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构造模型的输入</span></span><br><span class="line">    x = x_0 * a + e * am1</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 送入模型，得到t时刻的随机噪声预测值</span></span><br><span class="line">    output = model(x,t.squeeze(-<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 与真实噪声一起计算误差，求平均值</span></span><br><span class="line">    <span class="keyword">return</span> (e - output).square().mean()</span><br></pre></td></tr></table></figure>
<h3 id="编写逆扩散采样函数inference过程">5.7 编写逆扩散采样函数(inference过程)</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_loop</span>(<span class="params">model,shape,n_steps,betas,one_minus_alphas_bar_sqrt</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从x[T]恢复x[T-1],x[T-2],...,x[0]&quot;&quot;&quot;</span></span><br><span class="line">    cur_x = torch.randn(shape)</span><br><span class="line">    x_seq = [cur_x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(n_steps)):</span><br><span class="line">        cur_x = p_sample(model,cur_x,i,betas,one_minus_alphas_bar_sqrt)</span><br><span class="line">        x_seq.append(cur_x)</span><br><span class="line">    <span class="keyword">return</span> x_seq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">model,x,t,betas,one_minus_alphas_bar_sqrt</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从x[T]采样t时刻的重构值&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    t = torch.tensor([t])</span><br><span class="line">    coeff = betas[t] / one_minus_alphas_bar_sqrt[t]</span><br><span class="line">    eps_theta = model(x,t)</span><br><span class="line">    mean = (<span class="number">1</span> / (<span class="number">1</span>-betas[t]).sqrt() * (x - (coeff * eps_theta)))</span><br><span class="line">    </span><br><span class="line">    z = torch.randn_like(x)</span><br><span class="line">    sigma_t = betas[t].sqrt()</span><br><span class="line">    sample = mean + sigma_t * z</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (sample)</span><br></pre></td></tr></table></figure>
<h3 id="开始训练模型并打印loss及中间的重构效果">5.8开始训练模型，并打印loss及中间的重构效果</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">seed = 42</span><br><span class="line">class EMA():</span><br><span class="line">    &quot;&quot;&quot;构建一个参数平滑器&quot;&quot;&quot;</span><br><span class="line">    def __init__(self,mu=0.01):</span><br><span class="line">        self.mu = mu</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line">    def register(self,name,val):</span><br><span class="line">        self.shadow[name] = val.clone()</span><br><span class="line">        </span><br><span class="line">    def __cell__(self,name,x):</span><br><span class="line">        assert name in self.shadow</span><br><span class="line">        new_average = self.mu * x + (1.0 - self.mu) * self.shadow[name]</span><br><span class="line">        self.shadow[name] = new_average.clone()</span><br><span class="line">        return new_average</span><br><span class="line">    </span><br><span class="line">print(&quot;training model...&quot;)</span><br><span class="line"></span><br><span class="line">batch_size = 128</span><br><span class="line">dataloader = torch.utils.data.DataLoader(datasets,batch_size=batch_size,shuffle=True)</span><br><span class="line">dataloader = dataloader.cuda()</span><br><span class="line">num_epoch = 4000</span><br><span class="line">plt.rc(&quot;text&quot;,color=&quot;blue&quot;)</span><br><span class="line"></span><br><span class="line">model = MLPDiffusion(num_steps).cuda() # 输出维度是2，输入是x和step</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)</span><br><span class="line"></span><br><span class="line">for t in range(num_epoch):</span><br><span class="line">    for idx,batch_x in enumerate(dataloader):</span><br><span class="line">        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(),1.)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    # print loss</span><br><span class="line">    if (t % 100 == 0):</span><br><span class="line">        print(loss)</span><br><span class="line">        x_seq = p_sample_loop(model,datasets.shape,num_steps,betas,one_minus_alphas_bar_sqrt) #共有100个元素</span><br><span class="line">        </span><br><span class="line">        fig, axs = plt.subplots(1,10,figsize=(28,3))</span><br><span class="line">        for i in range(1,11):</span><br><span class="line">            cur_x = x_seq[i * 10].detach()</span><br><span class="line">            axs[i-1].scatter(cur_x[:,0],cur_x[:,1],color=&quot;red&quot;,edgecolor=&quot;white&quot;)</span><br><span class="line">            axs[i-1].set_axis_off()</span><br><span class="line">            axs[i-1].set_title(&quot;$q(\mathbf&#123;x&#125;_&#123;&quot;+str(i*10)+&quot;&#125;)$&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="动画演示">5.9 动画演示</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generating the forward image sequence 生成前向过程，也就是逐步加噪声</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    plt.clf()</span><br><span class="line">    q_i = q_x(datasets,torch.tensor([i]))</span><br><span class="line">    plt.scatter(q_i[:,<span class="number">0</span>],q_i[:,<span class="number">1</span>],color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>,s=<span class="number">5</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    img_buf = io.BytesIO()</span><br><span class="line">    plt.savefig(img_buf,<span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_buf)</span><br><span class="line">    imgs.append(img)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># generating the reverse diffusion sequence</span></span><br><span class="line">reverse = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    plt.clf()</span><br><span class="line">    cur_x = x_seq[i].detach() <span class="comment"># 拿到训练末尾阶段生成的x_seq</span></span><br><span class="line">    plt.scatter(q_i[:,<span class="number">0</span>],q_i[:,<span class="number">1</span>],color=<span class="string">&quot;red&quot;</span>,edgecolor=<span class="string">&quot;white&quot;</span>,s=<span class="number">5</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    img_buf = io.BytesIO()</span><br><span class="line">    plt.savefig(img_buf,<span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_buf)</span><br><span class="line">    reverse.append(img)</span><br><span class="line">    </span><br><span class="line"> imgs = imgs + reverse</span><br><span class="line"> imgs[<span class="number">0</span>].save(<span class="string">&quot;diffusion.gif&quot;</span>,<span class="built_in">format</span>=<span class="string">&quot;GIF&quot;</span>,append_images=imgs,save_all=<span class="literal">True</span>,duration=<span class="number">100</span>,loop=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">pytorch</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">diffusion model</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/8c9cf490.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
    <article id="post-用pytorch实现基础网络12-Unet(待完善)" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title_code_ant" href="/post/94472398.html">用pytorch实现基础网络12-Unet</a>
    </h2>
  

        
		
		  <a href="/post/94472398.html" class="archive-article-date">
  	<time datetime="2023-04-30T06:12:22.000Z" itemprop="datePublished">
	<!-- <i class="icon-calendar icon"></i> -->

	<i class="fa fa-calendar-check-o" aria-hidden="true"></i>
	&nbsp;
	2023-04-30</time>
	
	<!-- busuanzi阅读量统计
	
	-->
	
    <!-- waline阅读量统计 -->
	

</a>


        
		
		
		  <!-- 添加标题栏文字统计效果 -->
<div class="word-count">
	
      
        <span class="article-type" style="
          color: white;
          font-size: 14px;
          background: #0088CC;
          padding: 0 5px 1px 5px;
          margin-right: 5px;
          border-radius: 2px;">原创</span>
		  &nbsp; 
      
    
	
    <span class="post-time">
      <span class="post-meta-item-icon">
	    <i class="fa fa-bar-chart" aria-hidden="true"></i>
        <!-- <i class="fa fa-keyboard-o" aria-hidden="true"></i> -->
        <span class="post-meta-item-text">字数统计: </span>
        <span class="post-count">92字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
	    <i class="fa fa-pagelines" aria-hidden="true"></i>
        <span class="post-meta-item-text">阅读时长: </span>
        <span class="post-count">1min</span>
      </span>
    </span>
	

</div>
<!-- 添加标题栏文字统计效果结束 -->
		
      </header>
    
	
    <div class="article-entry" itemprop="articleBody">
	  <!-- 添加分类与标签 -->
	  
		  
		  <h1 id="u-net-convolutional-networks-for-biomedical-image-segmentation">U-Net: Convolutional Networks for Biomedical Image Segmentation</h1>
<p>https://arxiv.org/pdf/1505.04597.pdf</p>
<figure>
<img src="https://picgo-1259245122.cos.ap-shanghai.myqcloud.com/img/blog/202304301414602.png" alt="image-20230430141414836" /><figcaption aria-hidden="true">image-20230430141414836</figcaption>
</figure>
<ul>
<li>为什么输入图片和输出图片的大小不同？---因为输入的图片本质也是388*388的，只是通过了一定的填充方式变成了572*572。这是为了让边缘的像素具有更好的上下文信息。</li>
</ul>
<p>example：</p>
<p>https://github.com/yassouali/pytorch-segmentation</p>

				  
	  
      
	  <!-- 添加打赏 -->
	  
	
	  <!-- 添加版权声明 -->
      
	  
	</div>
	
	<!-- 添加置顶 -->
    <div class="article-info article-info-index">
      
	  
	  <!-- 分类页 -->
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">pytorch</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">network</a>
        		</li>
      		
		</ul>
	</div>

      

	  
	  <!-- 添加展开全文 -->
      
        <p class="article-more-link">
          <a class="article-more-a" href="/post/94472398.html">展开全文 >></a>
        </p>
      
	  
	  <!-- 添加分享 -->
      
	  
      <div class="clearfix"></div>
	  
    </div>
  </div>
</article>



<!-- 添加回到顶部和文章目录 -->
<aside class="wrap-side-operation">
  <div class="mod-side-operation">
    
      <div class="jump-container" id="js-jump-container" style="display:none;">
        <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
          <i class="icon-font icon-back"></i>
        </a>
      </div>
    
    
  </div>
</aside>

<!-- 添加评论 -->


<!-- 文章页添加mathjax公式 -->

  

<!-- 文章页添加mathjax公式 -->
  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">下一页 &gt;&gt;</a>
    </nav>
  


<!-- 主页添加mathjax公式 -->

  <!-- mathjax http://docs.mathjax.org/en/latest/web/start.html -->
<script>
window.MathJax = {
  tex: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true
  },
  chtml: {
    scale: 1,                      // global scaling factor for all expressions
    minScale: .5,                  // smallest scaling factor to use
    mtextInheritFont: false,       // true to make mtext elements use surrounding font
    merrorInheritFont: false,      // true to make merror text use surrounding font
    mtextFont: '',                 // font to use for mtext, if not inheriting (empty means use MathJax fonts)
    merrorFont: 'serif',           // font to use for merror, if not inheriting (empty means use MathJax fonts)
    unknownFamily: 'serif',        // font to use for character that aren't in MathJax's fonts
    mathmlSpacing: false,          // true for MathML spacing rules, false for TeX rules
    skipAttributes: {},            // RFDa and other attributes NOT to copy to the output
    exFactor: .5,                  // default size of ex in em units
    displayAlign: 'center',        // default for indentalign when set to 'auto'
    displayIndent: '0'             // default for indentshift when set to 'auto'
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
};
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<!-- 主页添加mathjax公式 -->

          </div>
        </div>
      </div>
	  
    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: false,
		root: "/",
		innerArchive: false,
		showTags: true
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev"><< 上一页</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">下一页 >></a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/js/main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/js/slider.e37972.js")}()</script>

<!--添加鼠标特效-->
<!-- https://blog.csdn.net/weixin_41287260/article/details/103050877 -->



<!--添加鼠标特效结束-->
    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="搜一搜">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">aboutme</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">pytorch</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">tools</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">diffusion model</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">project</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">tips</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">github</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">network</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">强化学习</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">yolo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">summary</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
			<pre style="font-size: 12px;" q-show="jsonFail">
			  jsonContent:
				meta: false
				pages: false
				posts:
				  title: true
				  date: true
				  path: true
				  text: false
				  raw: false
				  content: false
				  slug: false
				  updated: false
				  comments: false
				  link: false
				  permalink: false
				  excerpt: false
				  categories: false
				  tags: true
			</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://chat.openai.com/auth/login/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>chatgpt</a>
            </li>
          
            <li class="search-li">
              <a href="https://xs.scqylaw.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>google scholar</a>
            </li>
          
            <li class="search-li">
              <a href="https://pytorch.org/tutorials/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>pytorch</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.latexlive.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>在线latex公式编辑器</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接4</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接5</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接6</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
		<div class="aboutme-wrap"> 
			<div style="display:;color:LightSkyBlue;"> 
				
					<p id="hitokoto" style="margin:0 20px 0 20px;color:GreenYellow;"></p>
					<div style="margin:0 20px 0 20px;">
						<p id="from" style="margin:10px;text-align:right;color:Salmon;"></p>
					</div>	
					<script>
						var xmlhttp = new XMLHttpRequest();
						xmlhttp.onreadystatechange = function() {
								if (this.readyState == 4 && this.status == 200) {
									yiyan = JSON.parse(this.responseText);
									document.getElementById("hitokoto").innerHTML =yiyan.hitokoto;
								document.getElementById("from").innerHTML ="——《"+ yiyan.from+"》";
							 }
						};
						xmlhttp.open("GET", "https://v1.hitokoto.cn/?c=a&c=d&c=c", true);
						xmlhttp.send();
					</script>
				
			
				<br>
				  
					<p id="js-aboutme" style="margin:0 20px 0 20px;">just for fun</p>
				
			</div> 
		</div> 
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
  
  <!-- 代码块复制功能 -->
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js"></script>
  <script type="text/javascript" src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
  <script type="text/javascript" src="/js/clipboard_use.js"></script>
  <!-- 代码块复制功能结束 -->
  
  <!--全局添加雪花特效 -->
  
  <!--全局添加雪花特效结束 -->

  <!--全局添加 aplayer播放器 https://aplayer.js.org/#/zh-Hans/ -->
  
  <!-- aplayer播放器功能结束 -->
  
</body>
